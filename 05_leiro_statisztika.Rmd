# Leíró statisztika {#leiro_stat}

```{r include=FALSE}
source("_common.R")
```

## A szövegek a vektortérben

A szövegbányászati feladatok két altípusa a keresés és a rendszerezés. A keresés során olyan szövegeket keresünk, amelyekben egy adott kifejezés előfordul, a rendszerezés során pedig a szövegeket hasonlítjuk össze egymással és egy előre megadott, vagy egy előzetesen nem ismert kategóriarendszer csoportjaihoz soroljuk őket. A webes keresőprogramok egyik jellemző tevékenysége, az információ-visszakeresés (*information retrieval*) során például az a cél, hogy a korpuszból visszakeressük a kereső információigénye szempontjából releváns információkat, mely keresés alapulhat metaadatokon vagy teljes szöveges indexelésen [@tikkSzovegbanyaszat2007; @russelMestersegesIntelligencia2005, 742.o]. Az információkinyerés (*information extraction*) esetén a cél, hogy a strukturálatlan szövegekből strukturált adatokat állítsunk elő. Azaz az információkinyerés során nem a felhasználó által keresett információt keressük meg és lokalizáljuk, hanem az adott kérdés szempontjából releváns információkat gyűjtjük ki a dokumentumokból. Az információkinyerés alternatív megoldása segítségével már képesek lehetünk a kifejezések közötti kapcsolatok elemzésére, tendenciák és minták felismerésére és az információk összekapcsolása révén új információk létrehozására, azaz a segítségével strukturálatlan szövegekből is előállíthatunk strukturált információkat [@kwartlerTextMiningPractice2017; @schutzeIntroductionInformationRetrieval2008; @tikkSzovegbanyaszat2007, 63-81.o].

A szövegbányászati vizsgálatok során folyó szövegek, azaz strukturálatlan vagy részben strukturált dokumentumok elemzésére kerül sor. Ezekből a kutatási kérdéseink szempontjából releváns, látens összefüggéseket nyerünk ki, amelyek már strukturált szerkezetűek. A dokumentumok reprezentálásának három legelterjedtebb módja a halmazelmélet alapú, az algebrai és a valószínűségi modell. A halmazelméleti modellek a dokumentumok hasonlóságát halmazelmélet, a valószínűségi modellek pedig feltételes valószínűségi becslés alapján határozzák meg. Az algebrai modellek a dokumentumokat vektorként vagy mátrixként ábrázolják és algebrai műveletek segítségével hasonlítják össze. 

A vektortérmodell sokdimenziós vektortérben ábrázolja a dokumentumokat, úgy, hogy a dokumentumokat vektorokkal reprezentálja, a vektortér dimenziói pedig a dokumentumok összességében előforduló egyedi szavak. A modell alkalmazása során azok a dokumentumok hasonlítanak egymásra, amelyeknek a szókészlete átfedi egymást, és a hasonlóság mértéke az átfedéssel arányos. A vektortérmodellben a dokumentumgyűjteményt a szó-dokumentum mátrixszal (*term-document matrix*) reprezentáljuk, a mátrixban a sorok száma megegyezik az egyedi szavak számával, az oszlopokat pedig a dokumentumvektorok alkotják. Az egyedi szavak összességét szótárnak nevezzük. Mivel mátrixban az egyedi szavak száma általában igen nagy, ezért a mátrix hatékony kezeléséhez annak mérete különböző eljárásokkal csökkenthető. Fontos tudni, hogy a dokumentumok vektortér reprezentációjában a szavak szövegen belüli sorrendjére és pozíciójára vonatkozó információ nem található meg [@russelMestersegesIntelligencia2005, 742-744 o.; @kwartlerTextMiningPractice2017; @welbersTextAnalysis2017]. A vektortérmodellt szózsák (*bag of words*) modellnek is nevezzük, melynek segítségével a fent leírtak szerint az egyes szavak gyakoriságát vizsgálhatjuk meg egy adott korpuszon belül.

## Leíró statisztika

Fejezetünkben nyolc véletlenszerűen kiválasztott magyar miniszterelnöki beszéd vizsgálatát végezzük el,[^leiro-1] amihez az alábbi csomagokat használjuk:

[^leiro-1]: A beszédeket a Hungarian Comparative Agendas Project miniszterelnöki beszéd korpuszából válogattuk: <https://cap.tk.hu/vegrehajto>

```{r message=FALSE, warning=FALSE}
library(HunMineR)
library(readtext)
library(dplyr)
library(lubridate)
library(stringr)
library(ggplot2)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(GGally)
library(ggdendro)
library(tidytext)
```

Első lépésben a [Bevezetőben](#intro) már ismertetett módon a `HunMineR` csomagból betöltjük a beszédeket. A `glimpse()` függvénnyel egy gyors pilltast vethetünk a betöltött adatokra.

```{r}
texts <- HunMineR::data_miniszterelnokok_raw

dplyr::glimpse(texts)
```

Ezt követően az [*Adatkezelés R-ben*](#adatkezeles) című fejezetben ismertetett `mutate()` függvény használatával két csoportra osztjuk a beszédeket. Ehhez először a `string_extract()` függvény segítségével meghatározzuk, hogy a kettéosztáshoz használni kívánt új változó a doc_id legyen, a `[^\\.]*` regex segítségével leválasztva arról a `.txt` kiterjesztést, majd a `str_sub()` függvénnyel megmondjuk, hogy a miniszterelnökök neve a `doc_id` hátulról számított hatodik karakteréig tart. Ezután kialakítjuk a két csoportot, azaz az `if_else()` segítségével meghatározzuk, hogy ha „antall_jozsef", „boross_peter", „orban_viktor" beszédeiről van szó azokat a jobb csoportba tegye, a maradékot pedig a bal csoportba.

Majd azt is meghatározzuk, hogy melyik beszédnek mi a dátuma. Ehhez szintén a `str_sub()` függvényt használjuk, majd a `lubridate` segítségével alakítjuk ki a kívánt dátumformátumot.[^leiro-2]

[^leiro-2]: A `lubridate` használatának részletes leírása megtalálható az alábbi linken: <https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf>

Ezután a `glimpse()` függvény segítségével megtekintjük, hogy milyen változtatásokat végeztünk az adattáblánkon. Láthatjuk, hogy míg korábban 8 dokumentumunk és 2 változónk volt, az átalakítás eredményeként a 8 dokumentum mellett már 5 változót találunk. Ezzel a lépéssel tehát kialakítottuk azokat a változókat, amelyekre az elemzés során szükségünk lesz.


```{r}
miniszterelnokok <- c("antall_jozsef", "boross_peter", "orban_viktor")

texts <- texts %>%
  mutate(
    doc_id = stringr::str_extract(doc_id, "[^\\.]*"),
    mineln = stringr::str_sub(doc_id, end = -6),
    partoldal = dplyr::if_else(mineln %in% miniszterelnokok, "jobb", "bal")
  )

texts$year <- str_sub(texts$doc_id, start = -2) %>%
  stringr::str_c("-01-01") %>%
  lubridate::ymd() %>%
  lubridate::year()

glimpse(texts)
```

Ezt követően a további lépések elvégzéséhez létrehozzuk a `quanteda` korpuszt, majd a `summary()` függvény segítségével megtekinthetjük a korpusz alapvető statisztikai jellemzőit. Láthatjuk például, hogy az egyes dokumentumok hány tokenből vagy mondatból állnak.

```{r}
corpus_mineln <- corpus(texts)

summary(corpus_mineln)
```

Mivel az elemzés során a korpuszon belül két csoportra osztva szeretnénk összehasonlításokat tenni, az alábbiakban két alkorpuszt alakítunk ki.

```{r}
mineln_jobb <- quanteda::corpus_subset(corpus_mineln, mineln %in% c("antall_jozsef", "boross_peter", 
    "orban_viktor"))

mineln_bal <- quanteda::corpus_subset(corpus_mineln, mineln %in% c("horn_gyula", "gyurcsany_ferenc", 
    "medgyessy_peter", "bajnai_gordon"))

summary(mineln_jobb)

summary(mineln_bal)
```

A korábban létrehozott „jobb" és „bal" változó segítségével nem csak az egyes dokumentumokat, hanem a két csoportba sorolt beszédeket is összehasonlíthatjuk egymással.

```{r eval=FALSE}
summary(corpus_mineln) %>%
  group_by(partoldal) %>%
  summarise(
    mean_wordcount = mean(Tokens), 
    std_dev = sd(Tokens), 
    min_wordc = min(Tokens), 
    max_wordc = max(Tokens)
    )
```

```{r echo=FALSE}
# a <- summary(corpus_mineln) %>%
#   group_by(partoldal) %>%
#   summarise(
#     mean_wordcount = mean(Tokens), 
#     std_dev = sd(Tokens), 
#     min_wordc = min(Tokens), 
#     max_wordc = max(Tokens)
#     )
# 
# saveRDS(a, "data/temp/05_summary.rds")

readRDS("data/temp/05_summary.rds")
```


A `textstat_collocations()` függvény segítségével szókapcsolatokat kereshetünk. A függvény argumentumai közül a `size` a szókapcsolatok hossza, a `min_count` pedig a minimális előfordulásuk száma. Miután a szókapcsolatokat megkerestük, közülük a korábban már megismert `head()` függvény segítségével tetszőleges számút megnézhetünk.[^leiro-3]

[^leiro-3]: A lambda leírása megtalálható itt: <https://quanteda.io/reference/textstat_collocations.html>

```{r}
corpus_mineln %>%
  quanteda.textstats::textstat_collocations(
    size = 3,
    min_count = 6
  ) %>%
  head(n = 10)
```

A szókapcsolatok listázásánál is láthattuk, hogy a korpuszunk még minden szót tartalmaz, ezért találtunk például „hogy ez a" összetételt. A következőkben eltávolítjuk az ilyen funkció nélküli stopszavakat a korpuszból, amihez saját stopszólistát használunk. Először a `HunMineR` csomagból beolvassuk és egy `custom_stopwords` nevű objektumban tároljuk a stopszavakat, majd a `tokens()` függvény segítségével tokenizáljuk a korpuszt és a `tokens_select()` használatával eltávolítjuk a stopszavakat.

Ha ezután újra megnézzük a kollokációkat, jól látható a stopszavak eltávolításának eredménye:

```{r}
custom_stopwords <- HunMineR::data_stopwords_extra

corpus_mineln %>%
  tokens() %>%
  tokens_select(pattern = custom_stopwords, selection = "remove") %>%
  textstat_collocations(
    size = 3,
    min_count = 6
  ) %>%
  head(n = 10)
```

A korpusz további elemzése előtt fontos, hogy ne csak a stopszavakat távolítsuk el, hanem az egyéb alapvető szövegtisztító lépéseket is elvégezzük. Azaz a `tokens_select()` segítségével eltávolítsuk a számokat, a központozást, az elválasztó karaktereket, mint például a szóközöket, tabulátorokat, sortöréseket. 

Ezután a `tokens_ngrams()` segítségével ngramokat hozunk létre a tokenekből, majd kialakítjuk a dokumentum kifejezés mátrixot (`dfm`) és elvégezzük a `tf-idf` szerinti súlyozást. A `dfm_tfidf()` függvény kiszámolja a dokumentum gyakoriság inverz súlyozását. A függvény alapértelmezés szerint a normalizált kifejezések gyakoriságát használja a dokumentumon belüli relatív kifejezés gyakoriság helyett, ezt írjuk felül a `schem_tf = "prop"` használatával. Végül a `textstat_frequency()` segítségével gyakorisági statisztikát készíthetünk a korábban meghatározott (példánkban két és három tagú) n-gramokról.

```{r}
corpus_mineln %>%
  tokens(
    remove_numbers = TRUE, 
    remove_punct = TRUE, 
    remove_separators = TRUE
  ) %>%
  tokens_select(pattern = custom_stopwords, selection = "remove") %>%
  quanteda::tokens_ngrams(n = 2:3) %>%
  dfm() %>%
  dfm_tfidf(scheme_tf = "prop") %>%
  quanteda.textstats::textstat_frequency(n = 10, force = TRUE)
```

## A szövegek lexikai diverzitása

Az alábbiakban a korpuszunkat alkotó szövegek lexikai diverzitását vizsgáljuk. Ehhez a `quanteda` csomag `textstat_lexdiv()` függvényét használjuk. Mivel ez a függvény `dfm`-et elemez, először a `corpus_mineln` nevű korpuszunkból létrehozzuk a `mineln_dfm` nevű `dfm`-et, amelyen elvégezzük a korábban már megismert alapvető tisztító lépéseket. A `textstat_lexdiv()` függvény eredménye szintén egy `dfm`, így azt `arrange()` parancs argumentumában a `desc` megadásával csökkenő sorba is rendezhetjük. A`textstat_lexdiv()` különböző indexek segítségével számítja ki a szövegek lexikai különbözőségét, példánkban a `CTTR` indexet használjuk.[^leiro-4]

[^leiro-4]: A különböző indexek leírása megtalálható az alábbi linken: <https://quanteda.io/reference/textstat_lexdiv.html>

```{r}
mineln_dfm <- corpus_mineln %>%
  tokens(
    remove_punct = TRUE, 
    remove_separators = TRUE, 
    split_hyphens = TRUE
    ) %>%
  dfm() %>% 
  quanteda::dfm_remove(pattern = custom_stopwords)

mineln_dfm %>%
  quanteda.textstats::textstat_lexdiv(measure = "CTTR") %>%
  dplyr::arrange(dplyr::desc(CTTR))
```

A kiszámolt értéket hozzáadhatjuk a `dfm`-hez is.

```{r}
dfm_lexdiv <- mineln_dfm

cttr_score <- unlist(textstat_lexdiv(dfm_lexdiv, measure = "CTTR")[, 2])

quanteda::docvars(dfm_lexdiv, "cttr") <- cttr_score

docvars(dfm_lexdiv)
```

A fenti elemzést elvégezhetjük úgy is, hogy valamennyi indexálást egyben megkapjuk. Ehhez a `textstat_lexdiv()` függvény argumentumába a `measure = "all"` kifejezést kell megadnunk.

```{r}
mineln_dfm %>%
  textstat_lexdiv(measure = "all")
```

Ha pedig arra vagyunk kíváncsiak, hogy a kapott értékek hogyan viszonyulnak egymáshoz, azt a `cor()` függvény segítésével számolhatjuk ki.

```{r}
div_df <- mineln_dfm %>%
  textstat_lexdiv(measure = "all")

cor(div_df[, 2:13])
```

A kapott értékeket a `ggcorr()` függvény segítségével ábrázolhatjuk is. Ha a függvény argumentumában a `label = TRUE` szerepel, a kapott ábrán a kiszámított értékek is láthatók.

```{r, fig.cap="Korrelációs hőtérkép"}
GGally::ggcorr(div_df[, 2:13], label = TRUE)
```

Ezt követően azt is megvizsgálhatjuk, hogy a korpusz szövegei mennyire könnyen olvashatóak. Ehhez a `Flesch.Kincaid` pontszámot használjuk, ami a szavak és a mondatok hossza alapján határozza meg a szöveg olvashatóságát. Ehhez a `textstat_readability()` függvényt használjuk, mely a korpuszunkat elemzi.

```{r}
quanteda.textstats::textstat_readability(x = corpus_mineln, measure = "Flesch.Kincaid")
```

Ezután a kiszámított értékkel kiegészítjük a korpuszt.

```{r,  tidy = TRUE, tidy.opts=list(width.cutoff=80)}
docvars(corpus_mineln, "f_k") <- textstat_readability(corpus_mineln, measure = "Flesch.Kincaid")[, 2]

docvars(corpus_mineln)
```

```{r,  tidy = TRUE, tidy.opts=list(width.cutoff=80)}
docvars(corpus_mineln, "f_k") <- textstat_readability(corpus_mineln, measure = "Flesch.Kincaid")[, 2]

docvars(corpus_mineln)
```

Majd a `ggplot2` segítségével vizualizálhatjuk az eredményt. Ehhez az olvashatósági pontszámmal kiegészített korpuszból egy adattáblát alakítunk ki, majd beállítjuk az ábrázolás paramétereit. Azaz ahhoz, hogy a két tengelyen az év, illetve az olvashatósági pontszám szerepeljen, a színezés különböztesse meg a jobb és a bal oldalt, az egyes dokumentumokat ponttal jelöljük, a jobb és a bal oldali beszédeket vonallal kötjük össze, az ábrára fekete színnel felíratjuk a miniszterelnökök nevét. Valamint azt is beállítjuk, hogy az x tengely beosztása az egyes beszédek dátumához igazodjon. A `theme_minimal()` függvénnyel pedig azt határozzuk meg, hogy mindez fehér hátteret kapjon.

```{r}
corpus_df <- docvars(corpus_mineln)
```

```{r eval=FALSE}
ggplot(corpus_df, aes(year, f_k)) +
  geom_point(size = 2) +
  geom_line(aes(linetype = partoldal), size = 1) +
  geom_text(aes(label = mineln), color = "black", nudge_y = 0.15) +
  scale_x_continuous(limits = c(1988, 2020)) +
  labs(
    x = NULL,
    y = "Flesch-Kincaid index",
    color = NULL,
    linetype = NULL
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r, echo=FALSE, fig.cap="Az olvashatósági index alakulása"}
# b <- ggplot(corpus_df, aes(year, f_k)) +
#   geom_point(size = 2) +
#   geom_line(aes(linetype = partoldal), size = 1) +
#   geom_text(aes(label = mineln), color = "black", nudge_y = 0.15) +
#   scale_x_continuous(limits = c(1988, 2020)) +
#   labs(
#     x = NULL,
#     y = "Flesch-Kincaid index",
#     color = NULL,
#     linetype = NULL
#   ) +
#   theme_minimal() +
#   theme(legend.position = "bottom")
# 
# saveRDS(b, "data/temp/05_plot.rds")

readRDS("data/temp/05_plot.rds")
```


## Összehasonlítás[^leiro-5]

[^leiro-5]: [@schutzeIntroductionInformationRetrieval2008]
A fentiekben láthattuk az eltéréseket a jobb és a bal oldali beszédeken belül, sőt ugyanahhoz a miniszterelnökhöz tartozó két beszéd között is.
A következőkben `textstat_dist()` és `textstat_simil()` függvények segítségével megvizsgáljuk, valójában mennyire hasonlítanak vagy különböznek ezek a beszédek. Mindkét függvény bemenete `dmf`, melyből először egy súlyozott `dfm`-et készítünk, majd elvégezzük az összehasonlítást először a `jaccard`-féle hasonlóság alapján.

```{r}
mineln_dfm %>%
  dfm_weight("prop") %>%
  quanteda.textstats::textstat_simil(margin = "documents", method = "jaccard")
```

Majd a `textstat_dist()` függvény segítségével kiszámoljuk a dokumentumok egymástól való különbözőségét.

```{r}
mineln_dfm %>%
  quanteda.textstats::textstat_dist(margin = "documents", method = "euclidean")
```

Ezután vizualizálhatjuk is a dokumentumok egymástól való távolságát egy olyan dendogram[^leiro6] segítségével, amely megmutatja nekünk a lehetséges dokumentumpárokat.

[^leiro6]: Olyan ábra, amely hasonlóságaik vagy különbségeik alapján csoportosított objektumok összefüggéseit mutatja meg.

```{r}
dist <- mineln_dfm %>%
  textstat_dist(margin = "documents", method = "euclidean")
```

```{r fig.cap="A dokumentumok csoportosítása a távolságuk alapján"}
hierarchikus_klaszter <- hclust(as.dist(dist))

ggdendro::ggdendrogram(hierarchikus_klaszter)
```

```{r}
mineln_dfm %>%
  textstat_simil(y = mineln_dfm[, c("kormány")], margin = "features", method = "correlation") %>%
  head(n = 10)
```

Arra is van lehetőségünk, hogy a két alkorpuszt hasonlítsuk össze egymással. Ehhez a `textstat_keyness()` függvényt használjuk, melynek a bemenete a `dfm`. A függvény argumentumában a `target =` után kell megadni, hogy mely alkorpusz a viszonyítási alap. Az összehasonlítás eredményét a `textplot_keyness()` függvény segítségével ábrázolhatjuk, ami megjeleníti a két alkorpusz leggyakoribb kifejezéseit.

```{r}
dfm_keyness <- corpus_mineln %>% 
  tokens(remove_punct = TRUE) %>% 
  tokens_remove(pattern = custom_stopwords) %>% 
  dfm() %>% 
  quanteda::dfm_group(partoldal)

result_keyness <- quanteda.textstats::textstat_keyness(dfm_keyness, target = "jobb")
```

```{r fig.cap="A korpuszok legfontosabb kifejezései", message=FALSE, warning=FALSE}
quanteda.textplots::textplot_keyness(result_keyness, color = c("#484848", "#D0D0D0")) +
  xlim(c(-65, 65)) +
  theme(legend.position = c(0.9,0.1))
```

Ha az egyes miniszterelnökök beszédeinek leggyakoribb kifejezéseit szeretnénk összehasonlítani, azt a `textstat_frequency()` függvény segítségével tehetjük meg, melynek bemenete a megtisztított és súlyozott `dfm`. Az összehasonlítás eredményét pedig a `ggplot2` segítségével ábrázolhatjuk is.

```{r,  tidy.opts=list(width.cutoff=80)}
dfm_weighted <- corpus_mineln %>%
  tokens(
    remove_punct = TRUE, 
    remove_symbols = TRUE, 
    remove_numbers = TRUE
    ) %>%
  tokens_tolower() %>% 
  tokens_wordstem(language = "hungarian") %>% 
  tokens_remove(pattern = custom_stopwords) %>% 
  dfm() %>% 
  dfm_weight(scheme = "prop")

freq_weight <- textstat_frequency(dfm_weighted, n = 5, groups = mineln)
```

```{r fig.cap="Leggyakoribb kifejezések a miniszterelnöki beszédekben"}
ggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +
  geom_point() +
  facet_wrap(~ group, scales = "free", ncol = 2) +
  coord_flip() +
  scale_x_continuous(
    breaks = nrow(freq_weight):1,
    labels = freq_weight$feature
  ) +
  labs(
    x = NULL, 
    y = "Relatív szófrekvencia"
    )
```

Mivel a szövegösszehasonlítás egy komplex kutatási feladat, a témával bőbben is foglalkozunk a *[Szövegösszhasonlítás](#similarity)* fejezetben.

## Kulcsszavak kontextusa

Arra is lehetőségünk van, hogy egyes kulcszavakat a korpuszon belül szövegkörnyezetükben vizsgáljunk meg. Ehhez a `kwic()` függvényt használjuk, az argumentumok között a `pattern =` kifejezés után megadva azt a szót, amelyet vizsgálni szeretnénk, a `window =` után pedig megadhatjuk, hogy az adott szó hány szavas környezetére vagyunk kíváncsiak.

```{r tidy.opts=list(width.cutoff=80)}
corpus_mineln %>% 
  tokens() %>% 
  quanteda::kwic(
    pattern = "válság*", 
    valuetype = "glob",
    window = 3, 
    case_insensitive = TRUE
    ) %>%
  head(5)
```
