[["index.html", "Szövegbányászat és mesterséges intelligencia R-ben Üdvözöljük!", " Szövegbányászat és mesterséges intelligencia R-ben Sebk Miklós, Ring Orsolya, Máté Ákos 2021-04-29 11:01:29 Üdvözöljük! Könyvünk bevezeti az érdekldket a szövegbányászat és a mesterséges intelligencia társadalomtudományi alkalmazásának speciális problémáiba. Támaszkodva a Sebk Miklós által szerkesztett Kvantitatív szövegelemzés és szövegbányászat a politikatudományban (LHarmattan, 2016) cím kötet elméleti bevezetésére, ezúttal a társadalomtudományi elemzések során használható kvantitatív szövegelemzés legfontosabb gyakorlati feladatait vesszük sorra. A szövegek adatként való értelmezése (text as data) és kvantitatív elemzése, avagy a szövegbányászat (text mining) a nemzetközi társadalomtudományi kutatások egyik leggyorsabban fejld irányzata. A szövegbányászat emellett a társadalomtudósok számára az egyik legnyilvánvalóbb belépési pont a mesterséges intelligenciát, ezen belül is gépi tanulást alkalmazó kutatások területére. A magyar tankönyvpiacon elsként ismertetünk lépésrl-lépésre a nemzetközi társadalomtudományban használatos olyan kvantitatív szövegelemzési eljárásokat, mint a névelemfelismerés, a véleményelemzés, a topikmodellezés, illetve a szövegek felügyelt tanulásra épül osztályozása. A módszereink bemutatására szolgáló elemzéseket az egyik leggyakrabban használt programnyelv, az R segítségével végeztük el. A kötet anyaga akár minimális programozási ismerettel is elsajátítható, így teljesen kezdk számára is ajánlott. A hazai olvasók érdekldését szem eltt tartva példáink dönt többsége új, magyar nyelv korpuszokra épül, melyek alapján megismerhetk a magyar nyelv kvantitatív szövegelemzés módozatai. "],["intro.html", "1 Bevezetés 1.1 A kötet témái 1.2 A kötet témái 1.3 Használati utasítás 1.4 A HunMineR használata 1.5 Köszönetnyilvánítás", " 1 Bevezetés 1.1 A kötet témái A szövegek adatként való értelmezése (text as data) és kvantitatív elemzése (quantitative text analysis), avagy a szövegbányászat (text mining) a nemzetközi társadalomtudományi kutatások egyik leggyorsabban fejld irányzata. A szövegek és más kvalitatív adatok (filmek, képek) elemzése annyiban különbözik a mennyiségi (kvantitatív) adatokétól, hogy nyers formájukban még nem alkalmasak statisztikai, illetve ökonometriai elemzésre. Ezért van szükség az ezzel összefügg módszertani problémák speciális tárgyalására. Jelen kötet bevezeti az érdekldket a szövegbányászat és a mesterséges intelligencia társadalomtudományi alkalmazásának ilyen speciális problémáiba, valamint ezek gyakorlati megoldásába. Közvetlen elzménynek tekinthet a témában a Sebk Miklós által szerkesztett Kvantitatív szövegelemzés és szövegbányászat a politikatudományban címmel megjelent könyv, amely a magyar tudományos diskurzusban kevésbé bevett alapfogalmakat és eljárásokat mutatta be (Sebk 2016). A hangsúly az elméleten volt, bár számos fejezet foglalkozott konkrét kódrészletek elemzésével. Míg az elz kötet az egyes kódolási eljárásokat, illetve ezek kutatásmódszertani elnyeit és hátrányait ismertette, ezúttal a társadalomtudományi elemzések során használható kvantitatív szövegelemzés legfontosabb gyakorlati feladatait vesszük sorra. Könyvünk a magyar tankönyvpiacon elsként ismerteti lépésrl-lépésre a nemzetközi társadalomtudományban használatos kvantitatív szövegelemzési eljárásokat. A módszereink bemutatására szolgáló elemzéseket az R programnyelv segítségével végeztük el, mely a nemzetközi társadalomtudományi vizsgálatok során egyik leggyakrabban használt környezet a Python mellett. Mivel a szövegbányászat nyelve az angol, bár igyekeztünk, ahol lehetséges volt igyekeztünk magyar szakkifejezéseket használni, de ilyenkor is minden esetben megadtuk azok angol megfeleljét. Kivételt képeznek azok az esetek, ahol nincs használatban megfelel magyar terminológia, ezeknél megtartottuk az angol kifejezéseket, de magyarázattal láttuk el azokat. 1.2 A kötet témái Az Olvasó a két kötet együttes használatával olyan ismeretek birtokába jut, melyek révén képes lesz alkalmazni a kvantitatív szövegelemzés és szövegbányászat legalapvetbb eljárásait saját kutatásaiban. Deduktív vagy induktív felfedez logikája szerint dönthet az adatelemzés módjáról, és a felkínált menübl kiválaszthatja a kutatási tervéhez legjobban illeszked megoldásokat. A bemutatott konkrét példák segítségével pedig akár reprodukálhatja is ezen eljárásokat saját kutatásában. Mindezt a kötet fejezeteiben bséggel tárgyalt R-scriptek (kódok) részletes leírása is segíti. Ennek alapján a kötet két f célcsoportja a társadalomtudományi kutatói és felsoktatási hallgatói-oktatói közösség. Az oktatási alkalmazást segítheti a fontosabb fogalmak magyar és angol nyelv szószedete, valamint több helyen a további olvasásra ajánlott szakirodalom felsorolása. A kötet honlapján (www.hunminer.hu) közvetlenül is elérhetek a felhasznált adatbázisok és kódok. Kötetünk négy logikai egységbl épül fel. Az els négy fejezet bemutatja azokat a fogalmakat és eljárásokat, amelyek elengedhetetlenek egy szövegbányászati kutatás során, valamint itt kerül sor a szöveges adatforrásokkal való munkafolyamat ismertetésére, a szövegelkészítés és a korpuszépítés technikáinak bemutatására. A második blokkban az egyszerbb elemzési módszereket tárgyaljuk, így a leíró statisztikák készítését, a szótár alapú elemzést, valamint érzelemelemzést. A kötet harmadik blokkját a mesterséges intelligencia alapú megközelítéseknek szenteljük, melynek során az olvasó a felügyelt és felügyelet nélküli tanulás fogalmával ismerkedhet meg. A felügyelet nélküli módszerek közül a topic modellezést, szóbeágyazást és a szövegskálázást mutatjuk be, a felügyelt elemzések közül pedig az osztályozással foglalkozunk részletesebben. Végezetül kötetünket egy függelék zárja, melyben a kezd RStudió felhasználóknak adunk gyakorlati iránymutatást a programfelülettel való megismerkedéshez, használatának elsajátításához. 1.3 Használati utasítás A könyv célja, hogy keresztmetszeti képet adjon a szövegbányászat R programnyelven használatos eszközeirl. A fejezetekben ezért a magyarázó szövegben maga az R kód is megtalálható, illetve láthatóak a lefuttatott kód eredményei is. Az alábbi példában a sötét háttér az R környezetet jelöli, ahol az R kód bettípusa is eltér a fszövegétl. A kód eredményét pedig a #&gt; kezdet sorokba szedtük, ezzel szimulálva az R console ablakát. # példa R kód 1 + 1 ## [1] 2 Az egyes fejezetekben szerepl kódrészleteket egymás utáni sorrendben bemásolva és lefuttatva a saját R környezetünkben tudjuk reprodukálni a könyvben szerepl technikákat. A Függelékben részletesebben is foglalkozunk az R és az RStudio beállításaival, használatával. Az ajánlott R minimum verzió a 4.0.0, illetve az ajánlott minimum RStudio verzió az 1.4.0000.1 A könyvhöz tartozik egy HunMineR R csomag is, amely tartalmazza az egyes fejezetekben használt összes adatbázist, így az adatbeviteli problémákat elkerülve lehet gyakorolni a szövegbányászatot. A könyv megjelenésekor a csomag még nem került be a központi R CRAN csomag repozitóriumába, hanem a poltextLAB GitHub repozitóriumából tölthet le. A könyvben szerepl ábrák nagy része a ggplot2 csomaggal készült a theme_set(theme_light()) opció beállításával a háttérben. Ez azt jelenti, hogy az ábrákat elállító kódok a theme_light() sort nem tartalmazzák, de a tényleges ábrán már megjelennek a tematikus elemek. 1.4 A HunMineR használata A Windows rendszert használóknak elször az installr csomagot kell telepíteni, majd annak segítségével letölteni az Rtools nev programot (az OS X és Linux rendszerek esetében erre a lépésre nincs szükség). A lenti kód futtatásával ezek a lépések automatikusan megtörténnek. # az installr csomag letöltése és installálása install.packages(&quot;installr&quot;) # az Rtools.exe fájl letöltése és installálása installr::install.Rtools() Ezt követen a devtools csomagban található install_github paranccsal tudjuk telepíteni a HunMineR csomagot, a lenti kód lefuttatásával. # A devtools csomag letöltése és installálása install.packages(&quot;devtools&quot;) # A HunMineR csomag letöltése és installálása devtools::install_github(&quot;poltextlab/HunMineR&quot;) Ebben a fázisban a data függvénnyel tudjuk megnézni, hogy pontosan milyen adatbázisok szerepelnek a csomagban, illetve ugyanitt megtalálható az egyes adatbázisok részletes leírása. Ha egy adatbázisról szeretnénk többet megtudni, akkor a kiegészít információkat ?adatbazis_neve megoldással tudjuk megnézni.2 # A HunMineR csomag betöltése library(HunMineR) # csomagban lév adatok listázása data(package = &quot;HunMineR&quot;) # A miniszterelnöki beszédek minta adatbázisának részletei ?data_miniszterelnokok 1.5 Köszönetnyilvánítás Jelen kötet az ELKH Társadalomtudományi Kutatóközpont poltextLAB szövegbányászati kutatócsoportja (http://poltextlab.com/) mhelyében készült. A kötet fejezetei Sebk Miklós, Ring Orsolya és Máté Ákos közös munkájának eredményei. Az Alapfogalmak, illetve a Szövegösszehasonlítás fejezetekben társszerz volt Székely Anna. A kézirat a szerzk több éves oktatási gyakorlatára, a hallgatóktól kapott visszajelzésekre építve készült el. Köszönjük a Bibó Szakkollégiumban (2021), a Rajk Szakkollégiumban (20192021), valamint a Széchenyi Szakkollégiumban (2019) tartott féléves, valamint a Corvinus Egyetemen és a Társadalomtudományi Kutatóközpontban tartott rövidebb képzési alkalmak résztvevinek visszajelzéseit. Köszönjük a projekt gyakornokainak, Czene-Joó Máténak, Kaló Eszternek, Meleg Andrásnak, Lovász Dorottyának, Nagy Orsolyának, valamint kutatás asszisztenseinek, Balázs Gergnek, Gelányi Péternek és Lancsár Eszternek a kézirat végleges formába öntése során nyújtott segítséget. Külön köszönet illeti a Társadalomtudományi Kutatóközpont Comparative Agendas Project (https://cap.tk.hu/hu) kutatócsoportjának tagjait, kiemelten Boda Zsoltot, Molnár Csabát és Pokornyi Zsanettet a kötetben használt korpuszok sokéves elkészítéséért. Köszönettel tartozunk az egyes fejezetek alapjául szolgáló elemzések és publikációk szerzinek, Barczikay Tamásnak, Berki Tamásnak, Kacsuk Zoltánnak, Kubik Bálintnak, Molnár Csabának és Szabó Martina Katalinnak. Köszönjük a szakmai lektor hasznos megjegyzéseit, Fedinec Csilla nyelvi lektor alapos munkáját, valamint a Typotex Kiadó rugalmasságát és színvonalas közremködését a könyv kiadásában! Végül, de nem utolsósorban hálásak vagyunk a kötet megvalósulásához támogatást nyújtó szervezeteknek és ösztöndíjaknak: az MTA Könyvkiadási Alapjának, a Társadalomtudományi Kutatóközpont Könyvtámogatási Alapjának, a Nemzeti Kutatási, Fejlesztési és Innovációs Hivatalnak (NKFIH FK 123907, NKFIH FK 129018), az MTA Bolyai János Kutatási Ösztöndíjának. A kötet alapjául szolgáló kutatást, amelyet a Társadalomtudományi Kutatóközpont valósított meg, az Innovációs és Technológiai Minisztérium és a Nemzeti Kutatási, Fejlesztési és Innovációs Hivatal támogatta a Mesterséges Intelligencia Nemzeti Laboratórium keretében. Az R Windows, OS X és Linux változatai itt érhetek el: https://cloud.r-project.org/. Az RStudio pedig innen érhet el: https://www.rstudio.com/products/rstudio/download/. Többek között az adat forrása, a változók részletes leírása, illetve az adatbázis mérete is megtalálható így. "],["alapfogalmak.html", "2 Alapfogalmak 2.1 Elméleti alapok 2.2 Fogalmi alapok 2.3 A szövegbányászat alapelvei", " 2 Alapfogalmak 2.1 Elméleti alapok A szövegek géppel való feldolgozása és elemzése módszertanának számos megnevezése létezik. A szövegelemzés, kvantitatív szövegelemzés, szövegbányászat, természetes nyelvfeldolgozás, automatizált szövegelemzés, automatizált tartalomelemzés és hasonló fogalmak között nincs éles tartalami különbség. Ezek a kifejezések jellemzen ugyanarra az általánosabb kutatási irányra reflektálnak, csupán hangsúlybeli eltolódások vannak köztük, így gyakran szinonimaként is használják ket. A szövegek gépi feldolgozásával foglalkozó tudományág a Big Data forradalom részeként kezdett kialakulni, melyet az adatok egyre nagyobb és diverzebb tömegének elérhet és összegyjthet jellege hívott életre. Ennek megfelelen az adattudomány számos különböz adatforrás, így képek, videók, hanganyagok, internetes keresési adatok, telefonok lokációs adatai és megannyi különböz információ feldolgozásával foglalkozik. A szöveg is egy az adatbányászat érdekldési körébe tartozó számos adattípus közül, melynek elemzésére külön kutatási irány alakult ki. Mivel napjainkban minden másodpercben óriási mennyiség szöveg keletkezik és válik hozzáférhetvé az interneten, egyre nagyobb az igény az ilyen jelleg források és az emberi nyelv automatizált feldolgozására. Ebbl adódóan az elemzési eszköztár is egyre szélesebb kör és egyre szofisztikáltabb, így a tartalomelemzési és szövegbányászati ismeretekkel bíró elemzk számára rengeteg értékes információ kinyerhet. Ezért a szövegbányászat nemcsak a társadalomtudósok számára izgalmas kutatási irány, hanem gyakran hasznosítják üzleti célokra is. Gondoljunk például az online sajtótermékekre, az ezekhez kapcsolódó kommentekre vagy a politikusok beszédéire. Ezek mind-mind hatalmas mennyiségben rendelkezésre állnak, hasznosításukhoz azonban képesnek kell lenni ezeket a szövegeket összegyjteni, megfelel módon feldolgozni és kiértékelni. A könyv ebben is segítséget nyújt az Olvasónak. Mieltt azonban az adatkezelés és az elemzés részleteire rátérnénk, érdemes végigvenni néhány elvi megfontolást, melyek nélkülözhetetlenek a leend elemz számára az etikus, érvényes és eredményes szövegbányászati kutatások kivitelezéséhez. A nagy mennyiségben rendelkezésre álló szöveges források kiváló kutatási terepet kínálnak a társadalomtudósok számára megannyi vizsgálati kérdéshez, azonban fontos tisztában lenni azzal, hogy a mindenki által elérhet adatokat is meglehetsen körültekinten, etikai szempontok figyelembevételével kell használni. Egy másik szempont, amelyet érdemes szem eltt tartani, mieltt az ember fejest ugrana az adatok végtelenjébe, a 3V elve: volume, velocity, variety vagyis az adatok mérete, a keletkezésük sebessége és azok változatossága (Brady 2019). Ezek mind olyan tulajdonságok, amelyek az adatelemzt munkája során más (és sok esetben több vagy nagyobb) kihívások elé állítják, mint egy hagyományos statisztikai elemzés esetében. A szövegbányászati módszerek abban is eltérnek a hagyományos társadalomtudományi elemzésektl, hogy  az adattudományokba visszanyúló gyökerei miatt  jelents teret nyit az induktív (empiricista) kutatások számára a deduktív szemlélettel szemben. A deduktív kutatásmódszertani megközelítés esetén a kutató elre meghatározza az alkalmazandó fogalomrendszert, és azokat az elvárásokat, amelyek teljesülése esetén sikeresnek tekinti az elemzést. Az adattudományban az ilyen megközelítés a felügyelt tanulási feladatokat jellemzi, vagyis azokat a feladatokat, ahol ismert az elvárt eredmény. Ilyen például egy osztályozási feladat, amikor újságcikkeket szeretnénk különböz témakörökbe besorolni. Ebben az esetben az adatok egy részét általában kézzel kategorizáljuk, és a gépi eljárás sikerességét ehhez viszonyítjuk. Mivel az ideális eredmény (osztálycímke) ismert, a gépi teljesítmény könnyen mérhet (például a pontosság, a gép által sikeresen kategorizált cikkek százalékában kifejezve). Az induktív megoldás esetében kevésbé egyértelm a gépi eljárás teljesítményének mérése, hiszen a rejtett mintázatok feltárását várjuk az algoritmustól, emiatt nincsenek elre meghatározott eredmények sem, amelyekhez viszonyíthatjuk a teljesítményt. Az adattudományban az ilyen feladatokat hívják felügyelet nélküli tanulásnak. Ide tartozik a klaszterelemzés, vagy a topic modellezés, melynek esetén a kutató csak azt határozza meg, hány klasztert, hány témát szeretne kinyerni, a gép pedig létrehozza az egymáshoz leghasonlóbb csoportokat. Értelemszeren itt a kutatói validálás jóval nagyobb hangsúlyt kap, mint a deduktív megközelítés esetében. Egy harmadik, középutas megoldás a megalapozott elmélet megközelítése, mely ötvözi az induktív és a deduktív módszer elnyeit. Ennek során a kutató kidolgoz egy laza elméleti keretet, melynek alapján elvégzi az elemzést, majd az eredményeket figyelembe véve finomít a fogalmi keretén, és újabb elemzést futtat, addig folytatva ezt az iterációt, amíg a kutatás eredményeit kielégítnek nem találja. A szövegbányászati elemzéseket kategorizálhatjuk továbbá a gépi hozzájárulás mértéke szerint. Ennek megfelelen megkülönböztethetünk kézi, géppel támogatott és gépi eljárásokat. Mindhárom megközelítésnek megvan a maga elnye. A kézi megoldások esetén valószínbb, hogy azt mérjük a szövegünkben, amit mérni szeretnénk (például bizonyos szakpolitikai tartalmat), ugyanakkor ez id- és költségigényes. A gépi eljárások ezzel szemben költséghatékonyak és gyorsak, de fennáll a veszélye, hogy nem azt mérjük, amit eredetileg mérni szerettünk volna (ennek megállapításában ismét a validálás kap kulcsszerepet). Továbbá lehetséges kézzel támogatott gépi megoldások alkalmazása, ahol a humán és a gépi elemzés ideális arányának megtalálása jelenti a f kihívást. 2.2 Fogalmi alapok Miután áttekintettük a szövegbányászatban használatos elméleti megközelítéseket, érdemes tisztázni a fogalmi alapokat is. A szövegbányászat szempontjából a szöveg is egy adat. Az elemzéshez használatos strukturált adathalmazt korpusznak nevezzük. A korpusz az összes szövegünket jelöli, ennek részegységei a dokumentumok. Ha például a Magyar Nemzet cikkeit kívánjuk elemezni, a kiválasztott idszak összes cikke lesz a teljes korpuszunk, az egyes cikkek pedig a dokumentumaink. Az elemzés mindig egy meghatározott tématerületre (domain-re) koncentrál. E tématerület utalhat a nyelvre, amelyen a szövegek íródtak, vagy a specifikus tartalomra, amelyet vizsgálunk, de mindenképpen meghatározza a szöveg szókészletével kapcsolatos várakozásainkat. Más lesz tehát a szóhasználat egy bulvárlap cikkeiben, mint egy tudományos szaklap cikkeiben, aminek elssorban akkor van jelentsége, ha szótár alapú elemzéseket készítünk. A szótár alapú elemzések során olyan szószedeteket hozunk létre, amelyek segíthetnek a kutatásunk szempontjából érdekes témák vagy tartalmak azonosításában. Így például létrehozhatunk pozitív és negatív szótárakat, vagy a gazdasági és a külpolitikai témákhoz kapcsolódó szótárakat, melyek segíthetnek azonosítani, hogy adott dokumentum inkább gazdasági vagy inkább külpolitikai témákat tárgyal. Léteznek elre elkészített szótárak  angol nyelven például a Bing Liu által fejlesztett szótár egy jól ismert és széles körben alkalmazható példa (Liu 2010) , azonban fontos fejben tartani, hogy a vizsgált téma specifikus nyelvezete jellemzen meghatározza azt, hogy egy-egy szótárba milyen kifejezéseknek kellene kerülniük. Már említettük, hogy egy szövegbányászati elemzés során a szöveg is adatként kezelend. Tehát hasonló módon gondolhatunk az elemzend szövegeinkre, mint egy statisztikai elemzésre szánt adatbázisra, annak csupán, reprezentációja tér el az utóbbitól. Tehát míg egy statisztikai elemzésre szánt táblázatban elssorban számokat és adott esetben kategorikus változókat reprezentáló karakterláncokat (stringeket)  például férfi/n, falu/város  találunk, addig a szöveges adatokban els ránézésre nem tnik ki gépileg értelmezhet struktúra. Ahhoz, hogy a szövegeink a gépi elemzés számára feldolgozhatóvá váljanak, annak reprezentációját kell megváltoztatni, vagyis strukturálatlan adathalmazból strukturált adathalmazt kell létrehozni, melyet jellemzen a szövegek mátrixszá alakításával teszünk meg. A mátrixszá alakítás els hallásra bonyolult eljárás benyomását keltheti, azonban a gyakorlatban egy meglehetsen egyszer transzformációról van szó, melynek eredményeként a szavakat számokkal reprezentáljuk. A könnyebb megértés érdekében vegyük az alábbi példát: tekintsük a három példamondatot a három elemzend dokumentumnak, ezek összességét pedig a korpuszunknak. 1. Az Európai Unió 27 tagországának egyike Magyarország. 2. Magyarország 2004-ben csatlakozott az Európai Unóhoz. 3. Szlovákia, akárcsak Magyarország, 2004-ben lett ez Európai Unió tagja. A példamondatok dokumentum-kifejezés mátrixsza az alábbi táblázat szerint fog kinézni. Vegyük észre azt is, hogy több olyan kifejezés van, melyek csak ragozásukban térnek el: Unió, Unióhoz; tagja, tagjának. Ezeket a kifejezéseket a kutatói szándék függvényében azonos alakúra hozhatjuk, hogy egy egységként jelenjenek meg. Az elemzések többségében a szövegelkészítés egyik kiinduló lépése a szótövesítés vagy a lemmatizálás, elbbi a szavak toldalékainak levágását jelöli, utóbbi a szavak szótári alakra való visszaalakítását. A ragozás eltávolítását illeten elöljáróban annyit érdemes megjegyezni, hogy az agglutináló, vagyis ragasztó nyelvek esetén, mint amilyen a magyar is, a toldalékok eltávolítása gyakran igen komoly kihívást jelent. Nem csak a toldalékok formája lehet igen sokféle, de az is elfordulhat, hogy a tszó nem egyezik meg a toldalék levágásával keletkez szótvel. Ilyen például a vödröt kifejezés, melynek szótöve a vödr, de a nyelvtanilag helyes tszó a vödör. Hasonlóan a majmok kifejezés esetén a szót a majm lesz, míg a nyelvtanilag helyes tszó a majom. Emiatt a toldalékok levágását a magyar nyelv szövegek esetén megfelel körültekintéssel kell végezni. Table 2.1: Dokumentum-kifejezés mátrix három példamondattal Dokumentum száma 27 2004-ben akárcsak az csatlakozott egyike Európai lett Magyarország Szlovákia tagja tagjának Unió Unióhoz 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 2 0 1 0 1 1 0 1 0 1 0 0 0 0 1 3 0 1 1 1 0 0 1 1 1 1 1 0 1 0 A dokumentum-kifejezés mátrixban minden dokumentumot egy vektor (értsd: egy sor) reprezentál, az eltér kifejezések pedig külön oszlopokat kapnak. Tehát a fenti példában minden dokumentumunk egy 14 elem vektorként jelenik meg, amelynek elemei azt jelölik, hogy milyen gyakran szerepel az adott kifejezés a dokumentumban. A dokumentum-kifejezés mátrixok egy jellemz tulajdonsága, hogy igen nagy dimenziókkal rendelkezhetnek (értsd: sok sorral és sok oszloppal), hiszen minden kifejezést külön oszlopként reprezentálnak. Egy sok dokumentumból álló vagy egy témák tekintetében változatos korpusz esetében a kifejezés mátrix elemeinek jelents része 0 lesz, hiszen számos olyan kifejezés fordul el az egyes dokumentumokban, amelyek más dokumentumban nem szerepelnek. A sok nullát tartalmazó mátrixot hívjuk ritka mátrixnak. Az adatok jobb kezelhetsége érdekben a ritka mátrixot valamilyen dimenzióredukciós eljárással sr mátrixszá lehet alakítani (például a nagyon ritka kifejezések eltávolításával vagy valamilyen súlyozáson alapuló eljárással). 2.3 A szövegbányászat alapelvei A módszertani fogalmak tisztázásást követen néhány elméleti megfontolást osztanánk meg a Grimmer és Steward (2013) által megfogalmazott alapelvek nyomán, melyek hasznos útravalóul szolgálhatnak a szövegbányászattal ismerked kutatók számára. 1. A szövegbányászat rossz, de hasznos Az emberi nyelv egy meglehetsen bonyolult rendszer, így egy szöveg jelentésének, érzelmi telítettségének különböz olvasók általi értelmezése meglehetsen eltér lehet, így nem meglep, hogy egy gép sok esetben csak korlátozott eredményeket képes felmutatni ezen feladatok teljesítésében. Ettl függetlenül nem elvitatható a szövegbányászati modellek hasznossága, hiszen olyan mennyiség szöveg válik feldolgozhatóvá, ami gépi támogatás nélkül elképzelhetetlen lenne, mindemellett azonban nem lehet megfeledkezni a módszertan korlátairól sem. 2. A kvantitatív modellek kiegészítik az embert, nem helyettesítik azt A kvantitatív eszközökkel történ elemzés nem szünteti meg a szövegek elolvasásának szükségességét, hiszen egészen más információk kinyerését teszi lehetvé, mint egy kvantitatív megközelítés. Emiatt a kvantitatív szövegelemzés talán legfontosabb kihívása, hogy a kutató megtalálja a gépi és a humán erforrások együttes hasznosításának legjobb módját. 3. Nincs legjobb modell Minden kutatáshoz meg kell találni a leginkább alkalmas modellt a kutatási kérdés, a rendelkezésre álló adatok és a kutatói szándék alapján. Gyakran különböz eljárások kombinálása vezethet egy specifikus probléma legjobb megoldásához. Azonban minden esetben az eredmények értékelésére kell támaszkodni, hogy megállapíthassuk egy modell teljesítményét adott problémára és szövegkorpuszra nézve. 4. Validálás, validálás, validálás! Mivel az automatizált szövegelemzés számos esetben jelentsen lecsökkenti az elemzéshez szükséges idt és energiát, csábító lehet a gondolat, hogy ezekhez a módszerekhez forduljon a kutató, ugyanakkor nem szabad elfelejteni, hogy az elemzés csupán a kezdeti lépés, hiszen a kutatónak validálnia kell az eredményeket ahhoz, hogy valóban megbízható következtetésekre jussunk. Az érvényesség-vizsgálat (validálás) lényege egy felügyelet nélküli modell esetén  ahol az elvárt eredmények nem ismertek, így a teljesítmény nem tesztelhet , hogy meggyzdjünk: egy felügyelt modellel (olyan modellel, ahol az elvárt eredmény ismert, így ellenrizhet) egyenérték eredményeket hozzon. Ennek az elvárásnak a teljesítése gyakran nem egyszer, azonban az eljárások alapos kiértékelést (validálást) nélkülöz alkalmazása meglehetsen kétesélyes eredményekhez vezethet, emiatt érdemes megfelel alapossággal eljárni az érvényesség-vizsgálat során. "],["adatkezeles.html", "3 Az adatkezelés R-ben 3.1 Adatok importálása 3.2 Adatok exportálása 3.3 A pipe operátor 3.4 Mveletek adattáblákkal 3.5 Munka karakter vektorokkal6", " 3 Az adatkezelés R-ben 3.1 Adatok importálása library(readr) library(dplyr) library(gapminder) library(stringr) Az adatok importálására az R alapfüggvénye mellett több csomag is megoldást kínál. Ezek közül a könyv írásakor a legnépszerbbek a readr és a rio csomagok. A szövegek különböz karakterkódolásának problémáját tapasztalataink szerint a legjobban a readr csomag read_csv() függvénye kezeli, ezért legtöbbször ezt fogjuk használni a .csv állományok beolvasására. Amennyiben kihasználjuk az RStudio projekt opcióját (lásd a Függelékben) akkor elegend csak az elérni kívánt adatok relatív elérési útját megadni (relative path). Ideális esetben az adataink egy csv fájlban vannak (comma separated values), ahol az egyes értékeket vesszk (vagy egyéb speciális karakterek) választják el. Ez esetben a read_delim() függvényt használjuk. A beolvasásnál egybl el is tároljuk az adatokat egy objektumban. A sep = opcióval tudjuk a szeparátor karaktert beállítani, mert elfordulhat, hogy vessz helyett pontosvessz tagolja az adatainkat. df &lt;- read_csv(&quot;data/adatfile.csv&quot;) Az R képes linkrl letölteni fájlokat, elég megadnunk egy mköd elérési útvonalat. df_online &lt;- read.csv(&quot;https://www.qta.tk.mta.hu/adatok/adatfile.csv&quot;) Az R csomag ökoszisztémája kellen változatos ahhoz, hogy gyakorlatilag bármilyen inputtal meg tudjon birkózni. Az Excel fájlokat a readxl csomagot használva tudjuk betölteni a read_excel() függvény használatával.lásd ehhez a Függeléket A leggyakoribb statisztikai programok formátumait pedig a haven csomag tudja kezelni (például Stata, Spss, SAS). A szintaxis itt is hasonló: read_stata, read_spss, read_sas. A nagy mennyiség szöveges dokumentum (a legyakrabban elforduló kiterjesztések: .txt, .doc, .pdf, .json, .csv, .xml, .rtf, .odt) betöltésére a legalkalmasabb a readtext csomag. Az alábbi példa azt mutatja be, hogyan tudjuk beolvasni egy adott mappából az összes .txt kiterjesztés fájlt anélkül, hogy egyenként kellene megadnunk a fájlok neveit. A kódsorban szerepl * karakter ebben a környezetben azt jelenti, hogy bármilyen fájl, ami .txt-re végzdik. Amennyiben a fájlok nevei tartalmaznak valamilyen metaadatot, akkor ezt is be tudjuk olvasni a betöltés során. Ilyen metaadat lehet például egy parlamenti felszólalásnál a felszólaló neve, a beszéd ideje, a felszólaló párttagsága (például: kovacsjanos_1994_fkgp.txt). df_text &lt;- readtext( &quot;data/*.txt&quot;, docvarsfrom = &quot;filenames&quot;, dvsep = &quot;_&quot;, docvarnames = c(&quot;nev&quot;, &quot;ev&quot;, &quot;part&quot;) ) 3.2 Adatok exportálása Az adatainkat R-bl a write.csv()-vel exportálhatjuk a kívánt helyre, .csv formátumba. Az openxlsx csomaggal .xls és .xlsx Excel formátumokba is tudunk exportálni. Az R rendelkezik saját, .Rds és .Rda kiterjesztés, tömörített fájlformátummal. Mivel ezeket csak az R-ben nyithatjuk meg, érdemes a köztes, hosszadalmas számítást igényl lépések elmentésére használni, a saveRDS() és a save() parancsokkal. 3.3 A pipe operátor Az úgynevezett pipe operátor alapjaiban határozta meg a modern R fejldését és a népszer csomag ökoszisztéma, a tidyverse, egyik alapköve. Úgy gondoljuk, hogy a tidyverse és a pipe egyszerbbé teszi az R használatának elsajátítását, ezért mi is erre helyezzük a hangsúlyt.3 Vizuálisan a pipe operátor így néz ki: %&gt;%, és arra szolgál, hogy a kódban több egymáshoz kapcsolódó mveletet egybefzzünk.4 Technikailag a pipe a bal oldali elemet adja meg a jobb oldali függvény els argumentumának. A lenti példa ugyanazt a folyamatot írja le az alap R (base R), illetve a pipe használatával.5 Miközben a kódot olvassuk, érdemes a pipe-ot és aztán-nak fordítani. reggeli(oltozkodes(felkeles(ebredes(en, idopont = &quot;8:00&quot;), oldal = &quot;jobb&quot;), nadrag = TRUE, ing = TRUE)) en %&gt;% ebredes(idopont = &quot;8:00&quot;) %&gt;% felkeles(oldal = &quot;jobb&quot;) %&gt;% oltozkodes(nadrag = TRUE, ing = TRUE) %&gt;% reggeli() A fenti példa is jól mutatja, hogy a pipe a bal oldali elemet fogja a jobb oldali függvény els elemének berakni. A fejezet további részeiben még bven fogunk gyakorlati példát találni a pipe használatára. Mivel az itt bemutatott példák az alkalmazásoknak csak egy relatíve szk körét mutatják be, érdemes átolvasni a csomagokhoz tartozó dokumentációt, illetve ha van, akkor tanulmányozni a mködést demonstráló bemutató oldalakat is. 3.4 Mveletek adattáblákkal Az adattábla (data frame) az egyik leghasznosabb és leggyakrabban használt adattárolási mód az R-ben (a részletesebb leírás a Függelékben található). Ebben az alfejezetben azt mutatjuk be a dplyr és gapminder csomagok segítségével, hogyan lehet vele hatékonyan dolgozni. A dplyr az egyik legnépszerbb R csomag, a tidyverse része. A gapminder csomag pedig a példa adatbázisunkat tartalmazza, amiben a világ országainak különböz gazdasági és társadalmi mutatói találhatók. A sorok (megfigyelések) szréséhez a dplyr csomag filter() parancsát használva lehetségünk van arra, hogy egy vagy több kritérium alapján szkítsük az adatbázisunkat. A lenti példában azokat a megfigyeléseket tartjuk meg, ahol az év 1962 és a várható élettartam több mint 72 év. gapminder %&gt;% filter(year == 1962, lifeExp &gt; 72) #&gt; # A tibble: 5 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Denmark Europe 1962 72.4 4646899 13583. #&gt; 2 Iceland Europe 1962 73.7 182053 10350. #&gt; 3 Netherlands Europe 1962 73.2 11805689 12791. #&gt; 4 Norway Europe 1962 73.5 3638919 13450. #&gt; 5 Sweden Europe 1962 73.4 7561588 12329. Ugyanígy leválogathatjuk az adattáblából az adatokat akkor is, ha egy karakter változó alapján szeretnénk szrni. gapminder %&gt;% filter(country == &quot;Sweden&quot;, year &gt; 1990) #&gt; # A tibble: 4 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Sweden Europe 1992 78.2 8718867 23880. #&gt; 2 Sweden Europe 1997 79.4 8897619 25267. #&gt; 3 Sweden Europe 2002 80.0 8954175 29342. #&gt; 4 Sweden Europe 2007 80.9 9031088 33860. Itt tehát az adattábla azon sorait szeretnénk látni, ahol az ország megegyezik a Sweden\" karakterlánccal, az év pedig 1990 utáni. A select() függvény segítségével válogathatunk oszlopokat a data frame-bl. A változók kiválasztására több megoldás is van. A dplyr csomag tartalmaz apróbb kisegít függvényeket, amik megkönnyítik a nagy adatbázisok esetén a változók kiválogatását a nevük alapján. Ezek a függvények a contains(), starts_with(), ends_with(), matches(), és beszédesen arra szolgálnak, hogy bizonyos nev változókat ne kelljen egyenként felsorolni. A select()-en belüli változó sorrend egyben az eredmény data frame változójának sorrendjét is megadja. A negatív kiválasztás is lehetséges, ebben az esetben egy - jelet kell tennünk a nem kívánt változó(k) elé (pl.: select(df, year, country, -continent). gapminder %&gt;% select(contains(&quot;ea&quot;), starts_with(&quot;co&quot;), pop) #&gt; # A tibble: 1,704 x 4 #&gt; year country continent pop #&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 1952 Afghanistan Asia 8425333 #&gt; 2 1957 Afghanistan Asia 9240934 #&gt; 3 1962 Afghanistan Asia 10267083 #&gt; 4 1967 Afghanistan Asia 11537966 #&gt; 5 1972 Afghanistan Asia 13079460 #&gt; 6 1977 Afghanistan Asia 14880372 #&gt; 7 1982 Afghanistan Asia 12881816 #&gt; 8 1987 Afghanistan Asia 13867957 #&gt; 9 1992 Afghanistan Asia 16317921 #&gt; 10 1997 Afghanistan Asia 22227415 #&gt; # ... with 1,694 more rows Az így kiválogatott változókból létrehozhatunk és objektumként eltárolhatunk egy új adattáblát, amivek azután tovább dolgozhatunk, vagy kiírathatjuk például .csv fájlba, vagy elmenthetjük a saveRDS segítségével. gapminder_select &lt;- gapminder %&gt;% select(contains(&quot;ea&quot;), starts_with(&quot;co&quot;), pop) write.csv(gapminder_select, &quot;gapminder_select.csv&quot;) saveRDS(gapminder_select, &quot;gapminder_select.Rds&quot;) A saveRDS segítségével elmentett fájlt késbb a readRDS() függvénnyel olvashatjuk be, majd onnan folytathatjuk a munkát, ahol korábban abbahagytuk. readRDS(&quot;gapminder_select.Rds&quot;) Az elemzési munkafolyamat elkerülhetetlen része, hogy új változókat hozzunk létre, vagy a meglévket módosítsuk. Ezt a mutate()-el tehetjük meg, ahol a szintaxis a következ: mutate(data frame, uj valtozo = ertekek). Példaként kiszámoljuk a svéd GDP-t (milliárd dollárban) 1992-tl kezdve. A mutate() alkalmazását részletesebben is bemutatjuk a szövegek elkészítésével foglalkozó fejezetben. gapminder %&gt;% filter(country == &quot;Sweden&quot;, year &gt;= 1992) %&gt;% mutate(gdp = (gdpPercap * pop) / 10^9) #&gt; # A tibble: 4 x 7 #&gt; country continent year lifeExp pop gdpPercap gdp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Sweden Europe 1992 78.2 8718867 23880. 208. #&gt; 2 Sweden Europe 1997 79.4 8897619 25267. 225. #&gt; 3 Sweden Europe 2002 80.0 8954175 29342. 263. #&gt; 4 Sweden Europe 2007 80.9 9031088 33860. 306. Az adataink részletesebb és alaposabb megismerésében segítenek a különböz szint leíró statisztikai adatok. A szintek megadására a group_by() használható, a csoportokon belüli számításokhoz pedig a summarize(). A lenti példa azt illusztrálja, hogy ha kontinensenként csoportosítjuk a gapminder adattáblát, akkor a summarise() használatával megkaphatjuk a megfigyelések számát, illetve az átlagos per capita GDP-t. A summarise() a mutate() közeli rokona, hasonló szintaxissal és logikával használható. Ezt a függvénypárost fogjuk majd használni a szöveges adataink leíró statisztikáinál is a 4. fejezetben. gapminder %&gt;% group_by(continent) %&gt;% summarise(megfigyelesek = n(), atlag_gdp = mean(gdpPercap)) #&gt; # A tibble: 5 x 3 #&gt; continent megfigyelesek atlag_gdp #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Africa 624 2194. #&gt; 2 Americas 300 7136. #&gt; 3 Asia 396 7902. #&gt; 4 Europe 360 14469. #&gt; 5 Oceania 24 18622. 3.5 Munka karakter vektorokkal6 A szöveges adatokkal (karakter stringekkel) való munka elkerülhetetlen velejárója, hogy a felesleges szövegelemeket, karaktereket el kell távolítanunk, hogy javuljon az elemzésünk hatásfoka. Erre a célra a stringr csomagot fogjuk használni, kombinálva a korábban bemutatott mutate()-el. A stringr függvények az str_ eltaggal kezddnek és eléggé beszédes nevekkel rendelkeznek. Egy gyakran elforduló probléma, hogy extra szóközök maradnak a szövegben, vagy bizonyos szavakról, karakterkombinációkról tudjuk, hogy nem kellenek az elemzésünkhöz. Ebben az esetben egy vagy több reguláris kifejezés (regular expression, regex) használatával tudjuk pontosan kijelölni, hogy a karakter sornak melyik részét akarjuk módosítani.7 A legegyszerbb formája a regexeknek, ha pontosan tudjuk milyen szöveget akarunk megtalálni. A kísérletezésre az str_view()-t használjuk, ami megjeleníti, hogy a megadott regex mintánk pontosan mit jelöl. szoveg &lt;- c(&quot;gitar&quot;, &quot;ukulele&quot;, &quot;nagybogo&quot;) str_view(szoveg, pattern = &quot;ar&quot;) Az ún. horgonyokkal, (anchor) azt lehet megadni, hogy a karakter string elején vagy végén szeretnénk-e egyezést találni. A string eleji anchor a ^, a string végi pedig a $. str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;^Dr.&quot;) str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;Dr.$&quot;) Egy másik jellemz probléma, hogy olyan speciális karaktert akarunk leírni a regex kifejezésünkkel, ami amúgy a regex szintaxisban használt. Ilyen eset például a ., ami mint írásjel sokszor csak zaj, ám a regex kontextusban a bármilyen karakter megfelelje. str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;.k.&quot;) Ahhoz, hogy magát az írásjelet jelöljük, a \\\\ -t kell elé rakni. str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;\\\\.&quot;) Néhány hasznos regex kifejezés: [:digit:] - számok (123) [:alpha:] - betk (abc ABC) [:lower:] - kisbetk (abc) [:upper:] - nagybetk (ABC) [:alnum:] - betk és számok (123 abc ABC) [:punct:] - központozás (.!?\\(){}) [:graph:] - betk, számok és központozás (123 abc ABC .!?\\(){}) [:space:] - szóköz ( ) [:blank:] - szóköz és tabulálás [:cntrl:] - kontrol karakterek (\\n, \\r, stb.) * - bármi A tidyverse megközelítés miatt a kötetben szerepl R kód követi a The tidyverse style guide dokumentációt (https://style.tidyverse.org/) Az RStudio-ban a pipe operátor billenty kombinációja a Ctrl + Shift + M Köszönjük Andrew Heissnek a kitn példát. A könyv terjedelme miatt ezt a témát itt csak bemutatni tudjuk, de minden részletre kiterjeden nem tudunk elmélyülni benne. A témában nagyon jól használható online anyagok találhatóak az RStudio GitHub tárhelyén (https://github.com/rstudio/cheatsheets/raw/master/strings.pdf), illetve Wickham and Grolemund (2016) 14. fejezetében. A reguláris kifejezés egy olyan, meghatározott szintaktikai szabályok szerint leírt karakterlánc (string), amivel meghatározható stringek egy adott halmaza. Az ilyen kifejezés valamilyen minta szerinti szöveg keresésére, cseréjére, illetve a szöveges adatok ellenrzésére használható. További információ: http://www.regular-expressions.info/ "],["corpus-ch.html", "4 Korpuszépítés és szövegelkészítés 4.1 Szövegbeszerzés 4.2 Szövegelkészítés", " 4 Korpuszépítés és szövegelkészítés 4.1 Szövegbeszerzés A szövegbányászati elemzések egyik els lépése az elemzés alapjául szolgáló korpusz megépítése. A korpuszt alkotó szövegek beszerzésének egyik módja a webscarping, melynek során weboldalakról történik az információ kinyerése. A scrapelést végezhetjük R-ben az rvest csomag segítségével. Fejezetünkben a scrapelésnek csupán néhány alaplépését mutatjuk meg, a folyamatról bvebb információ található például az alábbi oldalakon: https://cran.r-project.org/web/packages/rvest/rvest.pdf, https://rvest.tidyverse.org. library(rvest) library(readr) library(dplyr) library(lubridate) library(stringr) library(quanteda) library(quanteda.textmodels) library(HunMineR) A szükséges csomagok beolvasása után a read_html() függvény segítségével az adott weboldal adatait kérjük le a szerverrl. A read_html() függvény argumentuma az adott weblap URL-je. Ha például a poltextLAB projekt honlapjáról szeretnénk adatokat gyjteni, azt az alábbi módon tehetjük meg: r &lt;- read_html(&quot;https://poltextlab.tk.hu/hu&quot;) r #&gt; {html_document} #&gt; &lt;html lang=&quot;hu&quot; class=&quot;no-js&quot;&gt; #&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;\\n&lt;met ... #&gt; [2] &lt;body class=&quot;index&quot;&gt;\\n\\n\\t&lt;script&gt;\\n\\t (function(i,s,o,g,r,a,m){i[&#39;GoogleAnalyti ... Ezután a html_nodes() függvény argumentumaként meg kell adnunk azt a HTML címkét vagy CSS azonosítót, ami a legyjteni kívánt elemeket azonosítja a weboldalon. Ezeket az azonosítókat az adott weboldal forráskódjának megtekintésével tudhatjuk meg, amire a különböz böngészk különböz lehetségeket kínálnak. Majd a html_text() függvény segítségével megkapjuk azokat a szövegeket, amelyek az adott weblapon az adott azonosítóval rendelkeznek. Példánkban a https://poltextlab.tk.hu/hu weboldalról azokat az információkat szeretnénk kigyjteni, amelyek az &lt;title&gt; címke alatt szerepelnek. title &lt;- read_html(&quot;https://poltextlab.tk.hu/hu&quot;) %&gt;% html_nodes(&quot;title&quot;) %&gt;% html_text() title #&gt; [1] &quot;MTA TK Political and Legal Text Mining and Artificial Intelligence Laboratory (poltextLAB)&quot; Ezután a kigyjtött információkat kiíratjuk egy csv fájlba. write_csv(title, &quot;title.csv&quot;) A webscraping során az egyik nehézség, ha a weboldal letiltja az automatikus letöltést, ezt kivédhetjük például különböz böngészbvítmények segítségével, illetve a fejléc (header) vagy a hálózati kliens (user agent) megváltoztatásával. De segíthet véletlenszer kiszolgáló (proxy) vagy VPN szolgáltatás8 használata is, valamint ha az egyes kérések között idt hagyunk. A weboldalakon legtöbbször a legyjtött szövegekhez tartozó különböz metaadatok is szerepelnek (például egy parlamenti beszéd dátuma, az azt elmondó képvisel neve), melyeket érdemes a scarpelés során szintén összegyjteni. A scrapelés során fontos figyelnünk arra, hogy késbb jól használható formában mentsük el az adatokat, például .csv,.json vagy .txt kiterjesztésekkel. A karakterkódolási problémák elkerülése érdekében érdemes UTF-8 vagy UTF-16-os kódolást alkalmazni, mivel ezek tartalmazzák a magyar nyelv ékezetes karaktereit is. A karakterkódolással kapcsolatosan további hasznos információk találhatóak az alábbi oldalon: http://www.cs.bme.hu/~egmont/utf8 Arra is van lehetség, hogy az elemezni kívánt korpuszt papíron keletkezett, majd szkennelt és szükség szerint optikai karakterfelismerés (Optical Character Recognition  OCR) segítségével feldolgozott szövegekbl építsük fel. Mivel azonban ezeket a feladatokat nem R-ben végezzük, ezekrl itt nem szólunk bvebben. Az így beszerzett és .txt vagy .csv fájllá alakított szövegekbl való korpuszépítés a következ lépésekben megegyezik a weboldalakról gyjtött szövegekével. 4.2 Szövegelkészítés Az elemzéshez vezet következ lépés a szövegelkészítés, amit a szöveg tisztításával kell kezdenünk. A szövegtisztításnál mindig járjunk el körültekinten és az egyes lépéseket a kutatási kérdésünknek megfelelen tervezzük meg, a folyamat során pedig idnként végezzünk ellenrzést, ezzel elkerülhetjük a kutatásunkhoz szükséges információk elvesztését. Miután az elemezni kívánt szövegeinket beszereztük, majd a Az adatok importálása alfejezetben leírtak szerint importáltuk, következhetnek az alapvet elfeldolgozási lépések, ezek közé tartozik például a scrapelés során a korpuszba került html címkék, számok és egyéb zajok (például a speciális karakterek, írásjelek) eltávolítása, valamint a kisbetsítés, a tokenizálás, a szótövezés és a stopszavazás. A stringr csomag segítségével elször eltávolíthatjuk a felesleges html címkéket a korpuszból. Ehhez elször létrehozzuk a text1 nev objektumot, ami egy karaktervektorból áll. text1 &lt;- c(&quot;MTA TK&quot;, &quot;&lt;font size=&#39;6&#39;&gt; Political and Legal Text Mining and Artificial Intelligence Laboratory (poltextLAB)&quot;) text1 #&gt; [1] &quot;MTA TK&quot; #&gt; [2] &quot;&lt;font size=&#39;6&#39;&gt; Political and Legal Text Mining and Artificial Intelligence Laboratory (poltextLAB)&quot; Majd a str_replace_all()függvény segítségével eltávolítjuk két html címke közötti szövegrészt. Ehhez a függvény argumentumában létrehozunk egy regex kifejezést, aminek segítségével a függvény minden &lt; &gt; közötti szövegrészt üres karakterekre cserél. Ezután a str_to_lower()mindent kisbetvé konvertál, majd a str_trim() eltávolítja a szóközöket a karakterláncok elejérl és végérl. text1 %&gt;% str_replace_all(pattern = &quot;&lt;.*?&gt;&quot;, replacement = &quot;&quot;) %&gt;% str_to_lower() %&gt;% str_trim() #&gt; [1] &quot;mta tk&quot; #&gt; [2] &quot;political and legal text mining and artificial intelligence laboratory (poltextlab)&quot; 4.2.1 Tokenizálás, szótövezés, kisbetsítés és a tiltólistás szavak eltávolítása Az elkészítés következ lépésében tokenizáljuk, azaz egységeire bontjuk az elemezni kívánt szöveget, így a tokenek az egyes szavakat vagy kifejezéseket fogják jelölni. Ennek eredményeként kapjuk meg az n-gramokat, amik a vizsgált egységek (számok, betk, szavak, kifejezések) n-elem sorozatát alkotják. A következkben a Példa az elkészítésre mondatot bontjuk elször tokenekre a tokens() függvénnyel, majd a tokeneket a tokens_tolower() segítségével kisbetsítjük, a tokens_wordstem() függvénnyel pedig szótövezzük. Végezetül a quanteda csomagban található magyar nyelv stopszótár segítségével, elvégezzük a stopszavak eltávolítását. Ehhez elször létrehozzuk az sw elnevezés karaktervektort a magyar stopszavakból. A head() függvény segítségével belenézhetünk a szótárba, és a console-ra kiírathatjuk a szótár els hat szavát. Végül a tokens_remove()segítségével eltávolítjuk a stopszavakat. text &lt;- &quot;Példa az elokészítésre&quot; toks &lt;- tokens(text) toks &lt;- tokens_tolower(toks) toks &lt;- tokens_wordstem(toks) toks #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;példa&quot; &quot;az&quot; &quot;elokészítésr&quot; sw &lt;- stopwords(&quot;hungarian&quot;) head(sw) #&gt; [1] &quot;a&quot; &quot;ahogy&quot; &quot;ahol&quot; &quot;aki&quot; &quot;akik&quot; &quot;akkor&quot; tokens_remove(toks, sw) #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;példa&quot; &quot;elokészítésr&quot; Ezt követi a szótövezés (stemmelés) lépése, melynek során az alkalmazott szótövez algoritmus egyszeren levágja a szavak összes toldalékát, a képzket, a jelzket és a ragokat. Szótövezés helyett alkalmazhatunk szótári alakra hozást is (lemmatizálás). A két eljárás közötti különbség abban rejlik, hogy a szótövezés során csupán eltávolítjuk a szavak toldalékként azonosított végzdéseit, hogy ugyanannak a szónak különböz megjelenési formáit közös törzsre redukáljuk, míg a lemmatizálás esetében rögtön az értelmes, szótári formát kapjuk vissza. A két módszer közötti választás a kutatási kérdés alapján meghozott kutatói döntésen alapul (Grimmer and Stewart 2013). Az alábbi példában egyetlen szó különböz alakjainak szótári alakra hozásával szemléltetjük a lemmatizálás mködését. Ehhez elször a text1 nev objektumban tároljuk a szótári alakra hozni kívánt szöveget, majd tokenizáljuk és eltávolítjuk a központozást. Ezután definiáljuk a megfelel szótövet és azt, hogy mely szavak alakjait szeretnénk erre a szótre egységesíteni, majd a rep() függvény segítségével elvégezzük a lemmatizálást, amely a korábban definiált szóalakokat az általunk megadott szótári alakkal helyettesíti. Hosszabb szövegek lemmatizálásához elre létrehozott szótárakat használhatunk, ilyen például a WordNet, ami magyar nyelven is elérhet: https://github.com/mmihaltz/huwn. A magyar nyelv szövegek lemmatizálását elvégezhetjük a szövegek R-be való beolvasása eltt is a magyarlanc nyelvi elemz segítségével, melyrl a Természetes-nyelv feldolgozás (NLP) és névelemfelismerés cím fejezetben szólunk részletesebben. text1 &lt;- &quot;Példa az elokészítésre. Az elokészítést a szövetisztítással kell megkezdenünk. Az elokészített korpuszon elemzést végzünk&quot; toks1 &lt;- tokens(text1, remove_punct = TRUE) elokeszites &lt;- c(&quot;elokészítésre&quot;, &quot;elokészítést&quot;, &quot;elokészített&quot;) lemma &lt;- rep(&quot;elokészítés&quot;, length(elokeszites)) toks1 &lt;- tokens_replace(toks1, elokeszites, lemma, valuetype = &quot;fixed&quot;) toks1 #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;Példa&quot; &quot;az&quot; &quot;elokészítés&quot; &quot;Az&quot; #&gt; [5] &quot;elokészítés&quot; &quot;a&quot; &quot;szövetisztítással&quot; &quot;kell&quot; #&gt; [9] &quot;megkezdenünk&quot; &quot;Az&quot; &quot;elokészítés&quot; &quot;korpuszon&quot; #&gt; [ ... and 2 more ] A fenti text1 objektumban tárolt szöveg szótövezését az alábbiak szerint tudjuk elvégezni. Megvizsgálva az elkészítés különböz alakjainak lemmatizált és stemmelt változatát jól láthatjuk a két módszer közötti különbséget. text1 &lt;- &quot;Példa az elokészítésre. Az elokészítést a szövetisztítással kell megkezdenünk. Az elokészített korpuszon elemzést végzünk&quot; toks2 &lt;- tokens(text1, remove_punct = TRUE) toks2 &lt;- tokens_wordstem(toks2) toks2 #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;Példa&quot; &quot;az&quot; &quot;elokészítésr&quot; &quot;Az&quot; #&gt; [5] &quot;elokészítést&quot; &quot;a&quot; &quot;szövetisztításs&quot; &quot;kell&quot; #&gt; [9] &quot;megkezdenünk&quot; &quot;Az&quot; &quot;elokészített&quot; &quot;korpuszon&quot; #&gt; [ ... and 2 more ] 4.2.2 Dokumentum kifejezés mátrix (DTM) A szövegbányászati elemzések nagy részéhez szükségünk van arra, hogy a szövegeinkbl dokumentum kifejezés mátrix-ot (Document Term Matrix  DTM vagy Document Feature Matrix  DFM) hozzunk létre. Ezzel a lépéssel alakítjuk a szövegeinket számokká, ami lehetvé teszi, hogy utána különböz statisztikai mveleteket végezzünk velük. A dokumentum kifejezés mátrix minden sora egy dokumentum, minden oszlopa egy kifejezés, az oszlopokban szerepl változók pedig megmutatják az egyes kifejezések számát az egyes dokumentumokban. A legtöbb dokumentum kifejezés mátrix ritka mátrix, mivel a legtöbb dokumentum és kifejezés párosítása nem történik meg: a kifejezések nagy része csak néhány dokumentumban szerepel, ezek értéke nulla lesz. Az alábbi példában három egy-egy mondatos dokumentumon szemléltetjük a fentieket. A korábban megismert módon elkészítjük, azaz kisbetsítjük, szótövezzük a dokumentumokat, eltávolítjuk a tiltólistás szavakat, majd létrehozzuk bellük a dokumentum kifejezés mátrixot. text &lt;- c( d1 = &quot;Ez egy példa az elfeldolgozásra&quot;, d2 = &quot;Egy másik lehetséges példa&quot;, d3 = &quot;Ez pedig egy harmadik példa&quot; ) dtm &lt;- dfm( text, tolower = TRUE, stem = TRUE, remove = stopwords(&quot;hungarian&quot;) ) dtm #&gt; Document-feature matrix of: 3 documents, 4 features (50.0% sparse). #&gt; features #&gt; docs példa elofeldolgozásra lehetség harmadik #&gt; d1 1 1 0 0 #&gt; d2 1 0 1 0 #&gt; d3 1 0 0 1 Egy másik szövegbányászati megközelítés a mátrixot nem DTM-nek, hanem DFM-nek nevezi, például a quanteda csomag használata során nem DTM-et, hanem DFM-et kell létrehoznunk. text &lt;- c( d1 = &quot;Ez egy példa az elofeldolgozásra&quot;, d2 = &quot;Egy másik lehetséges példa&quot;, d3 = &quot;Ez pedig egy harmadik példa&quot; ) dfm &lt;- dfm( text, tolower = TRUE, stem = TRUE, remove = stopwords(&quot;hungarian&quot;) ) dfm #&gt; Document-feature matrix of: 3 documents, 4 features (50.0% sparse). #&gt; features #&gt; docs példa elofeldolgozásra lehetség harmadik #&gt; d1 1 1 0 0 #&gt; d2 1 0 1 0 #&gt; d3 1 0 0 1 4.2.3 Súlyozás A dokumentum kifejezés mátrix lehet egy egyszer bináris mátrix, ami csak azt az információt tartalmazza, hogy egy adott szó elfordul-e egy adott dokumentumban. Míg az egyszer bináris mátrixban ugyanakkora súlya van egy szónak ha egyszer és ha tízszer szerepel, készíthetünk olyan mátrixot is, ahol egy szónak annál nagyobb a súlya egy dokumentumban, minél többször fordul el. A szógyakoriság (term frequency  TF) szerint súlyozott mátrixnál azt is figyelembe vesszük, hogy az adott szó hány dokumentumban szerepel. Minél több dokumentumban szerepel egy szó, annál kisebb a jelentsége. Ilyen szavak például a névelk, amelyek sok dokumentumban elfordulnak ugyan, de nem sok tartalmi jelentséggel bírnak. Két szó közül általában az a fontosabb, amelyik koncentráltan, kevés dokumentumban, de azokon belül nagy gyakorisággal fordul el. A dokumentum gyakorisági érték (document frequency  DF) egy szó gyakoriságát jellemzi egy korpuszon belül. A súlyozási sémákban általában a dokumentum gyakorisági érték inverzével számolnak (inverse document frequency - IDF) ez a leggyakrabban használt TF-IDF súlyozás (term frequency &amp; inverse document frequency - TF-IDF). Az így súlyozott TF mátrix egy-egy cellájában található érték azt mutatja, hogy egy adott szónak mekkora a jelentsége egy adott dokumentumban. A TF-IDF súlyozás értéke tehát magas azon szavak esetén, amelyek az adott dokumentumban gyakran fordulnak el, míg a teljes korpuszban ritkán; alacsonyabb azon szavak esetén, amelyek az adott dokumentumban ritkábban, vagy a korpuszban gyakrabban fordulnak el; és kicsi azon szavaknál, amelyek a korpusz lényegében összes dokumentumában elfordulnak (Tikk 2007, 3337 o.) Az alábbiakban az 1999-es törvényszövegeken szemléltetjük, hogy egy 125 dokumentumból létrehozott mátrix segítségével milyen alapvet statisztikai mveleteket végezhetünk.9 A HunMineR csomagból tudjuk importálni a törvényeket. lawtext_df &lt;- HunMineR::data_lawtext_1999 Majd az importált adatokból létrehozzuk a korpuszt lawtext_corpus néven. Ezt követi a dokumentum kifejezés mátrix kialakítása (mivel a quanteda csomaggal dolgozunk, dfm mátrixot hozunk létre), és ezzel egy lépésben elvégezzük az alapvet szövegtisztító lépéseket is. lawtext_corpus &lt;- corpus(lawtext_df) lawtext_dfm &lt;- dfm( lawtext_corpus, tolower = TRUE, remove = stopwords(&quot;hungarian&quot;), stem = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE ) A topfeatures függvény segítségével megnézhetjük a mátrix leggyakoribb szavait, a függvény argumentumában megadva a dokumentum kifejezés mátrix nevét és a kívánt kifejezésszámot. topfeatures(lawtext_dfm, 15) #&gt; the of szerzodo to b ha and kiadások #&gt; 7902 5665 3619 3290 2831 2794 2712 2447 #&gt; törvéni in következo muködési or évi is #&gt; 2385 2253 2178 2038 2034 1908 1864 Mivel látható, hogy a szövegekben sok angol kifejezés is volt egy következ lépcsben az angol stopszavakat is eltávolítjuk. lawtext_dfm_2 &lt;- dfm(lawtext_dfm, remove = stopwords(&quot;english&quot;)) Ezután megnézzük a leggyakoribb 15 kifejezést. topfeatures(lawtext_dfm_2, 15) #&gt; szerzodo b ha kiadások törvéni következo #&gt; 3619 2831 2794 2447 2385 2178 #&gt; muködési évi állam c meghatározott költségveté #&gt; 2038 1908 1718 1713 1654 1637 #&gt; államban lép fél #&gt; 1622 1616 1533 A következ lépés, hogy TF-IDF súlyozású statisztikát készítünk, a dokumentum kifejezés mátrix alapján. Ehhez elször létrehozzuk a lawtext_tfidf nev objektumot, majd a textstat_frequency függvény segítségével kilistázzuk annak els 10 elemét. lawtext_tfidf &lt;- dfm_tfidf(lawtext_dfm_2) textstat_frequency(lawtext_tfidf, force = TRUE, n = 10) #&gt; feature frequency rank docfreq group #&gt; 1 kiadások 2120.2303 1 17 all #&gt; 2 felhalmozási 1448.3465 2 7 all #&gt; 3 szerzodo 1378.5012 3 52 all #&gt; 4 költségveté 1302.8556 4 20 all #&gt; 5 shall 1291.1619 5 14 all #&gt; 6 államban 1223.7785 6 22 all #&gt; 7 részes 1155.9688 7 13 all #&gt; 8 muködési 1101.7581 8 36 all #&gt; 9 articl 967.8961 9 14 all #&gt; 10 parti 845.2246 10 20 all A VPN (Virtual Private Network, azaz a virtuális magánhálózat) azt teszi lehetvé, hogy a felhasználók egy megosztott vagy nyilvános hálózaton keresztül úgy küldjenek és fogadjanak adatokat, mintha számítógépeik közvetlenül kapcsolódnának a helyi hálózathoz. Az itt használt kódok az alábbiakon alapulnak: https://rdrr.io/cran/quanteda/man/dfm_weight.html, https://rdrr.io/cran/quanteda/man/dfm_tfidf.html. A példaként használt korpusz a Hungarian Comparative Agendas Project keretében készült adatbázis része: https://cap.tk.hu/torveny "],["leiro-stat.html", "5 Leíró statisztika 5.1 A szövegek a vektortérben 5.2 Leíró statisztika 5.3 A szövegek lexikai diverzitása 5.4 Összehasonlítás14 5.5 Kulcsszavak kontextusa", " 5 Leíró statisztika 5.1 A szövegek a vektortérben A szövegbányászati feladatok két altípusa a keresés és a rendszerezés. A keresés során olyan szövegeket keresünk, amelyekben egy adott kifejezés elfordul, a rendszerezés során pedig a szövegeket hasonlítjuk össze egymással és egy elre megadott, vagy egy elzetesen nem ismert kategóriarendszer csoportjaihoz soroljuk ket. A webes keresprogramok egyik jellemz tevékenysége, az információ-visszakeresés (information retrieval) során például az a cél, hogy a korpuszból visszakeressük a keres információigénye szempontjából releváns információkat, mely keresés alapulhat metaadatokon vagy teljes szöveges indexelésen (Tikk 2007; Russel and Norvig 2005, 742.o). Az információkinyerés (information extraction) esetén a cél, hogy a strukturálatlan szövegekbl strukturált adatokat állítsunk el. Azaz az információkinyerés során nem a felhasználó által keresett információt keressük meg és lokalizáljuk, hanem az adott kérdés szempontjából releváns információkat gyjtjük ki a dokumentumokból. Az információkinyerés alternatív megoldása segítségével már képesek lehetünk a kifejezések közötti kapcsolatok elemzésére, tendenciák és minták felismerésére és az információk összekapcsolása révén új információk létrehozására, azaz a segítségével strukturálatlan szövegekbl is elállíthatunk strukturált információkat (Kwartler 2017; Schütze, Manning, and Raghavan 2008; Tikk 2007, 6381.o). A szövegbányászati vizsgálatok során folyó szövegek, azaz strukturálatlan vagy részben strukturált dokumentumok elemzésére kerül sor. Ezekbl a kutatási kérdéseink szempontjából releváns, látens összefüggéseket nyerünk ki, amelyek már strukturált szerkezetek. A dokumentumok reprezentálásának három legelterjedtebb módja a halmazelmélet alapú, az algebrai és a valószínségi modell. A halmazelméleti modellek a dokumentumok hasonlóságát halmazelmélet, a valószínségi modellek pedig feltételes valószínségi becslés alapján határozzák meg. Az algebrai modellek a dokumentumokat vektorként vagy mátrixként ábrázolják és algebrai mveletek segítségével hasonlítják össze. A vektortérmodell sokdimenziós vektortérben ábrázolja a dokumentumokat, úgy, hogy a dokumentumokat vektorokkal reprezentálja, a vektortér dimenziói pedig a dokumentumok összességében elforduló egyedi szavak. A modell alkalmazása során azok a dokumentumok hasonlítanak egymásra, amelyeknek a szókészlete átfedi egymást, és a hasonlóság mértéke az átfedéssel arányos. A vektortérmodellben a dokumentumgyjteményt a szó-dokumentum mátrixszal (term-document matrix) reprezentáljuk, a mátrixban a sorok száma megegyezik az egyedi szavak számával, az oszlopokat pedig a dokumentumvektorok alkotják. Az egyedi szavak összességét szótárnak nevezzük. Mivel mátrixban az egyedi szavak száma általában igen nagy, ezért a mátrix hatékony kezeléséhez annak mérete különböz eljárásokkal csökkenthet. Fontos tudni, hogy a dokumentumok vektortér reprezentációjában a szavak szövegen belüli sorrendjére és pozíciójára vonatkozó információ nem található meg (Russel and Norvig 2005, 74244 o.; Kwartler 2017; Welbers, Van Atteveldt, and Benoit 2017). A vektortérmodellt szózsák (bag of words) modellnek is nevezzük, melynek segítségével a fent leírtak szerint az egyes szavak gyakoriságát vizsgálhatjuk meg egy adott korpuszon belül. 5.2 Leíró statisztika Fejezetünkben nyolc véletlenszeren kiválasztott magyar miniszterelnöki beszéd vizsgálatát végezzük el,10 amihez az alábbi csomagokat használjuk: library(HunMineR) library(readtext) library(dplyr) library(lubridate) library(stringr) library(ggplot2) library(quanteda) library(GGally) library(ggdendro) library(tidytext) Els lépésben a Bevezetben már ismertetett módon a HunMineR csomagból betöltjük a beszédeket. texts &lt;- HunMineR::data_miniszterelnokok_raw Ezt követen az Adatkezelés R-ben cím fejezetben ismertetett mutate() függvény használatával két csoportra osztjuk a beszédeket. Ehhez elször a string_extract() függvény segítségével meghatározzuk, hogy a kettéosztáshoz használni kívánt új változó a doc_id legyen, a [^\\\\.]* regex segítségével leválasztva arról a .txt kiterjesztést, majd a str_sub() függvénnyel megmondjuk, hogy a miniszterelnökök neve a doc_id hátulról számított hatodik karakteréig tart. Ezután kialakítjuk a két csoportot, azaz az if_else() segítségével meghatározzuk, hogy ha antall_jozsef, boross_peter, orban_viktor\" beszédeirl van szó azokat a jobb csoportba tegye, a maradékot pedig a bal csoportba. Majd azt is meghatározzuk, hogy melyik beszédnek mi a dátuma. Ehhez szintén a str_sub() függvényt használjuk, majd a lubridate segítségével alakítjuk ki a kívánt dátumformátumot.11 Ezután a glimpse() függvény segítségével megtekintjük, hogy milyen változtatásokat végeztünk az adattáblánkon. Láthatjuk, hogy míg korábban 8 dokumentumunk és 2 változónk volt, az átalakítás eredményeként a 8 dokumentum mellett már 5 változót találunk. Ezzel a lépéssel tehát kialakítottuk azokat a változókat, amelyekre az elemzés során szükségünk lesz. miniszterelnokok &lt;- c(&quot;antall_jozsef&quot;, &quot;boross_peter&quot;, &quot;orban_viktor&quot;) texts &lt;- texts %&gt;% mutate( doc_id = str_extract(doc_id, &quot;[^\\\\.]*&quot;), mineln = str_sub(doc_id, end = -6), partoldal = if_else(mineln %in% miniszterelnokok, &quot;jobb&quot;, &quot;bal&quot;) ) texts$year &lt;- str_sub(texts$doc_id, start = -2) %&gt;% str_c(&quot;-01-01&quot;) %&gt;% lubridate::ymd() %&gt;% lubridate::year() glimpse(texts) #&gt; Rows: 8 #&gt; Columns: 5 #&gt; $ doc_id &lt;chr&gt; &quot;antall_jozsef_1990&quot;, &quot;bajnai_gordon_2009&quot;, &quot;boross_peter_1993&quot;, &quot;gy~ #&gt; $ text &lt;chr&gt; &quot;Elnök Úr! Tisztelt Országgyulés! Hölgyeim és Uraim! Honfitársaim! Ü~ #&gt; $ mineln &lt;chr&gt; &quot;antall_jozsef&quot;, &quot;bajnai_gordon&quot;, &quot;boross_peter&quot;, &quot;gyurcsany_ferenc&quot;~ #&gt; $ partoldal &lt;chr&gt; &quot;jobb&quot;, &quot;bal&quot;, &quot;jobb&quot;, &quot;bal&quot;, &quot;bal&quot;, &quot;bal&quot;, &quot;jobb&quot;, &quot;jobb&quot; #&gt; $ year &lt;dbl&gt; 1990, 2009, 1993, 2005, 1994, 2002, 1995, 2018 Ezt követen a további lépések elvégzéséhez létrehozzuk a quanteda korpuszt, majd a summary() függvény segítségével megtekinthetjük a korpusz alapvet statisztikai jellemzit. Láthatjuk például, hogy az egyes dokumentumok hány tokenbl vagy mondatból állnak. corpus_mineln &lt;- corpus(texts) summary(corpus_mineln) #&gt; Corpus consisting of 8 documents, showing 8 documents: #&gt; #&gt; Text Types Tokens Sentences mineln partoldal year #&gt; antall_jozsef_1990 3745 9408 431 antall_jozsef jobb 1990 #&gt; bajnai_gordon_2009 1391 3277 201 bajnai_gordon bal 2009 #&gt; boross_peter_1993 1552 3170 179 boross_peter jobb 1993 #&gt; gyurcsany_ferenc_2005 2963 10267 454 gyurcsany_ferenc bal 2005 #&gt; horn_gyula_1994 1704 4372 226 horn_gyula bal 1994 #&gt; medgyessy_peter_2002 1021 2362 82 medgyessy_peter bal 2002 #&gt; orban_viktor_1995 1810 4287 212 orban_viktor jobb 1995 #&gt; orban_viktor_2018 933 1976 126 orban_viktor jobb 2018 Mivel az elemzés során a korpuszon belül két csoportra osztva szeretnénk összehasonlításokat tenni, az alábbiakban két alkorpuszt alakítunk ki. mineln_jobb &lt;- corpus_subset(corpus_mineln, mineln %in% c(&quot;antall_jozsef&quot;, &quot;boross_peter&quot;, &quot;orban_viktor&quot;)) mineln_bal &lt;- corpus_subset(corpus_mineln, mineln %in% c(&quot;horn_gyula&quot;, &quot;gyurcsany_ferenc&quot;, &quot;medgyessy_peter&quot;, &quot;bajnai_gordon&quot;)) summary(mineln_jobb) #&gt; Corpus consisting of 4 documents, showing 4 documents: #&gt; #&gt; Text Types Tokens Sentences mineln partoldal year #&gt; antall_jozsef_1990 3745 9408 431 antall_jozsef jobb 1990 #&gt; boross_peter_1993 1552 3170 179 boross_peter jobb 1993 #&gt; orban_viktor_1995 1810 4287 212 orban_viktor jobb 1995 #&gt; orban_viktor_2018 933 1976 126 orban_viktor jobb 2018 summary(mineln_bal) #&gt; Corpus consisting of 4 documents, showing 4 documents: #&gt; #&gt; Text Types Tokens Sentences mineln partoldal year #&gt; bajnai_gordon_2009 1391 3277 201 bajnai_gordon bal 2009 #&gt; gyurcsany_ferenc_2005 2963 10267 454 gyurcsany_ferenc bal 2005 #&gt; horn_gyula_1994 1704 4372 226 horn_gyula bal 1994 #&gt; medgyessy_peter_2002 1021 2362 82 medgyessy_peter bal 2002 A korábban létrehozott jobb\" és bal\" változó segítségével nem csak az egyes dokumentumokat, hanem a két csoportba sorolt beszédeket is összehasonlíthatjuk egymással. summary(corpus_mineln) %&gt;% group_by(partoldal) %&gt;% summarise( mean_wordcount = mean(Tokens), std_dev = sd(Tokens), min_wordc = min(Tokens), max_wordc = max(Tokens) ) #&gt; # A tibble: 2 x 5 #&gt; partoldal mean_wordcount std_dev min_wordc max_wordc #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 bal 5070. 3561. 2362 10267 #&gt; 2 jobb 4710. 3271. 1976 9408 A textstat_collocations() függvény segítségével szókapcsolatokat kereshetünk. A függvény argumentumai közül a size a szókapcsolatok hossza, a min_count pedig a minimális elfordulásuk száma. Miután a szókapcsolatokat megkerestük, közülük a korábban már megismert head() függvény segítségével tetszleges számút megnézhetünk.12 corpus_mineln %&gt;% textstat_collocations( size = 3, min_count = 6 ) %&gt;% head(n = 10) #&gt; collocation count count_nested length lambda z #&gt; 1 a kormány a 30 0 3 1.7266498 3.5500939 #&gt; 2 az új kormány 13 0 3 4.7126130 2.9870558 #&gt; 3 az a politika 6 0 3 3.9239765 2.4659912 #&gt; 4 a kormány az 6 0 3 2.6826277 1.7954739 #&gt; 5 a száz lépés 9 0 3 3.5817972 1.5956086 #&gt; 6 a magyar gazdaság 14 0 3 2.5135668 1.5757358 #&gt; 7 ez a program 9 0 3 1.9371894 1.2837433 #&gt; 8 tisztelt hölgyeim és 31 0 3 2.2327116 0.9596371 #&gt; 9 hogy ez a 10 0 3 0.4617372 0.9032165 #&gt; 10 hogy a magyar 18 0 3 0.6903168 0.7832561 A szókapcsolatok listázásánál is láthattuk, hogy a korpuszunk még minden szót tartalmaz, ezért találtunk például hogy ez a\" összetételt. A következkben eltávolítjuk az ilyen funkció nélküli stopszavakat a korpuszból, amihez saját stopszólistát használunk. Elször a HunMineR csomagból beolvassuk és egy custom_stopwords nev objektumban tároljuk a stopszavakat, majd a tokens() függvény segítségével tokenizáljuk a korpuszt és a tokens_select() használatával eltávolítjuk a stopszavakat. Ha ezután újra megnézzük a kollokációkat, jól látható a stopszavak eltávolításának eredménye: custom_stopwords &lt;- HunMineR::data_stopwords_extra corpus_mineln %&gt;% tokens() %&gt;% tokens_select(pattern = custom_stopwords, selection = &quot;remove&quot;) %&gt;% textstat_collocations( size = 3, min_count = 6 ) %&gt;% head(n = 10) #&gt; collocation count count_nested length lambda z #&gt; 1 taps MSZP soraiból 7 0 3 -1.848559 -1.003837 #&gt; 2 tisztelt hölgyeim uraim 31 0 3 -3.217896 -1.087495 #&gt; 3 taps kormánypártok soraiban 13 0 3 -1.884367 -1.102199 #&gt; 4 közbeszólás fidesz soraiból 12 0 3 -4.371498 -1.949939 #&gt; 5 taps MSZP soraiban 9 0 3 -4.711439 -2.780059 A korpusz további elemzése eltt fontos, hogy ne csak a stopszavakat távolítsuk el, hanem az egyéb alapvet szövegtisztító lépéseket is elvégezzük. Azaz a tokens_select() segítségével eltávolítsuk a számokat, a központozást, az elválasztó karaktereket, mint például a szóközöket, tabulátorokat, sortöréseket. Ezután a tokens_ngrams() segítségével ngramokat hozunk létre a tokenekbl, majd kialakítjuk a dokumentum kifejezés mátrixot (dfm) és elvégezzük a tf-idf szerinti súlyozást. A dfm_tfidf() függvény kiszámolja a dokumentum gyakoriság inverz súlyozását. A függvény alapértelmezés szerint a normalizált kifejezések gyakoriságát használja a dokumentumon belüli relatív kifejezés gyakoriság helyett, ezt írjuk felül a schem_tf = \"prop\" használatával. Végül a textstat_frequency() segítségével gyakorisági statisztikát készíthetünk a korábban meghatározott (példánkban két és három tagú) n-gramokról. corpus_mineln %&gt;% tokens( remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE ) %&gt;% tokens_select(pattern = custom_stopwords, selection = &quot;remove&quot;) %&gt;% tokens_ngrams(n = 2:3) %&gt;% dfm() %&gt;% dfm_tfidf(scheme_tf = &quot;prop&quot;) %&gt;% textstat_frequency(n = 10, force = TRUE) #&gt; feature frequency rank docfreq group #&gt; 1 tisztelt_hölgyeim 0.002788904 1 4 all #&gt; 2 tisztelt_hölgyeim_uraim 0.002788904 1 4 all #&gt; 3 fordítsanak_hátat 0.002439685 3 1 all #&gt; 4 fidesz_soraiból 0.002151922 4 1 all #&gt; 5 taps_mszp 0.001757918 5 2 all #&gt; 6 magyarország_európa 0.001457380 6 1 all #&gt; 7 tisztelt_képviselotársaim 0.001439932 7 2 all #&gt; 8 kormánypártok_soraiban 0.001430247 8 2 all #&gt; 9 taps_kormánypártok 0.001293617 9 2 all #&gt; 10 taps_kormánypártok_soraiban 0.001293617 9 2 all 5.3 A szövegek lexikai diverzitása Az alábbiakban a korpuszunkat alkotó szövegek lexikai diverzitását vizsgáljuk. Ehhez a quanteda csomag textstat_lexdiv() függvényét használjuk. Mivel ez a függvény dfm-et elemez, elször a corpus_mineln nev korpuszunkból létrehozzuk a mineln_dfm nev dfm-et, amelyen elvégezzük a korábban már megismert alapvet tisztító lépéseket. A textstat_lexdiv() függvény eredménye szintén egy dfm, így azt arrange() parancs argumentumában a desc megadásával csökken sorba is rendezhetjük. Atextstat_lexdiv() különböz indexek segítségével számítja ki a szövegek lexikai különbözségét, példánkban a CTTR indexet használjuk.13 mineln_dfm &lt;- corpus_mineln %&gt;% tokens( remove_punct = TRUE, remove_separators = TRUE, split_hyphens = TRUE ) %&gt;% dfm(remove = custom_stopwords) mineln_dfm %&gt;% textstat_lexdiv(measure = &quot;CTTR&quot;) %&gt;% arrange(desc(CTTR)) #&gt; document CTTR #&gt; 1 antall_jozsef_1990 32.99078 #&gt; 2 gyurcsany_ferenc_2005 26.14422 #&gt; 3 orban_viktor_1995 23.35548 #&gt; 4 horn_gyula_1994 22.25547 #&gt; 5 boross_peter_1993 21.98656 #&gt; 6 bajnai_gordon_2009 19.93214 #&gt; 7 medgyessy_peter_2002 16.81246 #&gt; 8 orban_viktor_2018 16.24532 A kiszámolt értéket hozzáadhatjuk a dfm-hez is. dfm_lexdiv &lt;- mineln_dfm cttr_score &lt;- unlist(textstat_lexdiv(dfm_lexdiv, measure = &quot;CTTR&quot;)[, 2]) docvars(dfm_lexdiv, &quot;cttr&quot;) &lt;- cttr_score docvars(dfm_lexdiv) #&gt; mineln partoldal year cttr #&gt; 1 antall_jozsef jobb 1990 32.99078 #&gt; 2 bajnai_gordon bal 2009 19.93214 #&gt; 3 boross_peter jobb 1993 21.98656 #&gt; 4 gyurcsany_ferenc bal 2005 26.14422 #&gt; 5 horn_gyula bal 1994 22.25547 #&gt; 6 medgyessy_peter bal 2002 16.81246 #&gt; 7 orban_viktor jobb 1995 23.35548 #&gt; 8 orban_viktor jobb 2018 16.24532 A fenti elemzést elvégezhetjük úgy is, hogy valamennyi indexálást egyben megkapjuk. Ehhez a textstat_lexdiv() függvény argumentumába a measure = \"all\" kifejezést kell megadnunk. mineln_dfm %&gt;% textstat_lexdiv(measure = &quot;all&quot;) #&gt; document TTR C R CTTR U S #&gt; 1 antall_jozsef_1990 0.6465054 0.9490329 46.65601 32.99078 72.92298 0.9601534 #&gt; 2 bajnai_gordon_2009 0.7283044 0.9566410 28.18830 19.93214 73.23764 0.9616371 #&gt; 3 boross_peter_1993 0.7209677 0.9565427 31.09369 21.98656 75.23509 0.9624951 #&gt; 4 gyurcsany_ferenc_2005 0.5563859 0.9301449 36.97351 26.14422 52.17985 0.9440104 #&gt; 5 horn_gyula_1994 0.7142122 0.9555469 31.47399 22.25547 73.97126 0.9618001 #&gt; 6 medgyessy_peter_2002 0.7110912 0.9514261 23.77641 16.81246 62.75886 0.9553276 #&gt; 7 orban_viktor_1995 0.7224880 0.9574810 33.02964 23.35548 78.08616 0.9637924 #&gt; 8 orban_viktor_2018 0.7529538 0.9584932 22.97435 16.24532 71.52920 0.9610435 #&gt; K I D Vm Maas lgV0 lgeV0 #&gt; 1 11.21473 419.0858 0.0009296389 0.02871363 0.1171029 11.191310 25.76894 #&gt; 2 16.40817 459.3906 0.0009739104 0.02691146 0.1168511 10.429595 24.01503 #&gt; 3 18.51659 355.0407 0.0013147316 0.03325578 0.1152895 10.725349 24.69603 #&gt; 4 11.86397 291.9314 0.0009601654 0.02791768 0.1384359 9.233313 21.26049 #&gt; 5 12.60022 571.6996 0.0007454724 0.02321726 0.1162702 10.656921 24.53847 #&gt; 6 26.48161 251.3022 0.0017552766 0.03728672 0.1262300 9.420533 21.69158 #&gt; 7 16.50145 400.1580 0.0011722374 0.03143078 0.1131652 11.019123 25.37247 #&gt; 8 26.17792 313.3935 0.0015453380 0.03451461 0.1182383 9.980932 22.98195 Ha pedig arra vagyunk kíváncsiak, hogy a kapott értékek hogyan viszonyulnak egymáshoz, azt a cor() függvény segítésével számolhatjuk ki. div_df &lt;- mineln_dfm %&gt;% textstat_lexdiv(measure = &quot;all&quot;) cor(div_df[, 2:13]) #&gt; TTR C R CTTR U S K #&gt; TTR 1.0000000 0.9709934 -0.64927227 -0.64927227 0.75866675 0.85148414 0.59914525 #&gt; C 0.9709934 1.0000000 -0.44988787 -0.44988787 0.88841537 0.95162677 0.42942231 #&gt; R -0.6492723 -0.4498879 1.00000000 1.00000000 -0.02424963 -0.16264543 -0.83367217 #&gt; CTTR -0.6492723 -0.4498879 1.00000000 1.00000000 -0.02424963 -0.16264543 -0.83367217 #&gt; U 0.7586668 0.8884154 -0.02424963 -0.02424963 1.00000000 0.98430655 0.01275208 #&gt; S 0.8514841 0.9516268 -0.16264543 -0.16264543 0.98430655 1.00000000 0.16142157 #&gt; K 0.5991452 0.4294223 -0.83367217 -0.83367217 0.01275208 0.16142157 1.00000000 #&gt; I 0.2327750 0.3698611 0.26241321 0.26241321 0.58486310 0.52740696 -0.58736335 #&gt; D 0.4013304 0.2514823 -0.65214634 -0.65214634 -0.10697212 0.01957486 0.94208077 #&gt; Vm 0.3075452 0.1968097 -0.47355632 -0.47355632 -0.07691131 0.01930842 0.84481216 #&gt; Maas -0.7882881 -0.9112332 0.05759202 0.05759202 -0.99701921 -0.99354677 -0.06058855 #&gt; lgV0 0.3571139 0.5682834 0.45841359 0.45841359 0.87598404 0.79330142 -0.40821917 #&gt; I D Vm Maas lgV0 #&gt; TTR 0.2327750 0.40133039 0.30754517 -0.78828811 0.3571139 #&gt; C 0.3698611 0.25148227 0.19680971 -0.91123321 0.5682834 #&gt; R 0.2624132 -0.65214634 -0.47355632 0.05759202 0.4584136 #&gt; CTTR 0.2624132 -0.65214634 -0.47355632 0.05759202 0.4584136 #&gt; U 0.5848631 -0.10697212 -0.07691131 -0.99701921 0.8759840 #&gt; S 0.5274070 0.01957486 0.01930842 -0.99354677 0.7933014 #&gt; K -0.5873634 0.94208077 0.84481216 -0.06058855 -0.4082192 #&gt; I 1.0000000 -0.77504336 -0.82248673 -0.56958764 0.6634351 #&gt; D -0.7750434 1.00000000 0.96943443 0.06563582 -0.4269281 #&gt; Vm -0.8224867 0.96943443 1.00000000 0.04532617 -0.3156220 #&gt; Maas -0.5695876 0.06563582 0.04532617 1.00000000 -0.8557245 #&gt; lgV0 0.6634351 -0.42692805 -0.31562199 -0.85572445 1.0000000 A kapott értékeket a ggcorr() függvény segítségével ábrázolhatjuk is. Ha a függvény argumentumában a label = TRUE szerepel, a kapott ábrán a kiszámított értékek is láthatók. ggcorr(div_df[, 2:13], label = TRUE) Figure 5.1: Korrelációs hotérkép Ezt követen azt is megvizsgálhatjuk, hogy a korpusz szövegei mennyire könnyen olvashatóak. Ehhez a Flesch.Kincaid pontszámot használjuk, ami a szavak és a mondatok hossza alapján határozza meg a szöveg olvashatóságát. Ehhez a textstat_readability() függvényt használjuk, mely a korpuszunkat elemzi. corpus_mineln %&gt;% textstat_readability(measure = &quot;Flesch.Kincaid&quot;) #&gt; document Flesch.Kincaid #&gt; 1 antall_jozsef_1990 16.48512 #&gt; 2 bajnai_gordon_2009 10.92243 #&gt; 3 boross_peter_1993 15.40159 #&gt; 4 gyurcsany_ferenc_2005 13.55911 #&gt; 5 horn_gyula_1994 13.77918 #&gt; 6 medgyessy_peter_2002 15.81893 #&gt; 7 orban_viktor_1995 13.04284 #&gt; 8 orban_viktor_2018 11.39180 Ezután a kiszámított értékkel kiegészítjük a korpuszt. docvars(corpus_mineln, &quot;f_k&quot;) &lt;- textstat_readability(corpus_mineln, measure = &quot;Flesch.Kincaid&quot;)[, 2] docvars(corpus_mineln) #&gt; mineln partoldal year f_k #&gt; 1 antall_jozsef jobb 1990 16.48512 #&gt; 2 bajnai_gordon bal 2009 10.92243 #&gt; 3 boross_peter jobb 1993 15.40159 #&gt; 4 gyurcsany_ferenc bal 2005 13.55911 #&gt; 5 horn_gyula bal 1994 13.77918 #&gt; 6 medgyessy_peter bal 2002 15.81893 #&gt; 7 orban_viktor jobb 1995 13.04284 #&gt; 8 orban_viktor jobb 2018 11.39180 docvars(corpus_mineln, &quot;f_k&quot;) &lt;- textstat_readability(corpus_mineln, measure = &quot;Flesch.Kincaid&quot;)[, 2] docvars(corpus_mineln) #&gt; mineln partoldal year f_k #&gt; 1 antall_jozsef jobb 1990 16.48512 #&gt; 2 bajnai_gordon bal 2009 10.92243 #&gt; 3 boross_peter jobb 1993 15.40159 #&gt; 4 gyurcsany_ferenc bal 2005 13.55911 #&gt; 5 horn_gyula bal 1994 13.77918 #&gt; 6 medgyessy_peter bal 2002 15.81893 #&gt; 7 orban_viktor jobb 1995 13.04284 #&gt; 8 orban_viktor jobb 2018 11.39180 Majd a ggplot2 segítségével vizualizálhatjuk az eredményt. Ehhez az olvashatósági pontszámmal kiegészített korpuszból egy adattáblát alakítunk ki, majd beállítjuk az ábrázolás paramétereit. Azaz ahhoz, hogy a két tengelyen az év, illetve az olvashatósági pontszám szerepeljen, a színezés különböztesse meg a jobb és a bal oldalt, az egyes dokumentumokat ponttal jelöljük, a jobb és a bal oldali beszédeket vonallal kötjük össze, az ábrára fekete színnel felíratjuk a miniszterelnökök nevét. Valamint azt is beállítjuk, hogy az x tengely beosztása az egyes beszédek dátumához igazodjon. A theme_minimal() függvénnyel pedig azt határozzuk meg, hogy mindez fehér hátteret kapjon. corpus_df &lt;- docvars(corpus_mineln) ggplot(corpus_df, aes(year, f_k)) + geom_point(size = 2) + geom_line(aes(linetype = partoldal), size = 1) + geom_text(aes(label = mineln), color = &quot;black&quot;, nudge_y = 0.15) + scale_x_continuous(limits = c(1988, 2020)) + labs( x = NULL, y = &quot;Flesch-Kincaid index&quot;, color = NULL, linetype = NULL ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) Figure 5.2: Az olvashatósági index alakulása 5.4 Összehasonlítás14 mineln_dfm %&gt;% dfm_weight(&quot;prop&quot;) %&gt;% textstat_simil(margin = &quot;documents&quot;, method = &quot;jaccard&quot;) #&gt; textstat_simil object; method = &quot;jaccard&quot; #&gt; antall_jozsef_1990 bajnai_gordon_2009 boross_peter_1993 #&gt; antall_jozsef_1990 1.0000 0.0559 0.1011 #&gt; bajnai_gordon_2009 0.0559 1.0000 0.0564 #&gt; boross_peter_1993 0.1011 0.0564 1.0000 #&gt; gyurcsany_ferenc_2005 0.0798 0.0850 0.0604 #&gt; horn_gyula_1994 0.0694 0.0592 0.0613 #&gt; medgyessy_peter_2002 0.0404 0.0690 0.0473 #&gt; orban_viktor_1995 0.0778 0.0626 0.0631 #&gt; orban_viktor_2018 0.0362 0.0617 0.0401 #&gt; gyurcsany_ferenc_2005 horn_gyula_1994 medgyessy_peter_2002 #&gt; antall_jozsef_1990 0.0798 0.0694 0.0404 #&gt; bajnai_gordon_2009 0.0850 0.0592 0.0690 #&gt; boross_peter_1993 0.0604 0.0613 0.0473 #&gt; gyurcsany_ferenc_2005 1.0000 0.0683 0.0684 #&gt; horn_gyula_1994 0.0683 1.0000 0.0587 #&gt; medgyessy_peter_2002 0.0684 0.0587 1.0000 #&gt; orban_viktor_1995 0.0734 0.0621 0.0650 #&gt; orban_viktor_2018 0.0503 0.0494 0.0504 #&gt; orban_viktor_1995 orban_viktor_2018 #&gt; antall_jozsef_1990 0.0778 0.0362 #&gt; bajnai_gordon_2009 0.0626 0.0617 #&gt; boross_peter_1993 0.0631 0.0401 #&gt; gyurcsany_ferenc_2005 0.0734 0.0503 #&gt; horn_gyula_1994 0.0621 0.0494 #&gt; medgyessy_peter_2002 0.0650 0.0504 #&gt; orban_viktor_1995 1.0000 0.0583 #&gt; orban_viktor_2018 0.0583 1.0000 Majd a textstat_dist() függvény segítségével kiszámoljuk a dokumentumok egymástól való különbözségét. mineln_dfm %&gt;% textstat_dist(margin = &quot;documents&quot;, method = &quot;euclidean&quot;) #&gt; textstat_dist object; method = &quot;euclidean&quot; #&gt; antall_jozsef_1990 bajnai_gordon_2009 boross_peter_1993 #&gt; antall_jozsef_1990 0 162.8 134.1 #&gt; bajnai_gordon_2009 163 0 84.6 #&gt; boross_peter_1993 134 84.6 0 #&gt; gyurcsany_ferenc_2005 186 137.8 149.7 #&gt; horn_gyula_1994 164 80.1 88.0 #&gt; medgyessy_peter_2002 160 68.1 81.8 #&gt; orban_viktor_1995 139 84.7 79.7 #&gt; orban_viktor_2018 167 67.3 85.4 #&gt; gyurcsany_ferenc_2005 horn_gyula_1994 medgyessy_peter_2002 #&gt; antall_jozsef_1990 186 163.6 160.2 #&gt; bajnai_gordon_2009 138 80.1 68.1 #&gt; boross_peter_1993 150 88.0 81.8 #&gt; gyurcsany_ferenc_2005 0 147.0 142.6 #&gt; horn_gyula_1994 147 0 75.9 #&gt; medgyessy_peter_2002 143 75.9 0 #&gt; orban_viktor_1995 147 89.6 77.5 #&gt; orban_viktor_2018 148 74.8 60.7 #&gt; orban_viktor_1995 orban_viktor_2018 #&gt; antall_jozsef_1990 139.2 167.4 #&gt; bajnai_gordon_2009 84.7 67.3 #&gt; boross_peter_1993 79.7 85.4 #&gt; gyurcsany_ferenc_2005 146.9 147.9 #&gt; horn_gyula_1994 89.6 74.8 #&gt; medgyessy_peter_2002 77.5 60.7 #&gt; orban_viktor_1995 0 83.6 #&gt; orban_viktor_2018 83.6 0 Ezután vizualizálhatjuk is a dokumentumok egymástól való távolságát egy olyan dendogram15 segítségével, amely megmutatja nekünk a lehetséges dokumentumpárokat. dist &lt;- mineln_dfm %&gt;% textstat_dist(margin = &quot;documents&quot;, method = &quot;euclidean&quot;) hierarchikus_klaszter &lt;- hclust(as.dist(dist)) ggdendrogram(hierarchikus_klaszter) Figure 5.3: A dokumentumok csoportosítása a távolságuk alapján mineln_dfm %&gt;% textstat_simil(y = mineln_dfm[, c(&quot;kormány&quot;)], margin = &quot;features&quot;, method = &quot;correlation&quot;) %&gt;% head(n = 10) #&gt; kormány #&gt; elnök -0.1185449 #&gt; tisztelt -0.5401371 #&gt; országgyulés 0.8082763 #&gt; hölgyeim -0.3580776 #&gt; uraim -0.3580776 #&gt; honfitársaim 0.8648335 #&gt; ünnepi 0.8737759 #&gt; pillanatban 0.8737759 #&gt; állok 0.6864778 #&gt; magyar 0.7530170 Arra is van lehetségünk, hogy a két alkorpuszt hasonlítsuk össze egymással. Ehhez a textstat_keyness() függvényt használjuk, melynek a bemenete a dfm. A függvény argumentumában a target = után kell megadni, hogy mely alkorpusz a viszonyítási alap. Az összehasonlítás eredményét a textplot_keyness() függvény segítségével ábrázolhatjuk, ami megjeleníti a két alkorpusz leggyakoribb kifejezéseit. dfm_keyness &lt;- dfm( corpus_mineln, groups = &quot;partoldal&quot;, remove = custom_stopwords, remove_punct = TRUE ) result_keyness &lt;- textstat_keyness(dfm_keyness, target = &quot;jobb&quot;) textplot_keyness(result_keyness, color = c(&quot;#484848&quot;, &quot;#D0D0D0&quot;)) + xlim(c(-65, 65)) + theme(legend.position = c(0.9,0.1)) Figure 5.4: A korpuszok legfontosabb kifejezései Ha az egyes miniszterelnökök beszédeinek leggyakoribb kifejezéseit szeretnénk összehasonlítani, azt a textstat_frequency() függvény segítségével tehetjük meg, melynek bemenete a megtisztított és súlyozott dfm. Az összehasonlítás eredményét pedig a ggplot2 segítségével ábrázolhatjuk is. dfm_weighted &lt;- corpus_mineln %&gt;% dfm( remove = custom_stopwords, tolower = TRUE, remove_punct = TRUE, stem = TRUE, remove_symbols = TRUE, remove_numbers = TRUE ) %&gt;% dfm_weight(scheme = &quot;prop&quot;) freq_weight &lt;- textstat_frequency(dfm_weighted, n = 5, groups = &quot;mineln&quot;) ggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) + geom_point() + facet_wrap(~ group, scales = &quot;free&quot;, ncol = 2) + coord_flip() + scale_x_continuous( breaks = nrow(freq_weight):1, labels = freq_weight$feature ) + labs( x = NULL, y = &quot;Relatív szófrekvencia&quot; ) Figure 5.5: Leggyakoribb kifejezések a miniszterelnöki beszédekben Mivel a szövegösszehasonlítás egy komplex kutatási feladat, a témával bbben is foglalkozunk a Szövegösszhasonlítás fejezetben. 5.5 Kulcsszavak kontextusa Arra is lehetségünk van, hogy egyes kulcszavakat a korpuszon belül szövegkörnyezetükben vizsgáljunk meg. Ehhez a kwic() függvényt használjuk, az argumentumok között a pattern = kifejezés után megadva azt a szót, amelyet vizsgálni szeretnénk, a window = után pedig megadhatjuk, hogy az adott szó hány szavas környezetére vagyunk kíváncsiak. kwic(corpus_mineln, pattern = &quot;válság*&quot;, valuetype = &quot;glob&quot;, window = 3, case_insensitive = TRUE) %&gt;% head(5) #&gt; #&gt; [antall_jozsef_1990, 1167] Átfogó és mély | válságba | #&gt; [antall_jozsef_1990, 1283] kell hárítanunk a | válságot | #&gt; [antall_jozsef_1990, 2772] és a lakásgazdálkodás | válságos | #&gt; [antall_jozsef_1990, 5226] gazdaság egészét juttatta | válságba | #&gt; [antall_jozsef_1990, 5286] gazdaság reménytelenül eladósodott | válsággócai | #&gt; #&gt; süllyedtünk a nyolcvanas #&gt; , de csakis #&gt; helyzetbe került. #&gt; , és amellyel #&gt; ellen. A A beszédeket a Hungarian Comparative Agendas Project miniszterelnöki beszéd korpuszából válogattuk: https://cap.tk.hu/vegrehajto A lubridate használatának részletes leírása megtalálható az alábbi linken: https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf A lambda leírása megtalálható itt: https://quanteda.io/reference/textstat_collocations.html A különböz indexek leírása megtalálható az alábbi linken: https://quanteda.io/reference/textstat_lexdiv.html (Schütze, Manning, and Raghavan 2008) A fentiekben láthattuk az eltéréseket a jobb és a bal oldali beszédeken belül, st ugyanahhoz a miniszterelnökhöz tartozó két beszéd között is. A következkben textstat_dist() és textstat_simil() függvények segítségével megvizsgáljuk, valójában mennyire hasonlítanak vagy különböznek ezek a beszédek. Mindkét függvény bemenete dmf, melybl elször egy súlyozott dfm-et készítünk, majd elvégezzük az összehasonlítást elször a jaccard-féle hasonlóság alapján. Olyan ábra, amely hasonlóságaik vagy különbségeik alapján csoportosított objektumok összefüggéseit mutatja meg. "],["sentiment.html", "6 Szótárak és érzelemelemzés 6.1 Szótárak az R-ben 6.2 Magyar Nemzet elemzése 6.3 MNB sajtóközlemények", " 6 Szótárak és érzelemelemzés A szentiment- vagy vélemény-, illetve érzelemelemzés a számítógépes nyelvészet részterülete, melynek célja az egyes szövegek tartalmából kinyerni azokat az információkat, amelyek értékelést fejeznek ki.16 A véleményelemzés a szövegeket három szinten osztályozza. A legáltalánosabb a dokumentumszint osztályozás, amikor egy hosszabb szövegegység egészét vizsgáljuk, míg a mondatszint osztályozásnál a vizsgálat alapegysége a mondat. A legrészletesebb adatokat akkor nyerjük, amikor az elemzést target-szinten végezzük, azaz meghatározzuk azt is, hogy egy-egy érzelem a szövegen belül mire vonatkozik. Mindhárom szinten azonos a feladat: egyrészt meg kell állapítani, hogy az adott egységben van-e értékelés, vélemény vagy érzelem, és ha igen, akkor pedig meg kell határozni, hogy milyen azok érzelmi tartalma. A pozitív-negatív-semleges skálán mozgó szentimentelemzés mellett az elmúlt két évtizedben jelents lépések történtek a szövegek emóciótartalmának automatikus vizsgálatára is. A módszer hasonló a szentimentelemzéshez, tartalmilag azonban más skálán mozog. Az emócióelemzés esetén ugyanis nem csak azt kell meghatározni, hogy egy kifejezés pozitív vagy negatív töltettel rendelkezik, hanem azt is, hogy milyen érzelmet (öröm, bánat, undor stb.) hordoz. A szótár alapú szentiment- vagy emócióelemzés alapja az az egyszer ötlet, hogy ha tudjuk, hogy egyes szavak milyen érzelmeket, érzéseket hordoznak, akkor ezeket a szavakat egy szövegben megszámolva képet kaphatunk az adott dokumentum érzelmi tartalmáról. Mivel a szótár alapú elemzés az adott kategórián belüli kulcsszavak gyakoriságán alapul, ezért van, aki nem tekinti statisztikai elemzésnek (lásd például Young and Soroka (2012)). A tágabb kvantitatív szövegelemzési kontextusban az osztályozáson (classification) belül a felügyelt módszerekhez hasonlóan itt is ismert kategóriákkal dolgozunk, azaz elre meghatározzuk, hogy egy-egy adott szó pozitív vagy negatív térték, vagy továbbmenve, milyen érzelmet hordoz, csak egyszerbb módszertannal (Grimmer and Stewart 2013). A kulcsszavakra építés miatt a módszer a kvalitatív és a kvantitatív kutatási vonalak találkozásának is tekinthet, hiszen egy-egy szónak az érzelmi töltete nem mindig ítélhet meg objektíven. Mint minden módszer esetében, itt is kiemelten fontos ellenrni, hogy a használt szótár kategóriák és kulcsszavak fedik-e a valóságot. Más szavakkal: validate, validate, validate. A módszer elnyei: Tökéletesen megbízható: a számításoknak nincs probabilisztikus (azaz valószínségre épül) eleme, mint például a Support Vector alapú osztályozásnak, illetve az emberi szövegkódolásnál elforduló problémákat is elkerüljük (például azt, hogy két kódoló, vagy ugyanazon kódoló két különböz idpontban nem azonosan értékeli ugyanazt a kifejezést). Általa képesek vagyunk mérni a szöveg látens dimenzióit. Széles körben alkalmazható, egyszeren számolható. A politikatudományon és a számítógépes nyelvészeten belül nagyon sok kész szótár elérhet, amelyek különböz módszerekkel készültek és különböz területet fednek le (például populizmus, pártprogramok policy tartalma, érzelmek, gazdasági tartalom). Relatíve könnyen adaptálható egyik nyelvi környezetbl a másikba, bár szótárfordítások esetén külön hangsúlyt kell fektetni a validálásra.17 A módszer lehetséges hátrányai: A szótár hatékonysága és validitása azon múlik, hogy mennyire egyezik a szótár és a vizsgálni kívánt dokumentum területe. Nem mindegy például, hogy a szótárunkkal tzsdei jelentések alapján a gazdasági bizonytalanságot vagy nézk filmekre adott értékeléseit szeretnénk-e vizsgálni. Léteznek általános szentimentszótárak, ezek hatékonysága azonban általában alulmúlja a terület-specifikus szótárakét. A terület-specifikus szótár építése kvalitatív folyamat, éppen ezért id- és emberi erforrás igényes. A szózsák alapú elemzéseknél a kontextus elvész. Gondoljunk például a tagadásra: a nem vagyok boldog kifejezés esetén egy általános szentiment szótár a tagadás miatt félreosztályozná a mondat érzelmi töltését, hiszen a boldog szó önmagában a pozitív kategóriába tartozik. Természetesen az automatikus tagadás kezelésére is vannak lehetségek, de a kérdés komplexitása miatt ezek bemutatásától most eltekintünk. A legnagyobb méret általános szentimentszótár az angol nyelv SentiWordNet (SWN), ami kb. 150 000 szót tartalmaz, amelyek mindegyike a három szentimentérték  pozitív, negatív, semleges  közül kapott egyet.18(Baccianella, Esuli, and Sebastiani 2010) Az R-ben végzett szentimentelemzés során az angol nyelv szövegekhez több beépített általános szentimentszótár is a rendelkezésünkre áll.19 A teljesség igénye nélkül említhetjük az AFINN,20 a bing21 és a az nrc22 szótárakat. Az elemzés sikere több faktortól is függ. Fontos, hogy a korpuszban lév dokumentumokat körültekinten tisztítsuk meg az elemzés elején (lásd a Korpuszépítés és elkészítés fejezetet). A következ lépésben meg kell bizonyosodnunk arról, hogy a kiválasztott szentiment szótár alkalmazható a korpuszunkra. Amennyiben nem találunk alkalmas szótárt, akkor a saját szótár validálására kell figyelni. A negyedik fejezetben leírtak itt is érvényesek, a dokumentum-kifejezés mátrixot érdemes valamilyen módon súlyozni. 6.1 Szótárak az R-ben A szótár alapú elemzéshez a quanteda csomagot fogjuk használni, illetve a 3. fejezetben már megismert readr, stringr, dplyr tidyverse csomagokat.23 library(stringr) library(dplyr) library(ggplot2) library(quanteda) library(HunMineR) Mieltt két esettanulmányt bemutatnánk, vizsgáljuk meg, hogyan néz ki egy szentimentszótár az R-ben. A szótárt kézzel úgy tudjuk készíthetünk, hogy egy listán belül létrehozzuk karaktervektorként a kategóriákat és a kulcsszavakat, és ezt a listát a quanteda dictionary függvényével eltároljuk. szentiment_szotar &lt;- dictionary( list( pozitiv = c(&quot;jó&quot;, &quot;boldog&quot;, &quot;öröm&quot;), negativ = c(&quot;rossz&quot;, &quot;szomorú&quot;, &quot;lehangoló&quot;) ) ) szentiment_szotar #&gt; Dictionary object with 2 key entries. #&gt; - [pozitiv]: #&gt; - jó, boldog, öröm #&gt; - [negativ]: #&gt; - rossz, szomorú, lehangoló A quanteda, quanteda.corpora és tidytext R csomagok több széles körben használt szentiment szótárat tartalmaznak, így nem kell kézzel replikálni minden egyes szótárat, amit használni szeretnénk. A szentiment elemzési munkafolyamat, amit ebben a részfejezetben bemutatunk, a következ lépésekbl áll: dokumentumok betöltése, szöveg elkészítése, a korpusz létrehozása, dokumentum-kifejezés mátrix létrehozása, szótár betöltése, a dokumentum-kifejezés mátrix szrése a szótárban lév kulcsszavakkal, az eredmény vizualizálása, további felhasználása. A fejezetben két különböz korpuszt fogunk elemezni: a 2006-os Magyar Nemzet címlapjainak egy 252 cikkbl álló mintáját vizsgáljuk egy magyar szentiment szótárral.24 A második korpusz a Magyar Nemzeti Bank angol nyelv sajtóközleményeibl áll, amin egy széles körben használt gazdasági szótár használatát mutatjuk be.25 6.2 Magyar Nemzet elemzése mn_minta &lt;- HunMineR::data_magyar_nemzet_small summary(mn_minta) #&gt; doc_id text doc_date #&gt; Min. : 1.0 Length:2834 Min. :2006-01-02 #&gt; 1st Qu.: 709.2 Class :character 1st Qu.:2006-03-29 #&gt; Median :1417.5 Mode :character Median :2006-06-28 #&gt; Mean :1417.5 Mean :2006-06-28 #&gt; 3rd Qu.:2125.8 3rd Qu.:2006-09-26 #&gt; Max. :2834.0 Max. :2006-12-29 A HunMineR csomag segítségével beolvassuk a Magyar Nemzet adatbázis egy kisebb részét, ami az esetünkben a 2006-os címlapokon szerepl híreket jelenti. A summary() parancs, ahogy a neve is mutatja, gyors áttekintést nyújt a betöltött adatbázisról. Látjuk, hogy 2834 sorból (megfigyelés) és 3 oszlopból (változó) áll. Els ránézésre látszik, hogy a text változónk tartalmazza a szövegeket, és hogy azok tisztításra szorulnak. Az els szöveget megnézve látjuk, hogy a standard elkészítési lépések mellett a sortörést (\\n) is ki kell törölnünk. mn_minta$text[1] #&gt; [1] &quot;Hat fovárosi képviselo öt percnél is kevesebbet beszélt egy év alatt a közgyulésben.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n&quot; Habár a quanteda is lehetséget ad néhány elkészít lépésre, érdemes ezt olyan céleszközzel tenni, ami nagyobb rugalmasságot ad a kezünkbe. Mi erre a célra a stringr csomagot használjuk. Els lépésben kitöröljük a sortöréseket (\\n), a központozást, a számokat és kisbetsítünk minden szót. Elfordulhat, hogy (számunkra nehezen látható) extra szóközök maradnak a szövegben. Ezeket az str_squish()függvénnyel tüntetjük el. A szöveg eleji és végi extra szóközöket (leading vagy trailing white space) az str_trim() függvény vágja le. mn_tiszta &lt;- mn_minta %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;\\n&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) A szöveg sokkal jobban néz ki, habár észrevehetjük, hogy maradhattak benne problémás részek, fleg a sortörés miatt, ami sajnos hol egyes szavak közepén van (a jobbik eset), vagy pedig pont szóhatáron, ez esetben a két szó sajnos összevonódik. Az egyszerség kedvéért feltételezzük, hogy ez kellen ritkán fordul el ahhoz, hogy ne befolyásolja az elemzésünk eredményét. mn_tiszta$text[1] #&gt; [1] &quot;hat fovárosi képviselo öt percnél is kevesebbet beszélt egy év alatt a közgyulésben&quot; Miután kész a tisztá(bb) szövegünk, korpuszt hozunk létre a quanteda corpus() függvényével. A corpus objektum a szöveg mellett egyéb dokumentum meta adatokat is tud tárolni (dátum, író, hely, stb.) Ezeket mi is hozzáadhatjuk (erre majd látunk példát), illetve amikor létrehozzuk a korpuszt a data frame-ünkbl, automatikusan metaadatokként tárolódnak az változóink. Jelen esetben az egyetlen dokumentum változónk a szöveg mellett a dátum lesz. A korpusz dokumentum változóihoz a docvars() függvény segítségével tudunk hozzáférni. mn_corpus &lt;- corpus(mn_tiszta) head(docvars(mn_corpus), 5) #&gt; doc_date #&gt; 1 2006-01-02 #&gt; 2 2006-01-02 #&gt; 3 2006-01-02 #&gt; 4 2006-01-02 #&gt; 5 2006-01-02 A következ lépés a dokumentum-kifejezés mátrix létrehozása a dfm() függvénnyel. Elször tokenekre bontjuk a szövegeket a tokens()paranccsal, és aztán ezt a tokenizált szózsákot kapja meg a dfm inputnak. A sor a végén a létrehozott mátrixunkat TF-IDF módszerrel súlyozzuk a dfm_tfidf() függvény használatával. mn_dfm &lt;- mn_corpus %&gt;% tokens(what = &quot;word&quot;) %&gt;% dfm() %&gt;% dfm_tfidf() A cikkek szentimentjét egy magyar szótárral fogjuk becsülni, amit a Társadalomtudományi Kutatóközpont kutatói a Mesterséges Intelligencia Nemzeti Laboratórium projekt keretében készítettek.26 Két dimenziót tarlamaz (pozitív és negatív), 2614 pozitív és 2654 negatív kulcsszóval. Ez nem számít kirívóan nagynak a szótárak között, mivel az adott kategóriák minél teljesebb lefedése a cél. poltext_szotar &lt;- HunMineR::dictionary_poltext poltext_szotar #&gt; Dictionary object with 2 key entries. #&gt; - [positive]: #&gt; - abszolút, ad, adaptív, adekvát, adócsökkentés, adókedvezmény, adomány, adományoz, adóreform, adottság, adottságú, áfacsökkentés, agilis, agytröszt, áhított, ajándék, ajándékoz, ajánl, ajánlott, akadálytalan [ ... and 2,279 more ] #&gt; - [negative]: #&gt; - aberrált, abnormális, abnormalitás, abszurd, abszurditás, ádáz, adócsalás, adócsaló, adós, adósság, áfacsalás, áfacsaló, affér, aggasztó, aggodalom, aggódik, aggódás, agresszió, agresszíven, agresszivitás [ ... and 2,568 more ] Az egyes dokumentumok szentimentjét a dfm_lookup() becsüli, ahol az elz lépésben létrehozott súlyozott dfm az input és a magyar szentimentszótár a dictionary. Egy gyors pillantás az eredményre és látjuk hogy minden dokumentumhoz készült egy pozitív és egy negatív érték. A TF-IDF súlyozás miatt nem látunk egész számokat (a súlyozás nélkül a sima szófrekvenciát kapnánk). mn_szentiment &lt;- dfm_lookup(mn_dfm, dictionary = poltext_szotar) head(mn_szentiment, 5) #&gt; Document-feature matrix of: 5 documents, 2 features (40.0% sparse) and 1 docvar. #&gt; features #&gt; docs positive negative #&gt; 1 0 0 #&gt; 2 0.8375026 12.497973 #&gt; 3 0 0 #&gt; 4 21.1044299 6.449036 #&gt; 5 11.0358129 8.131890 Ahhoz, hogy fel tudjuk használni a kapott eredményt, érdemes dokumentumváltozóként eltárolni a korpuszban. Ezt a fent már használt docvars() függvény segítségével tudjuk megtenni, ahol a második argumentumkét az új változó nevét adjuk meg. docvars(mn_corpus, &quot;pos&quot;) &lt;- as.numeric(mn_szentiment[, 1]) docvars(mn_corpus, &quot;neg&quot;) &lt;- as.numeric(mn_szentiment[, 2]) head(docvars(mn_corpus), 5) #&gt; doc_date pos neg #&gt; 1 2006-01-02 0.0000000 0.000000 #&gt; 2 2006-01-02 0.8375026 12.497973 #&gt; 3 2006-01-02 0.0000000 0.000000 #&gt; 4 2006-01-02 21.1044299 6.449036 #&gt; 5 2006-01-02 11.0358129 8.131890 Végül a kapott korpuszt a kiszámolt szentimentértékekkel a quanteda-ban lév convert() függvénnyel adattáblává alakítjuk. Aconvert() függvény dokumentációját érdemes elolvasni, mert ennek segítségével tudjuk a quanteda-ban elkészült objektumainkat átalakítani úgy, hogy azt más csomagok is tudják használni. mn_df &lt;- convert(mn_corpus, to = &quot;data.frame&quot;) summary(mn_df) #&gt; doc_id text doc_date pos #&gt; Length:2834 Length:2834 Min. :2006-01-02 Min. : 0.000 #&gt; Class :character Class :character 1st Qu.:2006-03-29 1st Qu.: 0.000 #&gt; Mode :character Mode :character Median :2006-06-28 Median : 2.373 #&gt; Mean :2006-06-28 Mean : 4.074 #&gt; 3rd Qu.:2006-09-26 3rd Qu.: 6.280 #&gt; Max. :2006-12-29 Max. :35.648 #&gt; neg #&gt; Min. : 0.000 #&gt; 1st Qu.: 0.000 #&gt; Median : 2.037 #&gt; Mean : 3.528 #&gt; 3rd Qu.: 5.348 #&gt; Max. :39.096 Mieltt vizualizálnánk az eredményt érdemes a napi szintre aggregálni a szentimentértéket és egy nettó értéket kalkulálni.27 mn_df &lt;- mn_df %&gt;% group_by(doc_date) %&gt;% summarise( daily_pos = sum(pos), daily_neg = sum(neg), net_daily = daily_pos - daily_neg ) A plot alapján a cikkek érzelemelemzése során több kiugrást is tapasztalhatunk. Természetesen messzemen következtetéseket egy ilyen kis korpusz alapján nem vonhatunk le, de a kiugrásokhoz tartozó cikkek kvalitatív vizsgálatával megállapíthatjuk, hogy az áprilisi kiugrást a választásokhoz kötd cikkek pozitív hangulata, míg az októberi negatív kilengést az öszödi beszéd nyilvánosságra kerüléséhez köthet cikkek negatív szentimentje okozza. ggplot(mn_df, aes(doc_date, net_daily)) + geom_line() + labs(y = &quot;Szentiment&quot;, x = NULL, caption = &quot;Adatforrás: https://cap.tk.hu/&quot;) Figure 6.1: Magyar Nemzet címlap szentimentje 6.3 MNB sajtóközlemények A második esettanulmányban a kontextuális szótárelemzést mutatjuk be egy angol nyelv korpusz és specializált szótár segítségével. A korpusz, az MNB kamatdöntéseit kísér nemzetközi sajtóközleményei, a szótár pedig a Loughran and McDonald (2011) pénzügyi szentimentszótár.28 penzugy_szentiment &lt;- HunMineR::dictionary_LoughranMcDonald penzugy_szentiment #&gt; Dictionary object with 9 key entries. #&gt; - [NEGATIVE]: #&gt; - abandon, abandoned, abandoning, abandonment, abandonments, abandons, abdicated, abdicates, abdicating, abdication, abdications, aberrant, aberration, aberrational, aberrations, abetting, abnormal, abnormalities, abnormality, abnormally [ ... and 2,335 more ] #&gt; - [POSITIVE]: #&gt; - able, abundance, abundant, acclaimed, accomplish, accomplished, accomplishes, accomplishing, accomplishment, accomplishments, achieve, achieved, achievement, achievements, achieves, achieving, adequately, advancement, advancements, advances [ ... and 334 more ] #&gt; - [UNCERTAINTY]: #&gt; - abeyance, abeyances, almost, alteration, alterations, ambiguities, ambiguity, ambiguous, anomalies, anomalous, anomalously, anomaly, anticipate, anticipated, anticipates, anticipating, anticipation, anticipations, apparent, apparently [ ... and 277 more ] #&gt; - [LITIGIOUS]: #&gt; - abovementioned, abrogate, abrogated, abrogates, abrogating, abrogation, abrogations, absolve, absolved, absolves, absolving, accession, accessions, acquirees, acquirors, acquit, acquits, acquittal, acquittals, acquittance [ ... and 883 more ] #&gt; - [CONSTRAINING]: #&gt; - abide, abiding, bound, bounded, commit, commitment, commitments, commits, committed, committing, compel, compelled, compelling, compels, comply, compulsion, compulsory, confine, confined, confinement [ ... and 164 more ] #&gt; - [SUPERFLUOUS]: #&gt; - aegis, amorphous, anticipatory, appertaining, assimilate, assimilating, assimilation, bifurcated, bifurcation, cessions, cognizable, concomitant, correlative, deconsolidation, delineation, demonstrable, demonstrably, derecognized, derecognizes, derivatively [ ... and 36 more ] #&gt; [ reached max_nkey ... 3 more keys ] A szentimentszótár 9 kategóriából áll. A legtöbb kulcsszó a negatív dimenzióhoz van (2355). A munkamenet hasonló a Magyar Nemzet-es példához: adat betöltés, szövegtisztítás, korpusz létrehozás, tokenizálás, kulcs kontextuális tokenek szrése, dfm elállítás és szentiment számítás, az eredmény vizualizálása, további felhasználása. mnb_pr &lt;- HunMineR::data_mnb_pr summary(mnb_pr) #&gt; date text id year #&gt; Min. :2005-01-24 Length:180 Min. : 1.00 Min. :2005 #&gt; 1st Qu.:2008-10-14 Class :character 1st Qu.: 45.75 1st Qu.:2008 #&gt; Median :2012-07-10 Mode :character Median : 90.50 Median :2012 #&gt; Mean :2012-07-08 Mean : 90.50 Mean :2012 #&gt; 3rd Qu.:2016-03-30 3rd Qu.:135.25 3rd Qu.:2016 #&gt; Max. :2019-12-17 Max. :180.00 Max. :2019 Adatbázisunk 180 megfigyelésbl és 4 változóból áll. Az egyetlen lényeges dokumentum metaadat itt is a szövegek megjelenési ideje. A szövegeket ugyanazokkal a standard eszközökkel kezeljük, mint a Magyar Nemzet esetében. Érdemes minden esetben ellenrizni, hogy az R-kód, amit használunk, tényleg azt csinálja-e, amit szeretnénk. Ez hatványozottan igaz abban az esetben, amikor szövegekkel és reguláris kifejezésekkel dolgozunk. mnb_tiszta &lt;- mnb_pr %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) Miután rendelkezésre állnak a tiszta dokumentumaink, egy karaktervektorba gyjtjük azokat a kulcsszavakat, amelyek környékén szeretnénk megfigyelni a szentiment alakulását. A példa kedvéért mi az unemp*, growth, gdp, inflation* szótöveket és szavakat választottuk. A tokens_keep() megtartja a kulcsszavainkat és egy általunk megadott +/- n tokenes környezetüket (jelen esetben 10). A szentimentelemzést pedig már ezen a jóval kisebb mátrixon fogjuk lefuttatni. A phrase() segítségével több szóból álló kifejezéséket is vizsgálhatunk. Ilyen szókapcsolat például az Európai Unió\" is, ahol lényeges, hogy egyben kezeljük a két szót. mnb_corpus &lt;- corpus(mnb_tiszta) gazdasag &lt;- c(&quot;unemp*&quot;, &quot;growth&quot;, &quot;gdp&quot;, &quot;inflation*&quot;, &quot;inflation expectation*&quot;) mnb_token &lt;- tokens(mnb_corpus) %&gt;% tokens_keep(pattern = phrase(gazdasag), window = 10) A szentimentet most is egy súlyozott dfm-bl számoljuk. A kész eredményt hozzáadjuk a korpuszhoz, majd adattáblát hozunk létre belle. A 9 kategóriából 5-öt használunk csak, amelyeknek jegybanki környezetben értelmezhet tartalma van. mnb_szentiment &lt;- tokens_lookup(mnb_token, dictionary = penzugy_szentiment) %&gt;% dfm() %&gt;% dfm_tfidf() docvars(mnb_corpus, &quot;negative&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;negative&quot;]) docvars(mnb_corpus, &quot;positive&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;positive&quot;]) docvars(mnb_corpus, &quot;uncertainty&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;uncertainty&quot;]) docvars(mnb_corpus, &quot;constraining&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;constraining&quot;]) docvars(mnb_corpus, &quot;superfluous&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;superfluous&quot;]) mnb_df &lt;- convert(mnb_corpus, to = &quot;data.frame&quot;) A célunk, hogy szentiment kategóriánkénti bontásban mutassuk be az elemzésünk eredményét, de eltte egy kicsit alakítani kell az adattáblán, hogy a korábban már tárgyalt tidy formára hozzuk. A különböz szentiment értékeket tartalmazó oszlopokat fogjuk átrendezni úgy, hogy kreálunk egy sent_type változót, ahol a kategória nevet fogjuk eltárolni és egy sent_score változót, ahol a szentiment értéket. Ehhez a tidyr-ben található pivot_longer() föggvényt használjuk. mnb_df &lt;- mnb_df %&gt;% pivot_longer( cols = negative:superfluous, names_to = &quot;sent_type&quot;, values_to = &quot;sent_score&quot; ) Az átalakítás után már könnyedén tudjuk kategóriákra bontva megjeleníteni az MNB közlemények különböz látens dimenzióit. Fontos emlékezni arra, hogy ez az eredmény a kulcsszavaink +/- 10 tokenes környezetében lév szavak szentimentjét méri. Ami érdekes eredmény, hogy a felesleges töltelék (superfluous) szövegek szinte soha nem fordulnak el a kulcsszavaink körül. A többi érték is nagyjából megfelel a várakozásainknak, habár a 2008-as gazdasági válság nem tnik kiugró pontnak. Azonban a 2010 utáni európai válság már láthatóan megjelenik az idsorainkban. Az általunk használt szótár alapveten az Egyesült Államokban a tzsdén keresked cégek publikus beszámolóiból készült, így elképzelhet, hogy egyes jegybanki környezetben sokat használt kifejezések nincsenek benne. A kapott eredmények validálása ezért is nagyon fontos, illetve érdemes azzal is tisztában lenni, hogy a szótáras módszer nem tökéletes (ahogy az emberi vagy más gépi kódolás sem). ggplot(mnb_df, aes(date, sent_score)) + geom_line() + labs( y = &quot;Szentiment&quot;, x = NULL ) + facet_wrap(~sent_type, ncol = 2) Figure 6.2: Magyar Nemzeti Bank közleményeinek szentimentje Bvebben lásd például: (Liu 2010) A lehetséges, területspecifikus szótáralkotási módszerekrl részletesebben ezekben a tanulmányokban lehet olvasni: Laver and Garry (2000); Young and Soroka (2012); Loughran and McDonald (2011); Máté, Sebk, and Barczikay (2021) A szótár és dokumentációja elérhet az alábbi linken: https://github.com/aesuli/SentiWordNet A quanteda.dictionaries csomag leírása és a benne található szótárak az alábbi github linken érhetek el: https://github.com/kbenoit/quanteda.dictionaries A szótár és dokumentációja elérhet itt: http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html A szótár és dokumentációja elérhet itt: https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html A szótár és dokumentációja elérhet itt: http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm A szentimentelemzéshez gyakran használt csomag még a tidytext. Online is szabadon elérhet könyvük Silge and Robinson (2017) 2. fejezetében részletesen is bemutatják a szerzk a tidytext munkafolyamatot: (https://www.tidytextmining.com/sentiment.html). A korpusz a Hungarian Compartive Agendas Project keretében készült és regisztáció után, kutatási célra elérhet az alábbi linken: https://cap.tk.hu/a-media-es-a-kozvelemeny-napirendje. A korpusz, a szótár és az elemzés teljes dokumentációja elérhet az alábbi github linken: https://github.com/poltextlab/central_bank_communication, a teljes elemzés (Máté, Sebk, and Barczikay 2021) elérhet: https://doi.org/10.1371/journal.pone.0245515 ELKH TK MILAB: https://milab.tk.hu/hu A szótár és a hozzátartozó dokumentáció elérhet az alábbi github oldalon: https://github.com/poltextlab/sentiment_hun A csoportosított adatokkal való munka bvebb bemutatását lásd a Függelékben. A témával részletesen foglalkozó tanulmányban egy saját monetáris szentimentszótárat mutatunk be: Az implementáció és a hozzá tartozó R forráskód nyilvános: https://doi.org/10.6084/m9.figshare.13526156.v1 "],["lda-ch.html", "7 Felügyelet nélküli tanulás 7.1 K-közép klaszterezés 7.2 LDA topik modellek29 7.3 Strukturális topik modellek", " 7 Felügyelet nélküli tanulás 7.1 K-közép klaszterezés A klaszterezés egy adathalmaz pontjainak, rekordjainak hasonlóság alapján való csoportosítása, ami szinte minden nagyméret adathalmaz leíró modellezésére alkalmas. A klaszterezés során az adatpontokat diszjunkt halmazokba, azaz klaszterekbe soroljuk, hogy az elemeknek egy olyan partíciója jöjjön létre, amelyben a közös csoportokba kerül elempárok lényegesen jobban hasonlítanak egymáshoz, mint azok a pontpárok, melyek két különböz csoportba sorolódtak. Klaszterezés során a megfelel csoportok kialakítása nem egyértelm feladat, mivel a különböz adatok eltér jelentése és felhasználása miatt adathalmazonként más szempontokat kell figyelembe vennünk. Egy klaszterezési feladat megoldásához ismernünk kell a különböz algoritmusok alapvet tulajdonságait és mindig szükség van az eredményként kapott klaszterezés kiértékelésére. Mivel egy klaszterezés az adatpontok hasonlóságából indul ki, ezért az eljárás során az els fontos lépés az adatpontok páronkénti hasonlóságát a lehet legjobban megragadó hasonlósági függvény kiválasztása (Tan, Steinbach, and Kumar 2011). Számos klaszterezési eljárás létezik, melyek között az egyik leggyakoribb különbségtétel, hogy a klaszterek egymásba ágyazottak vagy sem. Ez alapján beszélhetünk hierarchikus és felosztó klaszterezésrl. A hierarchikus klaszterezés egymásba ágyazott klaszterek egy fába rendezett halmaza, azaz ahol a klaszterek alklaszterekkel rendelkeznek. A fa minden csúcsa (klasztere), a levélcsúcsokat kivéve, a gyermekei (alklaszterei) uniója, és a fa gyökere az összes objektumot tartalmazó klaszter. Felosztó (partitional) klaszterezés esetén az adathalmazt olyan, nem átfed alcsoportokra bontjuk, ahol minden adatobjektum pontosan egy részhalmazba kerül (Tan, Steinbach, and Kumar 2011; Tikk 2007). A klaszterezési eljárások között aszerint is különbséget tehetünk, hogy azok egy objektumot csak egy vagy több klaszterbe is beilleszthetnek. Ez alapján beszélhetünk kizáró (exclusive), illetve nem-kizáró (non exclusive), vagy átfed (overlapping) klaszterezésrl. Az elbbi minden objektumot csak egyetlen klaszterhez rendel hozzá, az utóbbi esetén egy pont több klaszterbe is beleillik. Fuzzy klaszterezés esetén minden objektum minden klaszterbe beletartozik egy tagsági súly erejéig, melynek értéke 0 (egyáltalán nem tartozik bele) és 1 (teljesen beletartozik) közé esik. A klasztereknek is különböz típusai vannak, így beszélhetünk prototípus-alapú, gráf-alapú vagy srség-alapú klaszterekrl. A prototípus-alapú klaszter olyan objektumokat tartalmazó halmaz, amelynek mindegyik objektuma jobban hasonlít a klasztert definiáló objektumhoz, mint bármelyik másik klasztert definiáló objektumhoz. A prototípus-alapú klaszterek közül a K-közép klaszter az egyik leggyakrabban alkalmazott. A K-közép klaszterezési módszer els lépése k darab kezd középpont kijelölése, ahol k a klaszterek kívánt számával egyenl. Ezután minden adatpontot a hozzá legközelebb es középponthoz rendelünk. Az így képzett csoportok lesznek a kiinduló klaszterek. Ezután újra meghatározzuk mindegyik klaszter középpontját a klaszterhez rendelt pontok alapján. A hozzárendelési és frissítési lépéseket felváltva folytatjuk addig, amíg egyetlen pont sem vált klasztert, vagy ameddig a középpontok ugyanazok nem maradnak (Tan, Steinbach, and Kumar 2011). A K-közép klaszterezés tehát a dokumentumokat alkotó szavak alapján keresi meg a felhasználó által megadott számú (k) klasztert, amelyeket a középpontjaik képviselnek, és így rendezi a dokumentumokat csoportokba. A klaszterezés vagy csoportosítás egy induktív kategorizálás, ami akkor hasznos, amikor nem állnak a kutató rendelkezésére elzetesen ismert csoportok, amelyek szerint a vizsgált dokumentumokat rendezni tudná. Hiszen ebben az esetben a korpusz elemeinek rendezéséhez nem határozunk meg elzetesen csoportokat, hanem az eljárás során olyan különálló csoportokat hozunk létre a dokumentumokból, amelynek tagjai valamilyen szempontból hasonlítanak egymásra. A csoportosítás legfbb célja az, hogy az egy csoportba kerül szövegek minél inkább hasonlítsanak egymásra, miközben a különböz csoportba kerülk minél inkább eltérjenek egymástól. Azaz klaszterezésnél nem egy-egy szöveg jellemzire vagyunk kíváncsiak, hanem arra, hogy a szövegek egy-egy csoportja milyen hasonlóságokkal bír (Tikk 2007; Burtejin 2016). A gépi kódolással végzett klaszterezés egy felügyelet nélküli tanulás, mely a szöveg tulajdonságaiból tanul, anélkül, hogy elre meghatározott csoportokat ismerne. Alkalmazása során a dokumentum tulajdonságait és a modell becsléseit felhasználva jönnek létre a különböz kategóriák, melyekhez késbb hozzárendeli a szöveget (Grimmer and Stewart 2013). Az osztályozással ellentétben a csoportosítás esetén tehát nincs ismert címkékkel\" ellátott kategóriarendszer vagy olyan minta, mint az osztályozás esetében a tanítókörnyezet, amibl tanulva a modellt fel lehet építeni (Tikk 2007). A gépi kódolással végzett csoportosítás (klaszterezés) esetén a kutató feladata a megfelel csoportosító mechanizmus kiválasztása, mely alapján egy program végzi el a szövegek különböz kategóriákba sorolását. Ezt követi a hasonló szövegeket tömörít csoportok elnevezésének lépése. A több dokumentumból álló korpuszok esetében a gépi klaszterelemzés különösen eredményes és költséghatékony lehet, mivel egy nagy korpusz vizsgálata sok erforrást igényel (Grimmer and Stewart 2013, 1.). A klaszterezés bemutatásához a rendszerváltás utáni magyar miniszterelnökök egy-egy véletlenszeren kiválasztott beszédét használjuk. library(readr) library(dplyr) library(purrr) library(stringr) library(readtext) library(quanteda) library(tidytext) library(ggplot2) library(topicmodels) library(factoextra) library(stm) library(igraph) library(HunMineR) A beszédek szövege meglehetsen tiszta, ezért az egyszerség kedvéért most kihagyjuk a szövegtisztítás lépéseit. Az elemzés els lépéseként a .csv fájlból beolvasott szövegeinkbl a quanteda csomaggal korpuszt hozunk létre, majd abból egy dokumentum-kifejezés mátrixot készítünk a dfm() függvénnyel. Láthatjuk, hogy márixunk 7 megfigyelést és 4 változót tartalmaz. beszedek &lt;- HunMineR::data_miniszterelnokok beszedek_corpus &lt;- corpus(beszedek) beszedek_dfm &lt;- dfm(beszedek_corpus) A beszédek klaszterekbe rendezését az R egyik alapfüggvénye, a kmeans végzi. Els lépésben 3 klasztert készítünk. A table() függvénnyel megnézhetjük, hogy egy-egy csoportba hány dokumentum került. beszedek_klaszter &lt;- kmeans(beszedek_dfm, centers = 2) table(beszedek_klaszter$cluster) #&gt; #&gt; 1 2 #&gt; 2 5 A felügyelet nélküli klasszifikáció nagy kérdése, hány klasztert alakítsunk ki, hogy megközelítsük a valóságot, és ne csak mesterségesen kreáljunk csoportokat. Ez ugyanis azzal a kockázattal jár, hogy ténylegesen nem létez csoportok is létrejönnek. A klaszterek optimális számának meghatározására kvalitatív és kvantitatív lehetségeink is vannak. A következkben az utóbbira mutatunk példát, amihez a factoextra csomagot használjuk A lenti ábra azt mutatja, hogy a klasztereken belüli négyzetösszegek hogyan változnak a k paraméter változásának függvényében. Minél kisebb a klasztereken belüli négyzetösszegek értéke, annál közelebbi pontok tartoznak össze, így a kisebb értékekkel definiált klasztereket kapunk. Az ábra alapján tehát az ideális k 4 vagy 2, attól fuggen, hogy milyen feltevésekkel élünk a kutatásunk során. A 2-es érték azért lehet jó, mert a \\(k &gt; 2\\) értékek esetén a négyzetösszegek értéke nem csökken drasztikusan és a korpuszunk alapján a két (jobb-bal\") klaszter kvalitativ alapon is jól definiálható. A \\(k = 4\\) pedig azért lehet jó, mert utánna gyakorlatilag nem változik a kapott négyzetösszeg, ami azt jelzi, hogy a további klaszterek hozzáadásával nem lesz pontosabb a csoportosítás. fviz_nbclust(as.matrix(beszedek_dfm), kmeans, method = &quot;wss&quot;, k.max = 5, linecolor = &quot;black&quot;) + labs( title = NULL, x = &quot;Klaszterek száma&quot;, y = &quot;Klasztereken belüli négyzetösszeg&quot;) Figure 7.1: Optimális klaszterek száma A kialakított csoportokat vizuálisan is megjeleníthetjük. fviz_cluster( beszedek_klaszter, data = beszedek_dfm, pointsize = 2, repel = TRUE, ggtheme = theme_minimal() ) + labs( title = &quot;&quot;, x = &quot;Els dimenzió&quot;, y = &quot;Második dimenzió&quot; ) + theme(legend.position = &quot;none&quot;) Figure 7.2: A miniszterelnöki beszédek klaszterei 7.2 LDA topik modellek29 A topik modellezés a dokumentumok téma klasztereinek meghatározására szolgáló valószínség alapú eljárás, amely szógyakoriságot állapít meg minden témához, és minden dokumentumhoz hozzárendeli az adott témák valószínségét. A topik modellezés egy felügyelet nélküli tanulási módszer, amely során az alkalmazott algoritmus a dokumentum tulajdonságait és a modell becsléseit felhasználva hoz létre különböz kategóriákat, melyekhez késbb hozzárendeli a szöveget (Tikk 2007; Grimmer and Stewart 2013; Burtejin 2016). Az egyik leggyakrabban alkalmazott topik modellezési eljárás, a Látens Dirichlet Allokáció (LDA) alapja az a feltételezés, hogy minden korpusz topikok/témák keverékébl áll, ezen témák pedig statisztikailag a korpusz szókészlete valószínségi függvényeinek (eloszlásának) tekinthetek (Blei, Ng, and Jordan 2003). Az LDA a korpusz dokumentumainak csoportosítása során az egyes dokumentumokhoz topik szavakat rendel, a topikok megbecsléséhez pedig a szavak együttes megjelenését vizsgálja a dokumentum egészében. Az LDA algoritmusnak elzetesen meg kell adni a keresett klaszterek (azaz a keresett topikok) számát, ezt követen a dokumentumhalmazban szerepl szavak eloszlása alapján az algoritmus azonosítja a kulcsszavakat, amelyek eloszlása kirajzolja a topikokat (Blei, Ng, and Jordan 2003; Burtejin 2016; Jacobi, Van Atteveldt, and Welbers 2016). A következkben a magyar törvények korpuszán szemléltetjük a topik modellezés módszerét, hogy a mesterséges intelligencia segítségével feltárjuk a korpuszon belüli rejtett összefüggéseket. A korábban leírtak szerint tehát nincsenek elre meghatározott kategóriáink, dokumentumainkat a klaszterezés segítségével szeretnénk csoportosítani. Egy-egy dokumentumban keveredhetnek a témák és az azokat reprezentáló szavak. Mivel ugyanaz a szó több topikhoz is kapcsolódhat, így az eljárás komplex elemzési lehetséget nyújt, az egy szövegen belüli témák és akár azok dokumentumon belüli súlyának azonosítására. Példánkban csak a korpusz egy részén szemléltetjük a topik modellezést, a teljes korpusz és az annak elemzéséhez szükséges kód elérhet az alábbi GitHub linken: https://github.com/poltextlab. Az alábbiakban a 19982002-es és a 20022006-os parlamenti ciklus 1032 törvényszövegének topik modellezését és a szükséges elkészít, korpusztisztító lépéseket mutatjuk be. A fájlokat töltsük be az R által használt munkakönyvtárba.30 Töltsük be az elemezni kívánt csv fájlt, megadva az elérési útvonalát. torvenyek &lt;- HunMineR::data_lawtext_1998_2006 Az elz fejezetekben láthattuk, hogyan lehet használni a stringr csomagot a szövegtisztításra. A lépések a már megismert sztenderd folyamatot követik: számok, központozás, sortörések, extra szóközök eltávolítása, illetve a szöveg kisbetsítése. Az eddigieket további szövegtisztító lépésekkel is kiegészíthetjük. Olyan elemek esetében, amelyek nem feltétlenül különálló szavak és el akarjuk távolítani ket a korpuszból, szintén a str_remove_all() a legegyszerbb megoldás. torvenyek_tiszta &lt;- torvenyek %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;«&quot;), text = str_remove_all(string = text, pattern = &quot;»&quot;), text = str_remove_all(string = text, pattern = &quot;§&quot;), text = str_remove_all(string = text, pattern = &quot;°&quot;), text = str_remove_all(string = text, pattern = &quot;&lt;U+25A1&gt;&quot;), text = str_remove_all(string = text, pattern = &quot;@&quot;) ) A dokumentum változókat egy külön fájlból adjuk hozzá, ami a törvények keletkezési évét tartalmazza, illetve azt, hogy melyik kormányzati ciklusban születtek. Mindkét adatbázisban egy közös egyedi azonosító jelöli az egyes törvényeket, így ki tudjuk használni a dplyr left_join() függvényét, ami hatékonyan és gyorsan kapcsol össze adatbázisokat közös egyedi azonosító mentén. Jelen esetben ez az egyedi azonosító a txt_filename oszlopból fog elkészülni, amely a törvények neveit tartalmazza. Els lépésben betöltjük a metaadatokat tartalmazó adattáblát, majd a .txt rész eltti törvényneveket tartjuk csak meg a létrehozott doc_id- oszlopban. A [^\\\\.]* regular expression itt a string elejétl indulva kijelöl mindent az elso . karakterig. A str_extract() pedig ezt a kijelölt string szakaszt (ami a törvények neve) menti át az új változónkba. torveny_meta &lt;- HunMineR::data_lawtext_meta torveny_meta &lt;- torveny_meta %&gt;% mutate(doc_id = str_extract(txt_filename, &quot;[^\\\\.]*&quot;)) %&gt;% select(-txt_filename) head(torveny_meta, 5) #&gt; # A tibble: 5 x 4 #&gt; year electoral_cycle majortopic doc_id #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1998 1998-2002 13 1998XXXV #&gt; 2 1998 1998-2002 20 1998XXXVI #&gt; 3 1998 1998-2002 3 1998XXXVII #&gt; 4 1998 1998-2002 6 1998XXXVIII #&gt; 5 1998 1998-2002 13 1998XXXIX Végül összefzzük a dokumentumokat és a metaadatokat tartalmazó data frame-eket. torveny_final &lt;- left_join(torvenyek_tiszta, torveny_meta, by = &quot;doc_id&quot;) Majd létrehozzuk a korpuszt és ellenrizzük azt. #&gt; Text Types Tokens Sentences year electoral_cycle majortopic #&gt; 1 1998L 2879 9628 1 1998 1998-2002 3 #&gt; 2 1998LI 352 680 1 1998 1998-2002 20 #&gt; 3 1998LII 446 992 1 1998 1998-2002 9 #&gt; 4 1998LIII 126 221 1 1998 1998-2002 9 #&gt; 5 1998LIV 835 2013 1 1998 1998-2002 9 Az RStudio environments fülén láthatjuk, hogy egy 1032 elembl álló korpusz jött létre, amelynek tartalmát a summary() paranccsal kiíratva, a console ablakban megjelenik a dokumentumok listája és a fbb leíró statisztikai adatok (egyedi szavak  types; szószám  tokens; mondatok  sentences). Az elbbi fejezettl eltéren most a tokenizálás során is végzünk még egy kis tisztítást: a felesleges stop szavakat kitöröljük a tokens_remove() és stopwords() kombinálásával. A quanteda tartalmaz egy beépített magyar stopszó szótárat. A második lépésben szótövesítjük a tokeneket a tokens_words() használatával, ami szintén képes a magyar nyelv szövegeket kezelni. Szükség esetén a beépített magyar nyelv stopszó szótárat saját stopszavakkal is kiegészíthetjük. Példaként a HunMineR csomagban lév kiegészít stopszó data frame-t töltsük be. custom_stopwords &lt;- HunMineR::data_legal_stopwords Mivel a korpusz ellenrzése során találunk még olyan kifejezéseket, amelyeket el szeretnénk távolítani, ezeket is kiszrjük. custom_stopwords_egyeb &lt;- c(&quot;lábjegyzet&quot;, &quot;országgylés&quot;, &quot;ülésnap&quot;) Aztán pedig a korábban már megismert pipe operáror használatával elkészítjük a token objektumunkat. A szótövesített tokeneket egy külön objektumban tároljuk, mert gyakran elfordul, hogy késbb vissza kell térnünk az eredeti token objektumhoz, hogy egyéb mveleteket végezzünk el, például további stopszavakat távolítsunk el. torvenyek_tokens &lt;- tokens(torvenyek_corpus) %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% tokens_remove(custom_stopwords) %&gt;% tokens_remove(custom_stopwords_egyeb) %&gt;% tokens_wordstem(language = &quot;hun&quot;) Végül eltávolítjuk a dokumentum-kifejezés mátrixból a túl gyakori kifejezéseket. A dfm_trim() függvénnyel a nagyon ritka és nagyon gyakori szavak megjelenését kontrollálhatjuk. Ha termfreq_type opció értéke prop (úgymint proportional) akkor 0 és 1.0 közötti értéket vehetnek fel a max_termfreq/docfreq és min_termfreq/docfreq paraméterek. A lenti példában azokat a tokeneket tartjuk meg, amelyek legalább egyszer elfordulnak ezer dokumentumonként (így kizárva a nagyon ritka kifejezéseket). torvenyek_dfm &lt;- dfm(torvenyek_tokens) %&gt;% dfm_trim(min_termfreq = 0.001, termfreq_type = &quot;prop&quot;) A szövegtisztító lépesek eredményét úgy ellenrizhetjük, hogy a 2. fejezetben bemutatottak szerint szógyakorisági listát készítünk a korpuszban maradt kifejezésekrl. Itt kihasználhatjuk a korpuszunkban lév metaadatokat és megnézhetjük ciklus szerinti bontásban a szófrekvencia ábrát. Az ábránál figyeljünk arra, hogy a tidytext reorder_within függvényét használjuk, ami egy nagyon hasznos megoldás a csoportosított sorrendbe rendezésre a ggplot2 ábránál. top_tokens &lt;- textstat_frequency(torvenyek_dfm, n = 15, groups = docvars(torvenyek_dfm, field = &quot;electoral_cycle&quot;)) ggplot(top_tokens, aes(reorder_within(feature, frequency, group), frequency)) + geom_point(aes(shape = group), size = 2) + coord_flip() + labs(y = NULL, x = &quot;szófrekvencia&quot;) + facet_wrap(~group, nrow = 2, scales = &quot;free&quot;) + tidytext::scale_x_reordered() + theme(legend.position = &quot;none&quot;) Figure 7.3: A 15 leggyakoribb token a korpuszban A szövegtisztító lépéseket késbb újabbakkal is kiegészíthetjük, ha észrevesszük, hogy az elemzést zavaró tisztítási lépés maradt ki. Ilyen esetben tovább tisztíthatjuk a korpuszt, majd újra lefuttathatjuk az elemzést. Például, ha szükséges, további stopszavak eltávolítását is elvégezhetjük egy újabb stopszólista hozzáadásával. Ilyenkor ugyanúgy járunk el, mint az elz stopszólista esetén, vagyis beolvassuk a munkakönyvtárban elhelyezett .csv fájlt, a beolvasott stopszólistából karaktervektort, majd objektumot hozunk létre, végezetül pedig ezeket a szavakat is eltávolítjuk a kopuszból. custom_stopwords2 &lt;- HunMineR::data_legal_stopwords2 torvenyek_tokens_final &lt;- torvenyek_tokens %&gt;% tokens_remove(custom_stopwords2) Ezután újra ellenrizzük az eredményt. torvenyek_dfm_final &lt;- dfm(torvenyek_tokens_final) %&gt;% dfm_trim(min_termfreq = 0.001, termfreq_type = &quot;prop&quot;) top_tokens_final &lt;- textstat_frequency(torvenyek_dfm_final, n = 15, groups = docvars(torvenyek_dfm, field = &quot;electoral_cycle&quot;)) Ezt egy ábrán is megjelenítjük. ggplot(top_tokens_final, aes(reorder_within(feature, frequency, group), frequency)) + geom_point(aes(shape = group), size = 2) + coord_flip() + labs( y = NULL, x = &quot;szófrekvencia&quot; ) + facet_wrap(~group, nrow = 2, scales = &quot;free&quot;) + tidytext::scale_x_reordered() + theme(legend.position = &quot;none&quot;) Figure 7.4: A 15 leggyakoribb token a korpuszban, a bovített stop szó listával A szövegtisztító és a korpusz elkészít mveletek után következhet az LDA illesztése. Az alábbiakban az LDA illesztés két módszerét, a VEM-et és a Gibbs-et mutatjuk be. A modell mindkét módszer esetén ugyanaz, a különbség a következtetés módjában van. A VEM módszer variációs következtetés, míg a Gibbs mintavételen alapuló következtetés. (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004; Phan, Nguyen, and Horiguchi 2008). A két modell illesztése nagyon hasonló, meg kell adnunk az elemezni kívánt dfm nevét, majd a k\" értékét, ami egyenl az általunk létrehozni kívánt topikok számával, ezt követen meg kell jelölnünk, hogy a VEM vagy a Gibbs módszert alkalmazzuk. A set.seed() funkció az R véletlen szám generátor magjának beállítására szolgál, ami ahhoz kell, hogy a kapott eredmények, ábrák stb. pontosan reprodukálhatóak legyenek. A set.seed() bármilyen tetszleges egész szám lehet. Mivel az elemzésünk célja a két ciklus jogalkotásának összehasonlítása, a korpuszunkat két alkorpuszra bontjuk, ehhez a dokumentumok kormányzati ciklus azonosítóját használjuk fel. A dokumentum változók alapján a dfm_subset() parancs segítségével választjuk szét a már elkészült és a tisztított mátrixunkat. dfm_98_02 &lt;- dfm_subset(torvenyek_dfm_final, electoral_cycle == &quot;1998-2002&quot;) dfm_02_06 &lt;- dfm_subset(torvenyek_dfm_final, electoral_cycle == &quot;2002-2006&quot;) 7.2.1 A VEM módszer alkalmazása a magyar törvények korpuszán Saját korpuszunkon elször a VEM módszert alkalmazzuk, ahol k = 10, azaz a modell 10 témacsoportot alakít ki. Ahogyan korábban arról már volt szó, a k értékének meghatározása kutatói döntésen alapul, a modell futtatása során bevett gyakorlat a különböz k értékekkel való kísérletezés. Az elkészült modell kiértékelésére az elemzés elkészülte után a perplexity() függvény segítségével van lehetségünk  ahol a theta az adott topikhoz való tartozás valószínsége. A függvény a topikok által reprezentált elméleti szóeloszlásokat hasonlítja össze a szavak tényleges eloszlásával a dokumentumokban. A függvény értéke nem önmagában értelmezend, hanem két modell összehasonlításában, ahol a legalacsonyabb perplexity (zavarosság) értékkel rendelkez modellt tekintik a legjobbnak.31 Az illusztráció kedvéért lefuttatunk 4 LDA modellt az 19982002-es kormányzati ciklushoz tartozó dfm-en. Az iterációhoz a purrr csomag map függvényét használtuk. Fontos megjegyezni, hogy minél nagyobb a korpuszunk, annál több számítási kapacitásra van szükség (és annál tovább tart a számítás). k_topics &lt;- c(5, 10, 15, 20) lda_98_02 &lt;- k_topics %&gt;% map(LDA, x = dfm_98_02, control = list(seed = 1234)) perp_df &lt;- tibble( k = k_topics, perplexity = map_dbl(lda_98_02, perplexity) ) ggplot(perp_df, aes(k, perplexity)) + geom_point() + geom_line() + labs( x = &quot;Klaszterek száma&quot;, y = &quot;Zavarosság&quot; ) Figure 7.5: Zavarosság változása a k függvényében A zavarossági mutató alapján a 20 topikos modell szerepel a legjobban, de a megfelel k kiválasztása a kutató kvalitatív döntésén múlik. A zavarossági pontszám ehhez ad kvantitatív szempontokat.32 A reprodukálhatóság és futási sebesség érdekében a fejezet további részeiben a k paraméternek 10-es értéket adunk. Ezzel lefuttatunk egy-egy modellt a két ciklusra. vem_98_02 &lt;- LDA(dfm_98_02, k = 10, method = &quot;VEM&quot;, control = list(seed = 1234)) vem_02_06 &lt;- LDA(dfm_02_06, k = 10, method = &quot;VEM&quot;, control = list(seed = 1234)) Ezt követen a modell által létrehozott topic-okat tidy formátumba tesszük és egyesítjük egy adattáblában.33 topics_98_02 &lt;- tidy(vem_98_02, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;1998-2002&quot;) topics_02_06 &lt;- tidy(vem_02_06, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;2002-2006&quot;) lda_vem &lt;- bind_rows(topics_98_02, topics_02_06) Ezután listázzuk az egyes topikokhoz tartozó leggyakoribb kifejezéseket. top_terms &lt;- lda_vem %&gt;% group_by(electoral_cycle, topic) %&gt;% top_n(5, beta) %&gt;% top_n(5, term) %&gt;% ungroup() %&gt;% arrange(topic, -beta) Végül a ggplot2 csomag segítségével ábrán is megjeleníthetjük az egyes topikok 10 legfontosabb kifejezését. top_terms %&gt;% filter(electoral_cycle == &quot;1998-2002&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 2) + coord_flip() + labs( x = NULL, y = expression(beta) ) + tidytext::scale_x_reordered() Figure 7.6: 19982002-es ciklus topikok és kifejezések (VEM mintavételezéssel) top_terms %&gt;% filter(electoral_cycle == &quot;2002-2006&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 2) + coord_flip() + labs( x = NULL, y = expression(beta) ) + tidytext::scale_x_reordered() Figure 7.7: 20022006-os ciklus topikok és kifejezések (VEM mintavételezéssel) 7.2.2 Az LDA Gibbs módszer alkalmazása a magyar törvények korpuszán A következkben ugyanazon a korpuszon az LDA Gibbs módszert alkalmazzuk. A szövegelkészít és tisztító lépések ennél a módszernél is ugyanazok, mint a fentebb bemutatott VEM módszer esetében, így itt most csak a modell illesztését mutatjuk be. gibbs_98_02 &lt;- LDA(dfm_98_02, k = 10, method = &quot;Gibbs&quot;, control = list(seed = 1234)) gibbs_02_06 &lt;- LDA(dfm_02_06, k = 10, method = &quot;Gibbs&quot;, control = list(seed = 1234)) Itt is elvégezzük a topikok tidy formátumra alakítását. topics_g98_02 &lt;- tidy(gibbs_98_02, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;1998-2002&quot;) topics_g02_06 &lt;- tidy(gibbs_02_06, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;2002-2006&quot;) lda_gibbs &lt;- bind_rows(topics_g98_02, topics_g02_06) Majd listázzuk az egyes topikokhoz tartozó leggyakoribb kifejezéseket. top_terms_gibbs &lt;- lda_gibbs %&gt;% group_by(electoral_cycle, topic) %&gt;% top_n(5, beta) %&gt;% top_n(5, term) %&gt;% ungroup() %&gt;% arrange(topic, -beta) Ezután a ggplot2 csomag segítségével ábrán is megjeleníthetjük. top_terms_gibbs %&gt;% filter(electoral_cycle == &quot;1998-2002&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 2) + coord_flip() + labs( title = , x = NULL, y = expression(beta) ) + tidytext::scale_x_reordered() Figure 7.8: 19982002-es ciklus topikok és kifejezések (Gibbs mintavétellel) top_terms_gibbs %&gt;% filter(electoral_cycle == &quot;2002-2006&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 2) + coord_flip() + labs( x = NULL, y = expression(beta) ) + scale_x_reordered() Figure 7.9: 20022006-os ciklus topikok és kifejezések (Gibbs mintavétellel) 7.3 Strukturális topik modellek A kvantitatív szövegelemzés elterjedésével együtt megjelentek a módszertani innovációk is. Roberts et al. (2014) kiváló cikkben mutatták be a strukturális topic modelleket (structural topic models  stm), amelyek f újítása, hogy a dokumentumok metaadatai kovariánsként34 tudják befolyásolni, hogy egy-egy kifejezés mekkora valószínséggel lesz egy-egy téma része. A kovariánsok egyrészrl megmagyarázhatják, hogy egy-egy dokumentum mennyire függ össze egy-egy témával (topical prevalence), illetve hogy egy-egy szó mennyire függ össze egy-egy témán belül (topical content). Az stm modell becslése során mindkét típusú kovariánst használhatjuk, illetve ha nem adunk meg dokumentum metaadatot, akkor az stm csomag stm függvénye a Korrelált Topic Modell-t fogja becsülni. Az stm modelleket az R-ben az stm csomaggal tudjuk kivitelezni. A csomag fejleszti között van a módszer kidolgozója is, ami nem ritka az R csomagok esetében. A lenti lépésekben a csomag dokumentációjában szerepl ajánlásokat követjük, habár a könyv írásakor a stm már képes volt a quanteda-ban létrehozott dfm-ek kezelésére is. A kiinduló adatbázisunk a törvény_final, amit a fejezet elején hoztunk létre a dokumentumokból és a metaadatokból. A javasolt munkafolyamat a textProcessor() függvény használatával indul, ami szintén tartalmazza az alap szöveg elkészítési lépéseket. Az egyszerség és a futási sebesség érdekében itt most ezek többségétl eltekintünk, mivel a fejezet korábbi részeiben részletesen tárgyaltuk ket. Az elkészítés utolsó szakaszában az out objektumban tároljuk el a dokumentumokat, az egyedi szavakat, illetve a metaadatokat (kovariánsokat). data_stm &lt;- torveny_final processed_stm &lt;- textProcessor( torveny_final$text, metadata = torveny_final, lowercase = FALSE, removestopwords = FALSE, removenumbers = FALSE, removepunctuation = FALSE, ucp = FALSE, stem = TRUE, language = &quot;hungarian&quot;, verbose = FALSE ) out &lt;- prepDocuments(processed_stm$documents, processed_stm$vocab, processed_stm$meta) A strukturális topic modellünket az stm függvénnyel becsüljük és a kovariánsokat a prevalence opciónál tudjuk formulaként megadni. A lenti példában a Hungarian Comparative Agendas Project35 kategóriáit (például gazdaság, egészségügy stb.) és a kormányciklusokat használjuk. A futási id kicsit hosszabb mint az LDA modellek esetében. stm_fit &lt;- stm( out$documents, out$vocab, K = 10, prevalence = ~ majortopic + electoral_cycle, data = out$meta, init.type = &quot;Spectral&quot;, seed = 1234, verbose = FALSE ) Amennyiben a kutatási kérdés megkívánja, akkor megvizsgálhatjuk, hogy a kategorikus változóinknak milyen hatása volt az egyes topikok esetében. Ehhez az estimateEffect() függvénnyel lefuttatunk egy lineáris regressziót és a summary() használatával láthatjuk az egyes kovariánsok koefficienseit. Itt az els topikkal illusztráljuk az eredményt, ami azt mutatja, hogy (a kategórikus változóink els kategóriájához mérten) statisztikailag szignifikáns mind a téma, mind pedig a kormányzati ciklusok abban, hogy egyes dokumentumok milyen témákból épülnek fel. out$meta$electoral_cycle &lt;- as.factor(out$meta$electoral_cycle) out$meta$majortopic &lt;- as.factor(out$meta$majortopic) cov_estimate &lt;- estimateEffect(1:10 ~ majortopic + electoral_cycle, stm_fit, meta = out$meta, uncertainty = &quot;Global&quot;) summary(cov_estimate, topics = 1) #&gt; #&gt; Call: #&gt; estimateEffect(formula = 1:10 ~ majortopic + electoral_cycle, #&gt; stmobj = stm_fit, metadata = out$meta, uncertainty = &quot;Global&quot;) #&gt; #&gt; #&gt; Topic 1: #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.30415 0.03037 10.016 &lt; 2e-16 *** #&gt; majortopic2 -0.21244 0.06279 -3.383 0.000744 *** #&gt; majortopic3 -0.21468 0.05982 -3.589 0.000348 *** #&gt; majortopic4 -0.22654 0.05695 -3.978 7.45e-05 *** #&gt; majortopic5 0.10095 0.04808 2.100 0.035996 * #&gt; majortopic6 -0.22807 0.05747 -3.968 7.75e-05 *** #&gt; majortopic7 -0.16730 0.06390 -2.618 0.008972 ** #&gt; majortopic8 -0.21106 0.07374 -2.862 0.004295 ** #&gt; majortopic9 0.48594 0.09943 4.887 1.19e-06 *** #&gt; majortopic10 -0.11776 0.05220 -2.256 0.024292 * #&gt; majortopic12 -0.17952 0.04137 -4.339 1.57e-05 *** #&gt; majortopic13 -0.13749 0.05578 -2.465 0.013867 * #&gt; majortopic14 -0.21884 0.07493 -2.921 0.003571 ** #&gt; majortopic15 -0.14868 0.04301 -3.457 0.000568 *** #&gt; majortopic16 -0.10434 0.05516 -1.892 0.058829 . #&gt; majortopic17 -0.22342 0.05843 -3.824 0.000139 *** #&gt; majortopic18 0.22371 0.05670 3.945 8.52e-05 *** #&gt; majortopic19 0.05846 0.05144 1.136 0.256031 #&gt; majortopic20 -0.21748 0.03986 -5.456 6.14e-08 *** #&gt; majortopic21 -0.22344 0.06910 -3.233 0.001263 ** #&gt; majortopic23 -0.17838 0.09359 -1.906 0.056934 . #&gt; electoral_cycle2002-2006 -0.09963 0.01887 -5.279 1.59e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Az LDA modelleknél már bemutatott munkafolyamat az stm modellünk esetében is alkalmazható, hogy vizuálisan is megjelenítsük az eredményeinket. A tidy() függvény data frammé alakítja az stm objektumot, amit aztán a már ismers dplyr csomagban lév függvényekkel tudunk átalakítani és végül vizualizálni a ggplot2 csomaggal. A lenti ábrán az egyes témákhoz tartozó 5 legvalószínbb szót mutatjuk be. tidy_stm &lt;- tidy(stm_fit) tidy_stm %&gt;% group_by(topic) %&gt;% top_n(5, beta) %&gt;% ungroup() %&gt;% mutate( topic = paste0(&quot;Topic &quot;, topic), term = reorder_within(term, beta, topic) ) %&gt;% ggplot(aes(term, beta)) + geom_col() + facet_wrap(~topic, scales = &quot;free_y&quot;, ncol = 3) + coord_flip() + scale_x_reordered() + labs( x = NULL, y = expression(beta) ) Figure 7.10: Topikonkénti legmagasabb valószínuségu szavak Egy-egy topichoz tartozó meghatározó szavak annak függvényében változhatnak, hogy milyen algoritmust használunk. A labelTopics() függvény a már becsült stm modellünket alapul véve kínál négyféle alternatív opciót. Az egyes algoritmusok részletes magyarázatáért érdemes elolvasni a csomag részletes leírását.36 labelTopics(stm_fit, c(1:2)) #&gt; Topic 1 Top Words: #&gt; Highest Prob: szerzodo, vagi, egyezméni, fél, államban, nem, másik #&gt; FREX: megadóztatható, haszonhúzója, beruházóinak, segélycsapatok, adóztatást, jövedelemadók, kijelölések #&gt; Lift: árucikkeket, átalányösszegben, átléphetik, átszállítást, beruházóikat, célországban, cikktanulók #&gt; Score: szerzodo, államban, illetoségu, egyezméni, megadóztatható, adóztatható, cikka #&gt; Topic 2 Top Words: #&gt; Highest Prob: muködési, célú, támogatások, költségvetésegyéb, felhalmozási, terhelo, beruházási #&gt; FREX: kiadásokfelújításegyéb, kiadásokintézményi, kiadásokközponti, költségvetésfelhalmozási, kiadásokkormányzati, felújításegyéb, rek #&gt; Lift: a+b+c, a+b+c+d, adago, adódóa, adósságállományából, adósságrendezésr, adótartozásának #&gt; Score: költségvetésegyéb, költségvetésszemélyi, kiadásokfelhalmozási, járulékokdolog, költségvetésintézményi, kiadásokegyéb, juttatásokmunkaadókat A korpuszunkon belüli témák megoszlását a plot.STM()-el tudjuk ábrázolni. Jól látszik, hogy a Topic 6-ba tartozó szavak vannak jelen a legnagyobb arányban a dokumentumaink között. plot.STM(stm_fit, &quot;summary&quot;, main = &quot;&quot;, labeltype = &quot;frex&quot;, xlab = &quot;Várható topic arányok&quot;, xlim=c(0,1) ) Figure 7.11: Leggyakoribb témák és kifejezések Végezetül a témák közötti korrelációt a topicCorr függvénnyel becsülhetjük és az igraph csomagot betöltve a plot() paranccsal tudjuk vizualizálni. Az eredmény egy hálózat lesz, amit gráfként ábrázolunk. A gráfok élei a témák közötti összefüggést (korrelációt) jelölik. plot(topicCorr(stm_fit)) Figure 7.12: Témák közötti korreláció hálózat A kód részben az alábbiakon alapul: tidytextmining.com/topicmodeling.html. Az általunk is használt topicmodels csomag interfészt biztosít az LDA modellek és a korrelált témamodellek (CTM) C kódjához, valamint az LDA modellek illesztéséhez szükséges C ++ kódhoz. A törvényeket és a metaadatokat tartalmazó adatbázisokat regisztációt követen a Hungarian Comparative Agendas Projekt honlapjáról https://cap.tk.hu/ lehet letölteni. Részletesebben lásd például: http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/ A ldatuning csomagban további indikátor implementációja található, ami a perplexityhez hasonlóan minimalizásra alapoz (Arun et al. 2010; Cao et al. 2009), illetve maximalizálásra alapoz (Deveaud, SanJuan, and Bellot 2014; Griffiths and Steyvers 2004) A tidy formátumról bvebben: https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html A kovariancia megadja két egymástól különböz változó együttmozgását. Kis értékei gyenge, nagy értékei ers lineáris összefüggésre utalnak. A kódkönyv elérhet az alábbi linken: Comparative Agendas Project. Az stm csomaghoz tartozó leírás: https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf "],["embedding.html", "8 Szóbeágyazások 8.1 A szóbeágyazás célja 8.2 Word2Vec és GloVe", " 8 Szóbeágyazások 8.1 A szóbeágyazás célja Az eddigi fejezetekben elssorban a szózsák (bag of words) alapú módszerek voltak eltérben. A szózsák alapú módszerekkel szemben, amelyek alkalmazása során elveszik a kontextuális tartalom, a szóbeágyazáson (word embedding) alapuló modellek kimondottan a kontextuális információt ragadják meg. A szóbeágyazás a topikmodellekhez hasonlóan a felügyelet nélküli tanulás módszerére épül, azonban itt a dokumentum domináns kifejezéseinek és témáinak feltárása helyett a szavak közötti szemantikai kapcsolat megértése a cél. Vagyis a modellnek képesnek kell lennie az egyes szavak esetén szinonimáik és ellentétpárjaik megtalálására. A hagyományos topikmodellezés esetén a modell a szavak dokumentumokon belüli együttes megjelenési statisztikái alapján becsül dokumentum-topik, illetve topik-szó eloszlásokat, azzal a céllal, hogy koherens téma-csoportokat képezzen. Ezzel szemben a szóbeágyazás legújabb iskolája már neurális halókon alapul. A neurális háló a tanítási folyamata során az egyes szavak vektorreprezentációját állítja el. A vektorok jellemzen 100300 dimenzióból állnak, a távolságuk alapján pedig megállapítható, hogy az egyes kifejezések milyen szemantikai kapcsolatban állnak egymással. A szóbeágyazás célja tehát a szemantikai relációk feltárása. A szavak vektorizálásának köszönheten bármely (a korpuszunkban szerepl) tetszleges számú szóról eldönthetjük, hogy azok milyen szemantikai kapcsolatban állnak egymással, azaz szinonimaként vagy ellentétes fogalompárként szerepelnek. A szóvektorokon dimenziócsökkent eljárást alkalmazva, s a multidimenzionális (100300 dimenziós) teret 2 dimenziósra szkítve könnyen vizualizálhatjuk is a korpuszunk kifejezései között fennálló szemantikai távolságot, és ahogy a lenti ábrákon láthatjuk, azt, hogy az egyes kifejezések milyen relációban állnak egymással  a szemantikailag hasonló tartalmú kifejezések egymáshoz közel, míg a távolabbi jelentéstartalmú kifejezések egymástól távolabb foglalnak helyet. A klasszikus példa, amivel jól lehet szemléltetni a szóvektorok közötti összefüggést: king - man + woman = queen 8.2 Word2Vec és GloVe A társadalomtudományokban szóbeágyazásra a két legnépszerbb algoritmus  a Word2Vec és a GloVe  a kontextuális szövegeloszláson (distributional similarity based representations) alapul, vagyis abból a feltevésbl indul ki, hogy a hasonló kifejezések hasonló kontextusban fordulnak el, emellett mindkett sekély neurális hálón (2 rejtett réteg) alapuló modell.37 A Word2Vec-nek két verziója van: Continuous Bag-of-words (CBOW) és SkipGram (SG). Elbbi a kontextuális szavakból jelzi elre (predicting) a kontextushoz legszorosabban kapcsolódó kifejezést, míg utóbbi adott kifejezésbl jelzi elre a kontextust Mikolov et al. (2013). A GloVe (Global Vectors for Word Representation) a Word2Vec-hez hasonlóan neurális hálón alapuló, szóvektorok elállítását célzó modell, a Word2Vec-kel szemben azonban nem a meghatározott kontextus-ablakban (context window) megjelen kifejezések közti kapcsolatokat tárja fel, hanem a szöveg globális jellemzit igyekszik megragadni az egész szöveget jellemz együttes elfordulási gyakoriságok (co-occurrance) meghatározásával Pennington, Socher, and Manning (2014). Míg a Word2Vec modell prediktív jelleg, addig a GloVe egy statisztikai alapú (count-based) modell, melyek gyakorlati hasznosításukat tekintve nagyon hasonlóak. A szóvektor modellek között érdemes megemlíteni a fastText-et is, mely 157 nyelvre (köztük a magyarra is) kínál a szóbeágyazás módszerén alapuló, elre tanított szóvektorokat, melyet tovább lehet tanítani speciális szövegkorpuszokra, ezzel jelentsen lerövidítve a modell tanításához szükséges id- és kapacitásszükségletet (Mikolov et al. (2018)). Habár a GloVe és Word2Vec skip-gram módszerek hasonlóságát a szakirodalom adottnak veszi, a tényleges kép ennél árnyaltabb. A GloVe esetében a ritkán elforduló szavak kisebb súlyt kapnak a szóvektorok számításánál, míg a Word2Vec alulsúlyozza a nagy frekvenciájú szavakat. Ennek a következménye, hogy a Word2Vec esetében gyakori, hogy a szemantikailag legközelebbi szó az egy elütés, nem pedig valid találat. Ennek ellenére a két módszer (amennyiben a Word2Vec algoritmusnál a kisfrekvenciájú tokeneket kiszrjük) az emberi validálás során nagyon hasonló eredményeket hozott (Spirling and Rodriguez 2021). A fejezetben a gyakorlati példa során a GloVe algoritmust használjuk majd, mivel véleményünk szerint jobb és könnyebben követhet a dokumentációja az implementációt tartalmazó R csomagnak, mint a többi alternatívának. 8.2.1 GloVe használata magyar média korpuszon Az elemzéshez a text2vec csomagot használjuk, ami a GloVe implementációt tartalmazza. A lenti kód a csomag dokumentáción alapul és a Társadalomtudományi Kutatóközpont által a Hungarian Comparative Agendas Project (CAP) adatbázisában tárolt Magyar Nemzet korpuszt használja.38 library(text2vec) library(quanteda) library(readtext) library(readr) library(dplyr) library(tibble) library(stringr) library(ggplot2) library(HunMineR) A lenti kód blokk azt mutatja be, hogyan kell a betöltött korpuszt tokenizálni és mátrix formátumba alakítani. A korpusz a Magyar Nemzet 2004 és 2014 közötti címlapos cikkeit tartalmazza. Az eddigi elkészít lépéseket most is megtesszük: kitöröljük a központozást, a számokat, a magyar töltelékszavakat, illetve kisbetsítünk és eltávolítjuk a felesleges szóközöket és töréseket. mn &lt;- HunMineR::data_magyar_nemzet_large mn_clean &lt;- mn %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) Fontos különbség, hogy az eddigi munkafolyamatokkal ellentétben a GloVe algoritmus nem egy dokumentum-kifejezés mátrixon dolgozik, hanem egy kifejezések együttes elfordulását tartalmazó mátrixot (feature co-occurence matrix) kell készíteni inputként. Ezt a quanteda fcm() függvényével tudjuk elállítani, ami a tokenekbl készíti el a mátrixot. A tokenek sorrendiségét úgy tudjuk megrizni, hogy egy dfm objektumból csak a kifejezéseket tartjuk meg a featnames() függvény segítségével, majd a teljes token halmazból a tokens_select() függvénnyel kiválasztjuk ket. mn_corpus &lt;- corpus(mn_clean) mn_tokens &lt;- tokens(mn_corpus) %&gt;% tokens_remove(stopwords(language = &quot;hungarian&quot;)) features &lt;- dfm(mn_tokens) %&gt;% dfm_trim(min_termfreq = 5) %&gt;% featnames() mn_tokens &lt;- tokens_select(mn_tokens, features, padding = TRUE) Az fcm megalkotása során a célkifejezéstl való távolság függvényében súlyozzuk a tokeneket. mn_fcm &lt;- fcm(mn_tokens, context = &quot;window&quot;, count = &quot;weighted&quot;, weights = 1/(1:5), tri = TRUE) A tényleges szóbeágyazás a text2vec csomaggal történik. A GlobalVector egy új környezetet\" (environment) hoz létre. Itt adhatjuk meg az alapvet paramétereket. A rank a vektor dimenziót adja meg (a szakirodalomban a 300500 dimenzió a megszokott). A többi paraméterrel is lehet kísérletezni, hogy mennyire változtatja meg a kapott szóbeágyazásokat. A fit_transform pedig a tényleges becslést végzi. Itt az iterációk számát (a gépi tanulásos irodalomban epoch-nak is hívják a tanulási köröket) és a korai leállás (early stopping) kritériumát a convergence_tol megadásával állíthatjuk be. Minél több dimenziót szeretnénk és minél több iterációt, annál tovább fog tartani a szóbeágyazás futtatása. Az egyszerség és a gyorsaság miatt a lenti kód 10 körös tanulást ad meg, ami a relatíve kicsi Magyar Nemzet korpuszon ~3 perc alatt fut le.39 Természetesen minél nagyobb korpuszon, minél több iterációt futtatunk, annál pontosabb eredményt fogunk kapni. A text2vec csomag képes a számítások párhuzamosítására, így alapbeállításként a rendelkezésre álló összes CPU magot teljesen kihasználja a számításhoz. Ennek ellenére egy százezres, milliós korpusz esetén több óra is lehet a tanítás. glove &lt;- GlobalVectors$new(rank = 300, x_max = 10, learning_rate = 0.1) mn_main &lt;- glove$fit_transform(mn_fcm, n_iter = 10, convergence_tol = 0.01) A végleges szóvektorokat a becslés során elkészült két mátrix összegeként kapjuk. mn_context &lt;- glove$components mn_word_vectors &lt;- mn_main + t(mn_context) Az egyes szavakhoz legközelebb álló szavakat a koszinusz hasonlóság alapján kapjuk, a sim2() függvénnyel. A lenti példában l2 normalizálást alkalmazunk, majd a kapott hasonlósági vektort csökken sorrendbe rendezzük. Példaként a polgármester szónak a környezetét nézzük meg. Mivel a korpuszunk egy politikai napilap, ezért nem meglep, hogy a legközelebbi szavak a politikához kapcsolódnak. teszt &lt;- mn_word_vectors[&quot;polgármester&quot;, , drop = F] cos_sim_rom &lt;- sim2(x = mn_word_vectors, y = teszt, method = &quot;cosine&quot;, norm = &quot;l2&quot;) head(sort(cos_sim_rom[, 1], decreasing = TRUE), 5) #&gt; polgármester mszps szocialista fideszes elmondta #&gt; 1.0000000 0.5059529 0.4339177 0.4204766 0.4024232 A lenti show_vector() függvényt definiálva a kapott eredmény egy data frame lesz, és az n változtatásával a kapcsolódó szavak számát is könnyen változtathatjuk. show_vector &lt;- function(vectors, pattern, n = 5) { term &lt;- mn_word_vectors[pattern, , drop = F] cos_sim &lt;- sim2(x = vectors, y = term, method = &quot;cosine&quot;, norm = &quot;l2&quot;) cos_sim_head &lt;- head(sort(cos_sim[, 1], decreasing = TRUE), n) output &lt;- enframe(cos_sim_head, name = &quot;term&quot;, value = &quot;dist&quot;) return(output) } Példánkban láthatjuk, hogy a barack szó beágyazásának eredménye nem gyümölcsöt fog adni, hanem az Egyesült Államok elnökét és a hozzá kapcsolódó szavakat. show_vector(mn_word_vectors, &quot;barack&quot;, 10) #&gt; # A tibble: 10 x 2 #&gt; term dist #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 barack 1 #&gt; 2 obama 0.691 #&gt; 3 elnök 0.372 #&gt; 4 amerikai 0.349 #&gt; 5 demokrata 0.339 #&gt; 6 republikánus 0.294 #&gt; 7 részesülhessenek 0.256 #&gt; 8 egyesült 0.253 #&gt; 9 elnököt 0.251 #&gt; 10 bush 0.239 Ugyanez mködik magyar vezetkkel is. show_vector(mn_word_vectors, &quot;orbán&quot;, 10) #&gt; # A tibble: 10 x 2 #&gt; term dist #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 orbán 1 #&gt; 2 viktor 0.937 #&gt; 3 miniszterelnök 0.743 #&gt; 4 mondta 0.701 #&gt; 5 jelentette 0.673 #&gt; 6 kormányfo 0.667 #&gt; 7 fogalmazott 0.661 #&gt; 8 fidesz 0.656 #&gt; 9 hangsúlyozta 0.655 #&gt; 10 beszélt 0.624 A szakirodalomban klasszikus vektormveletes példákat is reprokuálni tudjuk a Magyar Nemzet korpuszon készített szóbeágyazásainkkal. A budapest - magyarország + német + németország eredményét úgy kapjuk meg, hogy az egyes szavakhoz tartozó vektorokat kivonjuk egymásból, illetve hozzáadjuk ket, ezután pedig a kapott mátrixon a quanteda csomag textstat_simil függvényével kiszámítjuk az új hasonlósági értékeket. budapest &lt;- mn_word_vectors[&quot;budapest&quot;, , drop = FALSE] - mn_word_vectors[&quot;magyarország&quot;, , drop = FALSE] + mn_word_vectors[&quot;német&quot;, , drop = FALSE] + + mn_word_vectors[&quot;németország&quot;, , drop = FALSE] cos_sim &lt;- textstat_simil(x = as.dfm(mn_word_vectors), y = as.dfm(budapest), method = &quot;cosine&quot;) head(sort(cos_sim[, 1], decreasing = TRUE), 5) #&gt; budapest német németország kancellár angéla #&gt; 0.6393528 0.6014288 0.5315928 0.4602480 0.4222456 A szavak egymástól való távolságát vizuálisan is tudjuk ábrázolni. Az egyik ezzel kapcsolatban felmerül probléma, hogy egy 2 dimenziós ábrán akarunk egy 3500 dimenziós mátrixot ábrázolni. Több lehetséges megoldás is van, mi ezek közül a lehet legegyszerbbet mutatjuk be.40 Els lépésben egy data frame-et készítünk a szóbeágyazás eredményeként kapott mátrixból, megtartva a szavakat az els oszlopban a tibble csomag rownames_to_column függvényével. Mivel csak 2 dimenziót tudunk ábrázolni egy tradícionális statikus ábrán, ezért a V1 és V2 oszlopokat tartjuk csak meg, amik az els és második dimenziót reprezentálják. mn_embedding_df &lt;- as.data.frame(mn_word_vectors[, c(1:2)]) %&gt;% rownames_to_column(var = &quot;words&quot;) Ezután pedig a ggplot függvényt felhasználva definiálunk egy új, embedding_plot nev, függvényt, ami az elkészült data frame alapján bármilyen kulcsszó kombinációt képes ábrázolni. embedding_plot &lt;- function(data, keywords) { data %&gt;% filter(words %in% keywords) %&gt;% ggplot(aes(V1, V2, label = words)) + labs( x = &quot;Els dimenzió&quot;, y = &quot;Második dimenzió&quot; ) + geom_text() + xlim(-1, 1) + ylim(-1, 1) } Példaként néhány településnevet megvizsgálva, azt látjuk, hogy a külföldi fvárosok közel helyezkednek el egymáshoz, míg a magyar települések kissé távolabb. Ennek az lehet az oka, hogy a külföldi fvárosok inkább a külpolitikai cikkekben szerepelnek, míg a magyarok sokkal több kontextusban elkerülhetnek. words_selected &lt;- c(&quot;moszkva&quot;, &quot;debrecen&quot;, &quot;budapest&quot;, &quot;washington&quot;) embedding_plot(data = mn_embedding_df, keywords = words_selected) Egy kiváló tanulmányban Spirling and Rodriguez (2021) (könyvünk írásakor még nem jelent meg) összehasonlítják a Word2Vec és GloVe módszereket, különböz paraméterekkel, adatbázisokkal. Azoknak, akiket komolyabban érdekelnek a szóbeágyazás gyakorlati alkalmazásának a részletei, mindenképp ajánljuk elolvasásra. A Magyar CAP Project által kezelt adatbázisok regisztrációt követen elérhetek az elábbi linken: https://cap.tk.hu/adatbazisok. A futtatásra használt PC konfiguráció: CPU: Intel Core i5-4460 (3.2GHz); RAM: 16GB Az egyik legelterjedtebb dimenzionalitás csökkent eljárás a szakirodalomban a fkomponens-analízis (principal component analysis), illetve szintén gyakran használt az irodalomban az úgynevezett t-SNE (t-distributed stochastic neighbor embedding). "],["scaling.html", "9 Szövegskálázás 9.1 Áttekintés 9.2 Wordfish 9.3 Wordscores", " 9 Szövegskálázás 9.1 Áttekintés A szövegskálázás célja a politikai szereplk elhelyezése az ideológiai térben. Ennek felügyelt típusa a wordscores, amely a szótári módszerekhez hasonlóan a szereplket szavaik alapján helyezi el a politikai térben oly módon, hogy az ún. referencia dokumentumok szövegét használja tanító halmazként. A wordscores kiindulópontja, hogy pozíció pontszámokat kell rendelni referencia szövegekhez. A modell számításba veszi a szövegek szavainak súlyozott gyakoriságát és a pozíciópontszám, valamint a szógyakoriság alapján becsüli meg a korpuszban lév többi dokumentum pozícióját (Laver, Benoit, and Garry 2003). A felügyelet nélküli wordfish módszer a skálázás során nem a referencia dokumentumokra támaszkodik, hanem olyan kifejezéseket keres a szövegben, amelyek megkülönböztetik egymástól a politikai spektrum különböz pontjain elhelyezked beszélket. Az IRT-n (item response theory) alapuló módszer azt feltételezi, hogy a politikusok egy kevés dimenziós politikai térben mozognak, amely tér leírható az i politikus \\(\\theta_1\\) paraméterével. Egy politikus (vagy párt) ezen a téren elfoglalt helyzete pedig befolyásolja a szavak szövegekben történ használatát. A módszer erssége, hogy kevés erforrás-befektetéssel megbízható becsléseket ad, ha a szövegek valóban az ideológiák mentén különböznek, tehát ha a szereplk ersen ideológiai tartalamú diskurzust folytatnak. Alkalmazásakor azonban tudnunk kell: a módszer nem képes kezelni, hogy a szövegek között nem csak ideológiai különbség lehet. Mivel a modell nem felügyelt, ezért nehéz garantálni, hogy valóban megbízhatóan azonosítja a szereplk elhelyezkedését a politikai térben, így az eredményeket mindenképpen körültekinten kell validálni (Slapin and Proksch 2008; Hjorth et al. 2015; Grimmer and Stewart 2013). library(readr) library(dplyr) library(stringr) library(ggplot2) library(ggrepel) library(quanteda) library(quanteda.textmodels) library(HunMineR) A skálázási algoritmusokat egy kis korpuszon mutatjuk be. A minta dokumentumok a 20142018-as parlamenti ciklusban az Országgylésben frakcióvezet politikusok egy-egy véletlenszeren kiválasztott napirend eltti felszólalásai. Ebben a ciklusban összesen 11 frakcióvezetje volt a két kormánypárti és öt ellenzéki frakciónak.41 A dokumentumokon elször elvégeztük a szokásos elkészítési lépéseket. parl_beszedek &lt;- HunMineR::data_parlspeakers_small beszedek_tiszta &lt;- parl_beszedek %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) A Wordfish és Wordscores algoritmus is ugyanazt a kiinduló corpus és dfm objektumot használja, amit a szokásos módon a quanteda csomag corpus() függvényével hozunk létre. A leíró statisztikai táblázatban látszik, hogy a beszédek hosszúsága nem egységes, a leghosszabb 10267, a legrövidebb pedig 1976 szavas. Az átlagos dokumentum hossz az 5136 szó. A korpusz szemléltet célú, alaposabb elemzéshez hosszabb és/vagy több dokumentummal érdemes dolgoznunk. beszedek_corpus &lt;- corpus(beszedek_tiszta) summary(beszedek_corpus) #&gt; Corpus consisting of 10 documents, showing 10 documents: #&gt; #&gt; Text Types Tokens Sentences id felszolalo #&gt; text1 442 819 1 20142018_024_0002_0002 Vona Gábor (Jobbik) #&gt; text2 354 607 1 20142018_055_0002_0002 Dr. Schiffer András (LMP) #&gt; text3 426 736 1 20142018_064_0002_0002 Dr. Szél Bernadett (LMP) #&gt; text4 314 538 1 20142018_115_0002_0002 Tóbiás József (MSZP) #&gt; text5 354 589 1 20142018_158_0002_0002 Schmuck Erzsébet (LMP) #&gt; text6 333 538 1 20142018_172_0002_0002 Dr. Tóth Bertalan (MSZP) #&gt; text7 344 559 1 20142018_206_0002_0002 Volner János (Jobbik) #&gt; text8 352 628 1 20142018_212_0002_0002 Kósa Lajos (Fidesz) #&gt; text9 317 492 1 20142018_236_0002_0002 Harrach Péter (KDNP) #&gt; text10 343 600 1 20142018_249_0002_0002 Dr. Gulyás Gergely (Fidesz) #&gt; part #&gt; Jobbik #&gt; LMP #&gt; LMP #&gt; MSZP #&gt; LMP #&gt; MSZP #&gt; Jobbik #&gt; Fidesz #&gt; KDNP #&gt; Fidesz A korpusz létrehozása után elkészítjük a dfm mátrixot, amelybl eltávolítjuk a magyar stopszvakat a quanteda beépített szótárának segítségével. beszedek_dfm &lt;- beszedek_corpus %&gt;% tokens() %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% dfm() 9.2 Wordfish A wordfish felügyelet nélküli skálázást a quanteda_textmodels csomagban implementált textmodel_wordfish() függvény fogja végezni. A megadott dir = c(1, 2) paraméterrel a két dokumentum relatív \\(\\theta\\) értékét tudjuk rögzíteni, mégpedig úgy hogy \\(\\theta_{dir1} &lt; \\theta_{dir2}\\). Alapbeállításként az algoritmus az els és az utolsó dokumentumot teszi be ide. A lenti példánál mi a pártpozíciók alapján a Jobbikos Vona Gábor és az LMP-s Schiffer András egy-egy beszédét használtuk. A summary() használható az illesztett modellel, és a dokumentumonkénti \\(\\theta\\) koefficienst tudjuk így megnézni. beszedek_wf &lt;- textmodel_wordfish(beszedek_dfm, dir = c(2, 1)) summary(beszedek_wf) #&gt; #&gt; Call: #&gt; textmodel_wordfish.dfm(x = beszedek_dfm, dir = c(2, 1)) #&gt; #&gt; Estimated Document Positions: #&gt; theta se #&gt; text1 1.79474 0.04219 #&gt; text2 0.08931 0.04001 #&gt; text3 1.00137 0.03908 #&gt; text4 -0.09988 0.04232 #&gt; text5 0.73596 0.04355 #&gt; text6 0.18572 0.04452 #&gt; text7 -0.72832 0.03590 #&gt; text8 -0.80587 0.03358 #&gt; text9 -0.52028 0.04005 #&gt; text10 -1.65273 0.03794 #&gt; #&gt; Estimated Feature Scores: #&gt; vona gábor jobbik tisztelt elnök úr országgyulés tegnapi napon helyen #&gt; beta 3.675 2.321 1.9710 0.2391 -0.11149 0.02755 1.2286 4.372 2.991 3.103 #&gt; psi -4.980 -2.734 -0.7531 0.4566 -0.05693 0.28721 -0.6705 -5.314 -3.009 -2.630 #&gt; tartottak idoközi önkormányzati választásokat két érdekelt recsken ózdon #&gt; beta 3.675 3.675 3.675 3.675 1.1894 3.675 4.372 4.774 #&gt; psi -4.980 -4.980 -4.980 -4.980 -0.9439 -4.980 -5.314 -5.545 #&gt; október nyertünk örömmel közlöm ország közvéleményével amúgy is tudnak #&gt; beta 3.405 3.675 3.675 3.675 1.7470 3.675 3.675 0.9128 1.433 #&gt; psi -3.230 -4.980 -4.980 -4.980 -0.3643 -4.980 -4.980 1.8345 -1.737 #&gt; mindkét jobbikos polgármester #&gt; beta 3.675 3.675 3.675 #&gt; psi -4.980 -4.980 -4.980 Amennyiben szeretnénk a szavak szintjén is megnézni a \\(\\beta\\) (a szavakhoz társított súly, ami a relatív fontosságát mutatja) és \\(\\psi\\) (a szó rögzített hatást (word fixed effects), ami az eltér szófrekvencia kezeléséért felels) koefficienseket, akkor a beszedek_wf objektumban tárolt értékeket egy data frame-be tudjuk bemásolni. A dokumentumok hosszát és a szófrekvenciát figyelembe véve, a negatív \\(\\beta\\) érték szavakat gyakrabban használják a negatív \\(\\theta\\) koefficienssel rendelkez politikusok. szavak_wf &lt;- data.frame( word = beszedek_wf$features, beta = beszedek_wf$beta, psi = beszedek_wf$psi ) szavak_wf %&gt;% arrange(beta) %&gt;% head(n = 15) #&gt; word beta psi #&gt; 1 czeglédy -5.900663 -6.222629 #&gt; 2 csaba -5.769959 -6.151399 #&gt; 3 human -5.438681 -5.975155 #&gt; 4 operator -5.438681 -5.975155 #&gt; 5 zrt -5.216835 -5.860931 #&gt; 6 fizette -4.927204 -5.717002 #&gt; 7 gyanú -4.927204 -5.717002 #&gt; 8 szocialista -4.927204 -5.717002 #&gt; 9 elkövetett -4.509192 -5.521276 #&gt; 10 tárgya -4.509192 -5.521276 #&gt; 11 céghálózat -4.509192 -5.521276 #&gt; 12 diákok -4.509192 -5.521276 #&gt; 13 májusi -4.509192 -5.521276 #&gt; 14 júniusi -4.509192 -5.521276 #&gt; 15 büntetoeljárás -4.509192 -5.521276 Ez a pozitív értékekre is igaz. szavak_wf %&gt;% arrange(desc(beta)) %&gt;% head(n = 15) #&gt; word beta psi #&gt; 1 nemzetközi 5.057078 -5.720709 #&gt; 2 önöknek 4.977502 -4.778607 #&gt; 3 ózdon 4.773523 -5.544626 #&gt; 4 kétharmados 4.773523 -5.544626 #&gt; 5 igenis 4.773523 -5.544626 #&gt; 6 választási 4.773523 -5.544626 #&gt; 7 geopolitikai 4.773523 -5.544626 #&gt; 8 ártatlanság 4.773523 -5.544626 #&gt; 9 vélelme 4.773523 -5.544626 #&gt; 10 tegnapi 4.372320 -5.314088 #&gt; 11 recsken 4.372320 -5.314088 #&gt; 12 lássuk 4.372320 -5.314088 #&gt; 13 tolünk 4.372320 -5.314088 #&gt; 14 janiczak 4.372320 -5.314088 #&gt; 15 szavazattal 4.372320 -5.314088 Az eredményeinket mind a szavak, mind a dokumentumok szintjén tudjuk vizualizálni. Elsként a klasszikus Eiffel-torony ábrát reprodukáljuk, ami a szavak gyakoriságának és a skálára gyakorolt befolyásának az illusztrálására szolgál. Ehhez a már elkészült szavak_wf data framet-et és a ggplot2 csomagot fogjuk használni. Mivel a korpuszunk nagyon kicsi, ezért csak 2410 kifejezést fogunk ábrázolni. Ennek ellenére a lényeg kirajzolódik a lenti ábrán is.42 Kihasználhatjuk, hogy a ggplot ábra definiálása közben a felhasznált bemeneti data frame-et különböz szempontok alapján lehet szrni. Így ábrázolni tudjuk a gyakran használt, ám semleges szavakat (magas \\(\\psi\\), alacsony \\(\\beta\\)), illetve a ritkább, de meghatározóbb szavakat (magas \\(\\beta\\), alacsony \\(\\psi\\)). ggplot(szavak_wf, aes(x = beta, y = psi)) + geom_point(color = &quot;grey&quot;) + geom_text_repel( data = filter(szavak_wf, beta &gt; 4.5 | beta &lt; -5 | psi &gt; 0), aes(beta, psi, label = word), alpha = 0.7 ) + labs( x = expression(beta), y = expression(psi) ) Figure 9.1: A Wordfish Eiffel-torony A dokumentumok szintjén is érdemes megvizsgálni az eredményeket. Ehhez a dokumentum szint paramétereket fogjuk egy data frame-be gyjteni: a \\(\\theta\\) ideológiai pozíciót, illetve a beszél nevét. A vizualizáció kedvéért a párttagságot is hozzáadjuk. A data frame összerakása után az alsó és a fels határát is kiszámoljuk a konfidencia intervallumnak és azt is ábrázoljuk. dokumentumok_wf &lt;- data.frame( speaker = beszedek_wf$x@docvars$felszolalo, part = beszedek_wf$x@docvars$part, theta = beszedek_wf$theta, theta_se = beszedek_wf$se.theta ) %&gt;% mutate( lower = theta - 1.96 * theta_se, upper = theta + 1.96 * theta_se ) ggplot(dokumentumok_wf, aes(theta, reorder(speaker, theta))) + geom_point() + geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0) + labs( y = NULL, x = expression(theta) ) Figure 9.2: A beszédek egymáshoz viszonyított pozíciója A párt metaadattal összehasonlíthatjuk az egy párthoz tartozó frakcióvezetk értékeit a facet_wrap() használatával. Figyeljünk arra, hogy az y tengelyen szabadon változhasson az egyes rész ábrák között, a scales = \"free\" opcióval. ggplot(dokumentumok_wf, aes(theta, reorder(speaker, theta))) + geom_point() + geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0) + labs( y = NULL, x = expression(theta) ) + facet_wrap(~part, ncol = 1, scales = &quot;free_y&quot;) Figure 9.3: Párton belüli pozíciók 9.3 Wordscores A modell illesztést a wordfish-hez hasonlóan a quanteda.textmodels csomagban található textmodel_wordscores() függvény végzi. A kiinduló dfm ugyanaz, mint amit a fejezet elején elkészítettünk, a beszedek_dfm. A referencia pontokat dokumentumváltozóként hozzáadjuk a dfm-hez (a refrencia_pont oszlopot, ami NA értéket kap alapértelmezetten). A kiválasztott referencia dokumentumoknál pedig egyenként hozzáadjuk az értékeket. Erre több megoldás is van, az egyszerbb út, hogy az egyik és a másik végletet a -1; 1 intervallummal jelöljük. Ennek a lehetséges alternatívája, hogy egy küls, már validált forrást használunk. Pártok esetén ilyen lehet a Chapel Hill szakérti kérdívének a pontszámai, a Manifesto projekt által kódolt jobb-bal (rile) dimenzió. A lenti példánál mi maradunk az egyszerbb bináris kódolásnál. A wordfish eredményt alapul véve a két referencia pont Gulyás Gergely és Szél Bernadett beszédei lesznek.43 Ezek a 3. és a 10. dokumentumok. docvars(beszedek_dfm, &quot;referencia_pont&quot;) &lt;- NA docvars(beszedek_dfm, &quot;referencia_pont&quot;)[3] &lt;- -1 docvars(beszedek_dfm, &quot;referencia_pont&quot;)[10] &lt;- 1 docvars(beszedek_dfm) #&gt; id felszolalo part referencia_pont #&gt; 1 20142018_024_0002_0002 Vona Gábor (Jobbik) Jobbik NA #&gt; 2 20142018_055_0002_0002 Dr. Schiffer András (LMP) LMP NA #&gt; 3 20142018_064_0002_0002 Dr. Szél Bernadett (LMP) LMP -1 #&gt; 4 20142018_115_0002_0002 Tóbiás József (MSZP) MSZP NA #&gt; 5 20142018_158_0002_0002 Schmuck Erzsébet (LMP) LMP NA #&gt; 6 20142018_172_0002_0002 Dr. Tóth Bertalan (MSZP) MSZP NA #&gt; 7 20142018_206_0002_0002 Volner János (Jobbik) Jobbik NA #&gt; 8 20142018_212_0002_0002 Kósa Lajos (Fidesz) Fidesz NA #&gt; 9 20142018_236_0002_0002 Harrach Péter (KDNP) KDNP NA #&gt; 10 20142018_249_0002_0002 Dr. Gulyás Gergely (Fidesz) Fidesz 1 A lenti wordscore modell specifikáció követi a Laver, Benoit, and Garry (2003) tanulmányban leírtakat. beszedek_ws &lt;- textmodel_wordscores( x = beszedek_dfm, y = docvars(beszedek_dfm, &quot;referencia_pont&quot;), scale = &quot;linear&quot;, smooth = 0 ) summary(beszedek_ws, 10) #&gt; #&gt; Call: #&gt; textmodel_wordscores.dfm(x = beszedek_dfm, y = docvars(beszedek_dfm, #&gt; &quot;referencia_pont&quot;), scale = &quot;linear&quot;, smooth = 0) #&gt; #&gt; Reference Document Statistics: #&gt; score total min max mean median #&gt; text1 NA 486 0 18 0.2017 0 #&gt; text2 NA 395 0 12 0.1639 0 #&gt; text3 -1 439 0 12 0.1822 0 #&gt; text4 NA 330 0 7 0.1369 0 #&gt; text5 NA 360 0 8 0.1494 0 #&gt; text6 NA 328 0 5 0.1361 0 #&gt; text7 NA 349 0 5 0.1448 0 #&gt; text8 NA 387 0 10 0.1606 0 #&gt; text9 NA 307 0 13 0.1274 0 #&gt; text10 1 383 0 8 0.1589 0 #&gt; #&gt; Wordscores: #&gt; (showing first 10 elements) #&gt; tisztelt elnök úr országgyulés ország is #&gt; -0.07547 0.39255 0.06813 0.06813 -1.00000 -0.19859 #&gt; sot nemhogy tette fidesz #&gt; -1.00000 -1.00000 -1.00000 1.00000 Az illesztett wordscores modellünkkel ezek után már meg tudjuk becsülni a korpuszban lév többi dokumentum pozícióját. Ehhez a predict() függvény megoldását használjuk. A kiegészít opciókkal a konfidencia intervallum alsó és fels határát is meg tudjuk becsülni, ami jól jön akkor, ha szeretnénk ábrázolni az eredményt. beszedek_ws_pred &lt;- predict( beszedek_ws, newdata = beszedek_dfm, interval = &quot;confidence&quot;) beszedek_ws_pred &lt;- as.data.frame(beszedek_ws_pred$fit) beszedek_ws_pred #&gt; fit lwr upr #&gt; text1 -0.489860579 -0.62138707 -0.35833409 #&gt; text2 -0.234609623 -0.39658117 -0.07263807 #&gt; text3 -0.909048451 -0.93507086 -0.88302605 #&gt; text4 -0.296528588 -0.47539855 -0.11765863 #&gt; text5 -0.259074418 -0.44948427 -0.06866457 #&gt; text6 0.006320468 -0.23056645 0.24320738 #&gt; text7 0.165042014 -0.06144022 0.39152425 #&gt; text8 -0.077739857 -0.27645536 0.12097565 #&gt; text9 -0.123985348 -0.31176579 0.06379509 #&gt; text10 0.909048451 0.87934394 0.93875296 A kapott modellünket a wordfish-hez hasonlóan tudjuk ábrázolni, miután a beszedek_ws_pred objektumból adattáblát csinálunk és a ggplot2-vel elkészítjük a vizualizációt. A dokumentumok_ws két részbl áll össze. Elször a wordscores modell objektumunkból a frakcióvezetk neveit és pártjaikat emeljük ki (kicsit körülményes a dolog, mert egy komplexebb objektumban tárolja ket a quanteda, de az str() függvény tud segíteni ilyen esetekben). A dokumentumok becsült pontszámait pedig a beszedek_ws_pred objektumból készített data frame hozzácsatolásával adjuk hozzá a már elkészült data frame-hez. Ehhez a dplyr csomag bind_cols függvényét használjuk. Fontos, hogy itt teljesen biztosnak kell lennünk abban, hogy a sorok a két data frame esetében ugyanarra a dokumentumra vonatkoznak. dokumentumok_ws &lt;- data.frame( speaker = beszedek_ws$x@docvars$felszolalo, part = beszedek_ws$x@docvars$part ) dokumentumok_ws &lt;- bind_cols(dokumentumok_ws, beszedek_ws_pred) dokumentumok_ws #&gt; speaker part fit lwr upr #&gt; text1 Vona Gábor (Jobbik) Jobbik -0.489860579 -0.62138707 -0.35833409 #&gt; text2 Dr. Schiffer András (LMP) LMP -0.234609623 -0.39658117 -0.07263807 #&gt; text3 Dr. Szél Bernadett (LMP) LMP -0.909048451 -0.93507086 -0.88302605 #&gt; text4 Tóbiás József (MSZP) MSZP -0.296528588 -0.47539855 -0.11765863 #&gt; text5 Schmuck Erzsébet (LMP) LMP -0.259074418 -0.44948427 -0.06866457 #&gt; text6 Dr. Tóth Bertalan (MSZP) MSZP 0.006320468 -0.23056645 0.24320738 #&gt; text7 Volner János (Jobbik) Jobbik 0.165042014 -0.06144022 0.39152425 #&gt; text8 Kósa Lajos (Fidesz) Fidesz -0.077739857 -0.27645536 0.12097565 #&gt; text9 Harrach Péter (KDNP) KDNP -0.123985348 -0.31176579 0.06379509 #&gt; text10 Dr. Gulyás Gergely (Fidesz) Fidesz 0.909048451 0.87934394 0.93875296 A lenti példánál a párton belüli bontást illusztráljuk a facet_wrap() segítségével. ggplot(dokumentumok_ws, aes(fit, reorder(speaker, fit))) + geom_point() + geom_errorbarh(aes(xmin = lwr, xmax = upr), height = 0) + labs( y = NULL, x = &quot;Wordscore&quot; ) + facet_wrap(~part, ncol = 1, scales = &quot;free_y&quot;) Figure 9.4: A párton belüli Wordscore alapú skála a mintába nem került be Rogán Antal, akinek csak egy darab napirend eltti felszólalása volt. A quanteda.textplots csomag több megoldást is kínál az ábrák elkészítésére. Mivel ezek a megoldások kifejezetten a quanteda elemzések ábrázolására készültek, ezért rövid egysoros függvényekkel tudunk gyorsan ábrákat készíteni. A hátrányuk, hogy kevésbé tudjuk személyre szabni az ábráinkat, mint a ggplot2 példák esetében. A quanteda.textplots megoldásokat ezen a linken demonstrálják a csomag készíti: https://quanteda.io/articles/pkgdown/examples/plotting.html. Azért nem Vona Gábor beszédét választottuk, mert az gyaníthatóan egy kiugró érték, ami nem reprezentálja megfelelen a sokaságot. "],["similarity.html", "10 Szövegösszehasonlítás 10.1 A szövegösszehasonlítás különböz megközelítései 10.2 Lexikális hasonlóság 10.3 Szemantikai hasonlóság 10.4 Hasonlóság-számítás 10.5 Szövegtisztítás 10.6 A Jaccard hasonlóság számítás 10.7 A Koszinusz hasonlóság számítás 10.8 Az eredmények vizualizációja", " 10 Szövegösszehasonlítás 10.1 A szövegösszehasonlítás különböz megközelítései A gépi szövegösszehasonlítás a mindennapi életünk számos területén megjelen szövegbányászati technika, bár az emberek többség nincs ennek tudatában. Ezen a módszeren alapulnak a böngészk keres mechanizmusai, vagy a kérdés-felelet (Q&amp;A) fórumok algoritmusai, melyek ellenrzik, hogy szerepel-e már a feltenni kívánt kérdés a fórumon (Sieg 2018). Alkalmazzák továbbá a szövegösszehasonlítást a gépi szövegfordításban és az automatikus kérdésmegválaszolási feladatok esetén is (Wang and Dong 2020), de akár automatizált esszé értékelésre vagy plágiumellenrzésre is hasznosítható az eljárás (Bar, Zesch, and Gurevych 2011). A szövegösszehasonlítás hétköznapi életben elforduló rejtett alkalmazásain túl a társadalomtudományok mveli is számos esetben hasznosítják az eljárást. A politikatudomány területén többek között használhatjuk arra, hogy eldöntsük, mennyire különböznek egymástól a benyújtott törvényjavaslatok és az elfogadott törvények szövegei, ezzel fontos információhoz jutva arról, hogy milyen szerepe van a parlamenti vitának a végleges törvények kialakításában. Egy másik példa a szakpolitikai prioritásokban és alapelvekben végbemen változások elemzése, melyet például szakpolitikai javaslatok vagy ilyen témájú viták leiratainak elemzésével is megtehetünk. A könyv korábbi fejezeteiben bemutatott eljárások között sok olyat találunk, melyek alkalmasak arra, hogy a szövegek hasonlóságából valamilyen információt nyerjünk. Ugyanakkor vannak módszerek, melyek segítségével számszersíthetjük a szövegek közötti különbségeket. Ez a fejezet ezekrl nyújt rövid áttekintést. Mindenekeltt azonban azt kell tisztáznunk, hogy miként értelmezzük a hasonlóságot. A hasonlóságelemzéseket jellemzen két nagy kategóriába szoktuk sorolni a mérni kívánt hasonlóság típusa szerint. Ez alapján beszélhetünk lexikális (formai) és szemantikai hasonlóságról. 10.2 Lexikális hasonlóság A lexikális hasonlóság a gépi szövegfeldolgozás egy egyszerbb megközelítése, amikor nem várjuk el az elemzésünktl, hogy értse a szöveget, csupán a formai hasonlóságot figyeljük. A megközelítés elnye, hogy számítási szempontból jelentsen egyszerbb, mint a szemantikai hasonlóságra irányuló elemzések, hátránya azonban, hogy az egyszerség könnyen tévútra vihet szofisztikáltabb elemzések esetén, így például a lexikális hasonlóság szempontjából az alábbi két példamondat azonosnak tekinthet, hiszen formailag (kifejezések szintjén) megegyeznek. A boszorkány megsüti Jancsit és Juliskát. Jancsi és Juliska megsüti a boszorkányt. Két dokumentum közötti lexikális hasonlóságot a szöveg számos szintjén mérhetjük: karakterláncok (stringek), szóalakok (tokenek), n-grammok (n egységbl álló karakterláncok), szózsákok (bag of words) között, de akár a dokumentum nagyobb egységei, így szövegrészletek és dokumentumok között is. Bevett megközelítés továbbá a szókészlet összehasonlítása, melyet lexikális és szemantikai hasonlóság feltárására egyaránt használhatunk. A hasonlóság számítására számos metrika létezik. Ezek jelents része valamilyen távolságszámításon alapul, mint például a koszinusz vagy a manhattan távolságon alapuló szöveghasonlóság. A koszinusz távolság a két szövegvektor (azaz szöveg vektorizált formája) által bezárt szögben határozza meg a dokumentumok távolságát (Wang and Dong 2020), míg a manhattan távolság a horizontális és a vertikális távolságok összegeként számítja azt (Ladd 2020). Széles körben alkalmazott dokumentumhasonlósági metrika továbbá a Jaccard hasonlóság, melynek számítása egy egyszer eljáráson alapul: a két dokumentumban egyez szavak számát elosztja a két dokumentumban szerepl szavak számának uniójával (vagyis az két dokumentum szavai számának összegével, melybl kivonja az egyez szavak számának összegét). A Jaccard hasonlóság tehát azt képes megmutatni, hogy a két dokumentum teljes szószámához képest mekkora az azonos kifejezések aránya (Wang and Dong 2020, 6.). \\[ Jaccard(doc_{1}, doc_{2}) = \\frac{doc_{1}\\,\\cap \\, doc_{2}}{doc1 \\, \\cup \\, doc2} = \\frac{doc_{1} \\, \\cap \\, doc_{2}}{doc_{1} + doc_{2} - doc_{1} \\, \\cap \\, doc_{2} } \\] 10.3 Szemantikai hasonlóság A szemantikai hasonlóság a lexikai hasonlósággal szemben egy komplexebb számítás, melynek során az algoritmus a szavak tartalmát is képes elemezni. Így például formai szempontból hiába nem azonos az alábbi két példamondat, a szemantikai hasonlóságvizsgálatnak észlelnie kell a tartalmi azonosságot. A diákok jegyzetelnek, amíg a professzor eladást tart. A nebulók írnak, amikor az oktató beszél. A jelentésbeli hasonlóság kimutatására számos megközelítés létezik. Többek között alkalmazható a témamodellezés (topik modellezés), melyet a Felügyelet nélküli tanulás fejezetben tárgyaltunk bvebben, ezen belül pedig az LDA Látens Dirillecht Allokáció (Latent Dirillecht Allocation), valamint az LSA látens érzelem elemzés (Latent Sentiment Analysis) is nagyszer lehetséget kínál arra, hogy az egyes dokumentumainkat tartalmi hasonlóságok alapján csoportosítsuk. Az LSA-nél és az LDA-nél azonban egy fokkal komplexebb megközelítés a szóbeágyazás, melyet a Szóbeágyazások cím fejezetben mutattunk be. Ez a módszertan a témamodellezéshez képest a szöveg mélyebb szemantikai tartalmait is képes feltárni, hiszen a beágyazásnak köszönheten képes formailag különböz, de jelentésükben azonos kifejezések azonosságát megmutatni. A jelentésbeli hasonlóság megállapítható a beágyazás során létrehozott vektorreprezentációkból (emlékezzünk: a hasonló vektorreprezentáció hasonló szemantikai tartalomra utal). Kimutathatjuk a szemantikai közelséget például a király  férfi  lovag kifejezések között, de olyan mesterségesen létrehozott jelentésbeli azonosságokat is feltárhatunk, mint az irányítószámok és az általuk jelölt városnevek kapcsolata. Abban az esetben, ha a szóbeágyazást kimondottan a szöveghasonlóság megállapítására szeretnénk használni, a WMD (Word Movers Distance) metrikát érdemes használni, mely a vektortérben elhelyezked szóvektorok közötti távolság által számszersíti a szövegek hasonlóságát (Kusner et al. 2015). 10.4 Hasonlóság-számítás 10.4.1 Adatbázis importálás és elkészítés A fejezet második felében a lexikai hasonlóság vizsgálatára, ezen belül a Jaccard hasonlóság és a Koszinusz hasonlóság számítására mutatunk be egy-egy példát a törvényjavaslatok és az elfogadott törvények szövegeinek összehasonlításával. Az alábbiakban bemutatott elemzés a Viscosity Revisited: The Power of Legislatures in New and Old Democracies  A Comparative Text Reuse Analysis cím, megjelenés eltt álló tanulmányból meríti elemzési fkuszát. Az eredeti cikk által megvalósított elemzést a svájci korpusz elemzése nélkül, a magyar korpusz egy részhalmazán replikáljuk az alábbiakban. A kutatási kérdés arra irányul, hogy mennyiben változik meg a törvényjavaslatok szövege a parlamenti vita folyamán, amíg a javaslat elfogadásra kerül. Az elemzés során a különböz kormányzati ciklusok közötti eltérésekre világítunk rá. Az elemzés megkezdése eltt a már ismert módon betöltjük a szükséges csomagokat: readr, stringr, dplyr, quanteda, readtext, ggplot2. library(readr) library(stringr) library(dplyr) library(tidyr) library(quanteda) library(readtext) library(ggplot2) library(nandb) library(HunMineR) Ezt követen betöltjük azokat az adatbázisokat, amelyeken a szövegösszehasonlítást fogjuk végezni: az elfogadott törvények szövegét tartalmazó korpuszt, a törvényjavaslatok szövegét tartalmazó korpuszt, valamint az ezek összekapcsolását segít adatbázist, melyben az összetartozó törvényjavaslatok és törvények azonosítóját (id-ját) tároltuk el. Ahogy behívjuk a három CSV-t, érdemes rögtön lekérni az oszlopneveket colnames() és a táblázat dimenzióit dim(), hogy lássuk, milyen adatok állnak a rendelkezésünkre, és mekkora táblákkal fogunk dolgozni. A dim() függvény els értéke a sorok száma, a második pedig az oszlopok száma lesz az adott táblázatban. torvenyek &lt;- HunMineR::data_lawtext_sample colnames(torvenyek) #&gt; [1] &quot;tv_id&quot; &quot;torveny_szoveg&quot; &quot;korm_ciklus&quot; &quot;ev&quot; &quot;korm_ell&quot; dim(torvenyek) #&gt; [1] 600 5 tv_javaslatok &lt;- HunMineR::data_lawprop_sample colnames(tv_javaslatok) #&gt; [1] &quot;tvjav_id&quot; &quot;tvjav_szoveg&quot; dim(tv_javaslatok) #&gt; [1] 600 2 parok &lt;- HunMineR::data_lawsample_match colnames(parok) #&gt; [1] &quot;tv_id&quot; &quot;tvjav_id&quot; dim(parok) #&gt; [1] 600 2 A beimportált adatbázisok megfigyeléseinek száma egységesen 600. Ez a több mint háromezer megfigyelést tartalmazó eredeti korpusz egy részhalmaza, mely gyorsabb és egyszerbb elemzést tesz lehetvé. Az oszlopnevek lekérésével láthatjuk, hogy a törvény korpuszban van néhány metaadat, amelyet az elemzés során felhasználhatunk: ezek a kormányzati ciklusra, törvény elfogadásának évére, valamint a benyújtó kormányzati vagy ellenzéki pártállására vonatkoznak. Ezenkívül rendelkezésre állnak a törvényeket és a törvényjavaslatokat azonosító kódok (tv_id és tvjav_id), melyek segítségével majd tudjuk párosítani az összetartozó törvényjavaslatok és törvények szövegeit. Ezt a left_join() függvénnyel tesszünk meg. Elsként a törvényeket tartalmazó adatbázishoz kapcsoljuk hozzá a törvénytörvényjavaslat párokat tartalmazó adatbázist a törvények azonosítója (tv_id) alapján. A colnames() függvény használatával ellenrizhetjük, hogy sikeres volt-e a mvelet, és az új táblában szerepelnek-e a kívánt oszlopok. tv_tvjavid_osszekapcs &lt;- left_join(torvenyek, parok, by = &quot;tv_id&quot;) colnames(tv_tvjavid_osszekapcs) #&gt; [1] &quot;tv_id&quot; &quot;torveny_szoveg&quot; &quot;korm_ciklus&quot; &quot;ev&quot; &quot;korm_ell&quot; #&gt; [6] &quot;tvjav_id&quot; dim(tv_tvjavid_osszekapcs) #&gt; [1] 600 6 Második lépésben a törvényjavaslatokat tartalmazó adatbázist rendeljük hozzá az elzekben már összekapcsolt két adatbázishoz. tv_tvjav_minta &lt;- left_join(tv_tvjavid_osszekapcs, tv_javaslatok, by = &quot;tvjav_id&quot;) colnames(tv_tvjav_minta) #&gt; [1] &quot;tv_id&quot; &quot;torveny_szoveg&quot; &quot;korm_ciklus&quot; &quot;ev&quot; &quot;korm_ell&quot; #&gt; [6] &quot;tvjav_id&quot; &quot;tvjav_szoveg&quot; dim(tv_tvjav_minta) #&gt; [1] 600 7 Ha jól végeztük a dolgunkat az adatbázisok összekapcsolása során, az eljárás végére 7 oszlopunk és 600 sorunk van, vagyis az újonnan létrehozott adatbázisba bekerült az összes változó (oszlop). A korpuszaink egy adattáblában való kezelése azért hasznos, mert így nem kell párhuzamosan elvégezni az azonos mveleteket a két korpusz, a törvények és a törvényjavaslatok tisztításához, hanem párhuzamosan tudunk dolgozni a kettvel. Kicsit közelebbrl megvizsgálva az adatbázist, azt láthatjuk, hogy minden adatbázisunkban szerepl kormányzati ciklusra 100 megfigyelés áll rendelkezésünkre: tv_tvjav_minta %&gt;% count(korm_ciklus). tv_tvjav_minta %&gt;% count(korm_ciklus) #&gt; # A tibble: 6 x 2 #&gt; korm_ciklus n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1994-1998 100 #&gt; 2 1998-2002 100 #&gt; 3 2002-2006 100 #&gt; 4 2006-2010 100 #&gt; 5 2010-2014 100 #&gt; 6 2014-2018 100 Hasonlóan ellenrizhetjük az egyes évekre es megfigyelések számát is. tv_tvjav_minta %&gt;% count(ev) #&gt; # A tibble: 24 x 2 #&gt; ev n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1994 11 #&gt; 2 1995 31 #&gt; 3 1996 23 #&gt; 4 1997 31 #&gt; 5 1998 17 #&gt; 6 1999 20 #&gt; 7 2000 34 #&gt; 8 2001 30 #&gt; 9 2002 3 #&gt; 10 2004 35 #&gt; # ... with 14 more rows 10.5 Szövegtisztítás Mivel az elemzés során két különböz korpusszal dolgozunk  két oszlopnyi szöveggel , egyszerbb, ha a szövegtisztítás lépéseibl létrehozunk egy külön függvényt, amely magában foglalja a mvelet egyes lépéseit, és lehetvé teszi, hogy ne kelljen minden szövegtisztítási lépést külön definiálni az egyes korpuszok esetén. A függvény neve jelen esetben szovegtisztitas lesz, és a már ismert lépéseket foglalja magában: kontrol karakterek szóközzé alakítása, központozás és a számok eltávolítása. Kisbetsítés, ismétld stringek és a stringek eltt található szóközök eltávolítása. Továbbá a str_remove_all() függvénnyel eltávolítjuk azokat az írásjeleket, amelyek elfordulnak a szövegben, de számunkra nem hasznosak. A függvény definiálását az alábbi szintaxissal tehetjük meg. fuggveny &lt;- function(bemenet) { elvegzendo_lepesek return(kimenet) } A bemenet helyen azt jelöljük, hogy milyen objektumon fogjuk végrehajtani a mveleteket, a kimenetet pedig a return() függvénnyel definiáljuk, ez lesz a függvényünk úgynevezett visszatérési értéke, vagyis az elvégzend lépések szerint átalakított objektum. A szövegtisztító függvény bemeneti és kimeneti értéke is text lesz, mivel ebbe a változóba mentettük az elvégzend változtatásokat. szovegtisztitas &lt;- function(text) { text = str_replace(text, &quot;[:cntrl:]&quot;, &quot; &quot;) text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;) text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;) text = str_to_lower(text) text = str_trim(text) text = str_squish(text) text = str_remove_all(string = text, pattern = &quot;&quot;) text = str_remove_all(string = text, pattern = &quot;&quot;) text = str_remove_all(string = text, pattern = &quot;&quot;) text = str_remove_all(string = text, pattern = &quot;&quot;) text = str_remove_all(string = text, pattern = &quot;&quot;) text = str_remove_all(string = text, pattern = &quot;&quot;) text = str_remove_all(string = text, pattern = &quot;«&quot;) text = str_remove_all(string = text, pattern = &quot;»&quot;) text = str_remove_all(string = text, pattern = &quot;§&quot;) text = str_remove_all(string = text, pattern = &quot;°&quot;) text = str_remove_all(string = text, pattern = &quot;&lt;U+25A1&gt;&quot;) text = str_remove_all(string = text, pattern = &quot;@&quot;) return(text) } Miután létrehoztuk a szövegtisztításra alkalmas függvényünket, az adatbázis két oszlopára fogjuk alkalmazni: a törvények szövegét és a törvényjavaslatok szövegét tartalmazó oszlopra, amiben a mapply() függvény lesz a segítségünkre. A mapply() függvényen belül megadjuk az adatbázist, és ennek vonatkozó részeire való hivatkozást tv_tvjav_minta[ ,c(\"torveny_szoveg\",\"tvjav_szoveg\")]. Az alkalmazni kívánt függvényt a FUN argumanetumaként adhatjuk meg  értelemszeren ez esetünkben az elzekben létrehozott szovegtisztitas függvény lesz. Végezetül pedig a fügvényünk által megtisztított új oszlopokkal felülírjuk az elz adatbázisunk vonatkozó oszlopait, vagyis a torveny_szoveg és a tvjav_szoveg oszlopokat: tv_tvjav_minta[, c(\"torveny_szoveg\",\"tvjav_szoveg\")] &lt;- &gt;&gt;újonnan létrehozott oszlopok&lt;&lt;. Amennyiben számítunk rá, hogy még változhatnak a szövegeket tartalmazó oszlopok, akkor érdemes elre definiálni a szöveges oszlopok neveit, hogy késbb csak egy helyen kelljen változtatni a kódon. szovegek &lt;- c(&quot;torveny_szoveg&quot;,&quot;tvjav_szoveg&quot;) tv_tvjav_minta[, szovegek] &lt;- mapply(tv_tvjav_minta[, szovegek], FUN = szovegtisztitas) A szövegtisztítás következ lépése a stopszavak meghatározása és kiszrése a szövegbl. Itt a quanteda csomagban elérhet magyar nyelv stopszavakat, valamint a 7. fejezetben meghatározott speciális jogi stopszavak listáját használjuk. legal_stopwords &lt;- HunMineR::data_legal_stopwords A stopszavak beimportálását követen korpusszá alakítjuk a szövegeinket és tokenizáljuk azokat. Ezt már külön-külön végezzük el a törvények és a törvényjavaslatok szövegeire, azonos lépésekben haladva. A létrehozott objektumokat itt is ellenrizhetjük, például a summary(torvenyek_coprus) paranccsal, vagy a torvenyek_tokens[1:3] paranccsal, mely az els 3 dokumentum tokenjeit fogja megmutatni. torvenyek_corpus &lt;- corpus(tv_tvjav_minta$torveny_szoveg) tv_javaslatok_corpus &lt;- corpus(tv_tvjav_minta$tvjav_szoveg) torvenyek_tokens &lt;- tokens(torvenyek_corpus) %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% tokens_remove(legal_stopwords) %&gt;% tokens_wordstem(language = &quot;hun&quot;) tv_javaslatok_tokens &lt;- tokens(tv_javaslatok_corpus) %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% tokens_remove(legal_stopwords) %&gt;% tokens_wordstem(language = &quot;hun&quot;) A szövegek tokenizálásával és a stopszavak eltávolításával a szövegtisztítás végére értünk, így megkezdhetjük az elemzést. 10.6 A Jaccard hasonlóság számítás A Jaccard hasonlóság kiszámításához a quanteda textstat_simil() függvényét fogjuk alkalmazni. Mivel a textstat_simil() függvény dokumentum-kifejezés mátrixot vár bemenetként, elsként alakítsuk át ennek megfelelen a korpuszainkat. Az elz fejezetekhez hasonlóan itt is a TF-IDF súlyozást választottuk a mátrix létrehozásakor. torvenyek_dfm &lt;- dfm(torvenyek_tokens) %&gt;% dfm_tfidf() tv_javaslatok_dfm &lt;- dfm(tv_javaslatok_tokens) %&gt;% dfm_tfidf() Miután létrehoztuk a dokumentum-kifejezés mátrixokat, érdemes a leggyakoribb tokeneket ellenrizni a textstat_frequency() függvénnyel, hogy biztosak lehessünk abban, hogy a megfelel eredményt értük el a szövegtisztítás során. (Amennyiben nem vagyunk elégedettek, érdemes visszatérni a stopszavakhoz és újabb kifejezéseket hozzárendelni a stopszó listához.) tv_toptokens &lt;- textstat_frequency(torvenyek_dfm, n = 10, force = TRUE) tv_toptokens #&gt; feature frequency rank docfreq group #&gt; 1 an 6992.110 1 131 all #&gt; 2 szerzodo 4794.806 2 150 all #&gt; 3 felhalmozás 3617.640 3 28 all #&gt; 4 articl 3425.718 4 74 all #&gt; 5 kiadás 3327.803 5 224 all #&gt; 6 for 3304.808 6 100 all #&gt; 7 contracting 3295.353 7 39 all #&gt; 8 költségvetés 3129.739 8 206 all #&gt; 9 befektetés 3129.628 9 73 all #&gt; 10 szanálás 2629.448 10 13 all tvjav_toptokens &lt;- textstat_frequency(tv_javaslatok_dfm, n = 10, force = TRUE) tvjav_toptokens #&gt; feature frequency rank docfreq group #&gt; 1 an 6050.864 1 160 all #&gt; 2 szerzodo 4533.032 2 148 all #&gt; 3 articl 3489.540 3 75 all #&gt; 4 befektetés 3447.234 4 90 all #&gt; 5 buncselekmény 3274.250 5 96 all #&gt; 6 contracting 3266.005 6 40 all #&gt; 7 szanálás 3234.839 7 12 all #&gt; 8 bíróság 3225.629 8 301 all #&gt; 9 egyezmény 3045.205 9 231 all #&gt; 10 for 3045.119 10 109 all A létrehozott dokumentum-kifejezés mátrixokon elvégezhetjük a dokumentumhasonlóság vizsgálatot. (A Jaccard hasonlóság metrika, illetve a quanteda textstat_simil() függvénye alkalmazható egy korpuszra is. Egy korpuszra végezve az elemzést, a függvény a korpusz dokumentumai közötti hasonlóságot számítja ki, míg két korpuszra mindkét korpusz összes dokumentuma közötti hasonlóságot. Érdemes továbbá azt is megjegyezni, hogy a textstat_simil() method argumentumaként megadható számos más hasonlósági metrika is, melyekkel további érdekes számítások végezhetk. Bvebben a textstat_simil() függény használatáról és argumentumairól a quanteda hivatalos honlapján olvashatunk44. A textstat_simil() függvény kapcsán azt is érdemes figyelembe venni, hogy mivel nem csak a dokumentum párokra, hanem az összes bemenetként megadott dokumentumra külön kiszámítja a Jaccard indexet, a korpusz(ok) méretének növelésével a számítás kapacitás- és idigényessége exponenciálisan növekszik. Két 600 dokumentumból álló korpusz esetén kb. 45 perc a számítási id, míg 360 dokumentum esetén csupán 12 perc. jaccard_hasonlosag &lt;- textstat_simil(torvenyek_dfm, tv_javaslatok_dfm, method = &quot;jaccard&quot;) Mivel az eredménymátrixunk meglehetsen terjedelmes, nem érdemes az egészet egyben megtekinteni, egyszerbb az els néhány dokumentum közötti hasonlóságra szrni, melyet a szögletes zárójelben való indexeléssel tudunk megtenni. Az [1:5, 1:5] kifejezéssel specifikálhatjuk a sorokat és az oszlopkat az elstl az ötödikig. jaccard_hasonlosag[1:5, 1:5] #&gt; 5 x 5 Matrix of class &quot;dgeMatrix&quot; #&gt; text1 text2 text3 text4 text5 #&gt; text1 0.55407750 0.12565172 0.11437309 0.12500000 0.15256496 #&gt; text2 0.09830508 0.57181572 0.12781350 0.16494845 0.09432387 #&gt; text3 0.08797954 0.12447257 0.72876712 0.10216718 0.07635695 #&gt; text4 0.10887290 0.18933333 0.11823802 0.57200000 0.11339359 #&gt; text5 0.15041322 0.09396914 0.08807829 0.09842271 0.62733813 A mátrix fdiagonáljában jelennek meg az összetartozó törvényekre és törvényszövegekre vonatkozó értékek, minden más érték nem összetartozó törvény és törvényjavaslat szövegek hasonlóságára vonatkozik, vagyis a vizsgálatunk szempontjából irreleváns. Ahhoz, hogy kinyerjük a számunkra értékes adatokat, a Jaccard hasonlóság változót mátrixszá kell alakítani. (Ránézésre úgy tnhet, hogy már most is mátrix, de valójában ez egy speciális S4 típusú objektum, melyben a mátrixon kívül más típusú információk is el vannak mentve. Az egyes objektumok típusát mindig ellenrizhetjük a typeof() függvénnyel: typeof(jaccard_hasonlosag)). A mátrixszá alakításban az as.matrix() függvény lesz a segítségünkre, melynek egyúttal a diagonálját is kinyerhetjük a diag() függvénnyel. Ha jól dolgoztunk, a létrehozott jaccard_diag els öt eleme (jaccard_diag[1:5]) megegyezik a fent megjelenített 5x5-ös mátrix fdiagonáljában elhelyezked értékekkel, hossza pedig (length()) a mátrix bármelyik dimenziójával. jaccard_diag &lt;- diag(as.matrix(jaccard_hasonlosag)) jaccard_diag[1:5] #&gt; text1 text2 text3 text4 text5 #&gt; 0.5540775 0.5718157 0.7287671 0.5720000 0.6273381 Miután sikerült kinyerni az egyes törvénytörvényjavaslat párokra vonatkozó Jaccard értéket, érdemes a számításainkat hozzárendelni az eredeti adattáblánkhoz, hogy a meglév metaadatok fényében tudjuk kiértékelni az egyes dokumentumok közötti hasonlóságot. A hozzárendeléshez egyszeren definiálunk egy új oszlopot a meglév adatbázisban tv_tvjav_minta$jaccard_index, melyhez hozzárendeljük a diagonálból kinyert értékeket. tv_tvjav_minta$jaccard_index &lt;- jaccard_diag Érdemes megnézni a végeredményt, ellenrizni a Jaccard hasonlóság legmagasabb és legalacsonyabb értékeit. A top_n() függvény használatával ki tudjuk válogatni a legmagasabb és a legalacsonyabb értékeket. A top_n() függvény els argumentuma a változó lesz, ami alapján a legalacsonyabb és a legmagasabb értékeket keressük, a második argumentum pedig azt specifikálja, hogy a legmagasabb és a legalacsonyabb értékek közül hányat szeretnénk látni. Az n=5 értékkel a legmagasabb, az n=-5 értékkel a legalacsonyabb 5 Jaccard indexszel rendelkez sort tudjuk kiszrni. Emellett érdemes arra is odafigyelni, hogy a szövegeket tartalmazó oszlopainkat ne próbáljuk meg kiíratni, hiszen ez jelentsen lelassítja az RStudio mködését és csökkenti a kiírt eredmények áttekinthetségét. tv_tvjav_minta[, c(&quot;tv_id&quot;, &quot;korm_ciklus&quot;, &quot;tvjav_id&quot;, &quot;jaccard_index&quot;)] %&gt;% top_n(jaccard_index, n = 5) #&gt; # A tibble: 5 x 4 #&gt; tv_id korm_ciklus tvjav_id jaccard_index #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1994XCV 1994-1998 1994-1998_T0276 0.991 #&gt; 2 1995LXXXI 1994-1998 1994-1998_T1296 0.987 #&gt; 3 1999XXXV 1998-2002 1998-2002_T0807 0.985 #&gt; 4 2013XLII 2010-2014 2010-2014_T10219 0.981 #&gt; 5 2014VIII 2010-2014 2010-2014_T13631 0.980 tv_tvjav_minta[, c(&quot;tv_id&quot;, &quot;korm_ciklus&quot;, &quot;tvjav_id&quot;, &quot;jaccard_index&quot;)] %&gt;% top_n(jaccard_index, n=-5) #&gt; # A tibble: 5 x 4 #&gt; tv_id korm_ciklus tvjav_id jaccard_index #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1998I 1994-1998 1994-1998_T4328 0.0513 #&gt; 2 2005LII 2002-2006 2002-2006_T16291 0.0532 #&gt; 3 2007CLXVIII 2006-2010 2006-2010_T04678 0.0270 #&gt; 4 2010CLV 2010-2014 2010-2014_T01809 0.0273 #&gt; 5 2012CXCV 2010-2014 2010-2014_T09103 0.00895 Láthatjuk, hogy az öt leghasonlóbb törvénytörvényjavaslat pár esetén 0.98 felett van a Jaccard hasonlóság értéke, míg a leginkább különböz ötnél 0.03 alatt. 10.7 A Koszinusz hasonlóság számítás A Jaccard hasonlóság számítás után a koszinusz távolság számítása már nem jelent nagy kihívást, hiszen a textat_simil() függvénnyel ezt is kiszámíthatjuk, csupán a metrika paramétereként (method =) megadhatjuk a koszinuszt is. Ahogy az elbbiekben, itt is a dokumentum-kifejezés mátrixokat adjuk meg bemeneti értékként. koszinusz_hasonlosag &lt;- textstat_simil(torvenyek_dfm, tv_javaslatok_dfm, method = &quot;cosine&quot;) Érdmes itt is megtekinteni a mátrix els néhány sorába és oszlopába es értékeket. koszinusz_hasonlosag[0:5, 0:5] #&gt; 5 x 5 Matrix of class &quot;dgeMatrix&quot; #&gt; text1 text2 text3 text4 text5 #&gt; text1 0.61881872 0.015832786 0.016163982 0.014361259 0.055145416 #&gt; text2 0.01051437 0.928929092 0.006185991 0.045378617 0.007042933 #&gt; text3 0.01484353 0.007634593 0.984988124 0.005659447 0.001826568 #&gt; text4 0.01845665 0.050139950 0.005569874 0.960631668 0.018335364 #&gt; text5 0.07399905 0.006867184 0.002846785 0.017476290 0.753752459 Ebben az esetben is csak a mátrix diagonáljára van szükségünk, melyet a fent ismertetett módon nyerünk ki a mátrixból. koszinusz_diag &lt;- diag(as.matrix(koszinusz_hasonlosag)) koszinusz_diag[1:5] #&gt; text1 text2 text3 text4 text5 #&gt; 0.6188187 0.9289291 0.9849881 0.9606317 0.7537525 Végezetül pedig a diagonálból kinyert koszinusz értékeket is hozzárendeljük az adatbázisunkhoz. tv_tvjav_minta$koszinusz &lt;- koszinusz_diag colnames(tv_tvjav_minta) #&gt; [1] &quot;tv_id&quot; &quot;torveny_szoveg&quot; &quot;korm_ciklus&quot; &quot;ev&quot; &quot;korm_ell&quot; #&gt; [6] &quot;tvjav_id&quot; &quot;tvjav_szoveg&quot; &quot;jaccard_index&quot; &quot;koszinusz&quot; 10.8 Az eredmények vizualizációja A hasonlóság metrikák vizulizációjára gyakran alkalmazott megoldás a htérkép (heatmap), mellyel korrelációs mátrixokat ábrázolhatunk. Ebben az esetben a mátrix értékeit egy színskálán vizualizáljuk, ahol a világosabb színek a magasabb, a sötétebb színek az alacsonyabb értékeket jelölik. A Jaccard hasonlóság számítás és a Koszinusz hasonlóság számításakor kapott mátrixok esetén is ábrázolhatjuk az értékeinket ilyen módon. Mivel azonban mindkét mátrix 600x600-as, nem érdemes a teljes mátrixot megjeleníteni, mert ilyen nagy mennyiség adatnál már értelmezhetetlenné válik az ábra, így csak az utolsó 100 elemet, vagyis a 20142018-as idszakra vonatkozó értékeket jelenítjük meg. Ezt a kosziunsz_hasonlóság nev objektumunk feldarabolásával tesszük meg, szögletes zárójelben jelölve, hogy a mátrix mely sorait, és mely oszlopait szeretnénk használni: koszinusz_hasonlosag[501:600, 501:600. A koszinusz_hasonlosag objektumból egy data frame-t készítünk, ahol a dokumentumok közötti hasonlóság szerepel. A mátrix formátumból a tidyr csomag pivot_longer() függvényét használva tudjuk a kívánt formátumot elérni. koszinusz_df &lt;- as.matrix(koszinusz_hasonlosag[501:600, 501:600]) %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;docs1&quot;) %&gt;% pivot_longer(&quot;text501&quot;:&quot;text600&quot;, names_to = &quot;docs2&quot;, values_to = &quot;similarity&quot;) glimpse(koszinusz_df) #&gt; Rows: 10,000 #&gt; Columns: 3 #&gt; $ docs1 &lt;chr&gt; &quot;text501&quot;, &quot;text501&quot;, &quot;text501&quot;, &quot;text501&quot;, &quot;text501&quot;, &quot;text501&quot;, &quot;~ #&gt; $ docs2 &lt;chr&gt; &quot;text501&quot;, &quot;text502&quot;, &quot;text503&quot;, &quot;text504&quot;, &quot;text505&quot;, &quot;text506&quot;, &quot;~ #&gt; $ similarity &lt;dbl&gt; 0.956008320, 0.022967812, 0.042582749, 0.028090268, 0.009058144, 0.~ Ezt követen pedig a ggplot függvényt használva a geom_tile segítségével tudjuk elkészíteni a htérképet ami a hasonlósági mátrixot ábrázolja. ggplot(koszinusz_df, aes(docs1, docs2, fill = similarity)) + geom_tile() + scale_fill_gradient(high = &quot;#2c3e50&quot;, low = &quot;#bdc3c7&quot;) + labs( x = NULL, y = NULL, fill = &quot;Koszinusz hasonlóság&quot; ) + theme( axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank() ) Figure 10.1: A koszinusz hasonlósági hotérkép A Jaccard hasonlósági htérképet ugyanezzel a módszerrel tudjuk elkészíteni. # adatok átalakítása jaccard_df &lt;- as.matrix(jaccard_hasonlosag[501:600, 501:600]) %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;docs1&quot;) %&gt;% pivot_longer(&quot;text501&quot;:&quot;text600&quot;, names_to = &quot;docs2&quot;, values_to = &quot;similarity&quot;) # a ggplot ábra ggplot(jaccard_df, aes(docs1, docs2, fill = similarity)) + geom_tile() + scale_fill_gradient(high = &quot;#2c3e50&quot;, low = &quot;#bdc3c7&quot;) + labs( x = NULL, y = NULL, fill = &quot;Jaccard hasonlóság&quot; ) + theme( axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank() ) Figure 10.2: A Jaccard hasonlósági hotérkép A két plot összehasonlításánál láthatjuk, hogy a koszinusz hasonlóság általában magasabb hasonlósági értékeket mutat. A mátrix fátlójában kiugró világos csík azt mutatja meg, hogy a legnagyobb hasonlóság az összetartozó törvény-törvényjavaslat szövegek között mutatkozik meg, eddig tehát az adataink az elvárásaink szerinti képet mutatják. Amennyiben a világos csíkot nem látnánk, az egyértelm visszajelzés volna arról, hogy elrontottunk valamit a szövegelkészítés eddigi lépéseinek során, vagy a várakozásásaink voltak teljesen rosszak. A koszinusz és a Jaccard hasonlóság értékét ábrázolhatjuk közös pontdiagrammon a geom_jitter() segítségével. Ehhez elször egy kicsit átalakítjuk a data frame-t a tidyr csomag pivot_longer() függvényével, hogy a két hasonlósági érték egy oszlopban legyen. Ez azért szükséges, hogy a ggplot ábránkat könnyebben tudjuk létrehozni. tv_tvjav_tidy &lt;- tv_tvjav_minta %&gt;% pivot_longer(&quot;jaccard_index&quot;:&quot;koszinusz&quot;, values_to = &quot;hasonlosag&quot;, names_to = &quot;hasonlosag_tipus&quot;) glimpse(tv_tvjav_tidy) #&gt; Rows: 1,200 #&gt; Columns: 9 #&gt; $ tv_id &lt;chr&gt; &quot;1994LXXXII&quot;, &quot;1994LXXXII&quot;, &quot;1996CXXXI&quot;, &quot;1996CXXXI&quot;, &quot;1995CX~ #&gt; $ torveny_szoveg &lt;chr&gt; &quot;évi lxxxii törvény a magánszemélyek jövedelemadójáról szóló ~ #&gt; $ korm_ciklus &lt;chr&gt; &quot;1994-1998&quot;, &quot;1994-1998&quot;, &quot;1994-1998&quot;, &quot;1994-1998&quot;, &quot;1994-199~ #&gt; $ ev &lt;dbl&gt; 1994, 1994, 1996, 1996, 1995, 1995, 1995, 1995, 1996, 1996, 1~ #&gt; $ korm_ell &lt;dbl&gt; 900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 900, 9~ #&gt; $ tvjav_id &lt;chr&gt; &quot;1994-1998_T0233&quot;, &quot;1994-1998_T0233&quot;, &quot;1994-1998_T3167&quot;, &quot;199~ #&gt; $ tvjav_szoveg &lt;chr&gt; &quot;magyar köztársaság kormánya t számú törvényjavaslat a magáns~ #&gt; $ hasonlosag_tipus &lt;chr&gt; &quot;jaccard_index&quot;, &quot;koszinusz&quot;, &quot;jaccard_index&quot;, &quot;koszinusz&quot;, &quot;~ #&gt; $ hasonlosag &lt;dbl&gt; 0.5540775, 0.6188187, 0.5718157, 0.9289291, 0.7287671, 0.9849~ ggplot(tv_tvjav_tidy, aes(ev, hasonlosag))+ geom_jitter(aes(shape = hasonlosag_tipus, color = hasonlosag_tipus), width = 0.1, alpha = 0.45) + scale_x_continuous(breaks = seq(1994, 2018, by = 2)) + labs(y = &quot;Jaccard és koszinusz hasonlóság&quot;, shape = NULL, color = NULL, x = NULL) + theme( legend.position = &quot;bottom&quot;, panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank() ) Figure 10.3: Évenkénti hasonlóság a dokumentumok között A hasonlósági értékek évenkénti alakulásának megértése érdekében érdemes átlagot számolni a mutatókra. Ezt a group_by() és a summarize() függvények együttes alkalmazásával tehetjük meg. Megadjuk, hogy évenkénti bontásban szeretnénk a számításainkat elvégezni group_by(ev), és azt, hogy átlag számítást szeretnénk végezni mean(). evenkenti_atlag &lt;- tv_tvjav_tidy %&gt;% group_by(ev, hasonlosag_tipus) %&gt;% summarize(atl_hasonlosag = mean(hasonlosag)) head(evenkenti_atlag) #&gt; # A tibble: 6 x 3 #&gt; # Groups: ev [3] #&gt; ev hasonlosag_tipus atl_hasonlosag #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1994 jaccard_index 0.556 #&gt; 2 1994 koszinusz 0.757 #&gt; 3 1995 jaccard_index 0.499 #&gt; 4 1995 koszinusz 0.776 #&gt; 5 1996 jaccard_index 0.675 #&gt; 6 1996 koszinusz 0.880 Az évenkénti átlagot tartalmazó adattáblánkara ezt követen vonal diagramot illesztünk. ggplot(evenkenti_atlag, aes(ev, atl_hasonlosag)) + geom_line(aes(linetype = hasonlosag_tipus)) + labs(y = &quot;Átlagos Jaccard és koszinusz hasonlóság&quot;, linetype = NULL, x = NULL) + theme( legend.position = &quot;bottom&quot;, panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank() ) Figure 10.4: Évenkénti átlagos hasonlóság alakulása Ahhoz, hogy valamivel pontosabb képet kapjunk a Jaccard index értékének alakulásáról, érdemes vizualizálni a ggplot2 segítségével. Elsként évenkénti bontásban ábrázoljuk a Jaccard index értékének alakulását. A ggplot magától csak néhány értéket rendelne az x tengelyhez feliratként, amit átállíthatunk a scale_x_continuous() fügvénnyel és a breaks paraméterrel tudjukaz adatfeliratok helyét specifikálni. ggplot(tv_tvjav_minta, aes(ev, jaccard_index)) + geom_jitter(width = 0.1, alpha = 0.5) + scale_x_continuous(breaks = seq(1994, 2018, by = 2)) + labs( x = NULL, y = &quot;Jaccard hasonlóság&quot; ) Figure 10.5: Évenkénti Jaccard hasonlóság A pontdiagram látványos, de esetünkben kevés érdemi információ derül ki róla. A második ábránkon boxplotokkal fogjuk ábrázolni a Jaccard hasonlóság alakulását. ggplot(tv_tvjav_minta, aes(as.factor(ev), jaccard_index)) + geom_boxplot() + labs( x = NULL, y = &quot;Jaccard hasonlóság&quot; ) + theme( axis.text.x = element_text(angle = 45), legend.position = &quot;none&quot; ) Figure 10.6: Évenkénti Jaccard hasonlóság, boxplotokkal Az ábrán szembetn a 2003-as év kiugróan alacsony értéke, azonban itt érdemes figyelembe venni  ami a pontdiagramról is leolvasható , hogy 2003-ra csupán 2 adatpont áll rendelkezésre. A legalacsonyabb Jaccard hasonlóság talán az 19941998-as idszakra jellemz, míg a 20142018-as iszakra szembetnen magas Jaccard értékeket látunk az els ábrázolt ciklushoz képest. Összességében nehéz trendet látni az ábrán, de érdemes azt is megjegyezni, hogy a negatív irányba kiugró adatpontok a 20142018-as ciklusban jelentsen nagyobb arányban tnnek fel, mint a korábbi kormányzati ciklusok alatt. Végezetül pedig ábrázolhatjuk a Jaccard hasonlóságot a benyújtó személye alapján is a korm_ell változónk alapján. A változók értékei a következk a CAP kódkönyve alapján: 0 - Ellenzéki benyújtó 1 - Kormánypárti benyújtó 2 - Kormánypárti és ellenzéki benyújtó közösen 3 - Benyújtók legalább két ellenzéki pártból 4 - Benyújtók legalább két kormánypártból 900 - Nem releváns  a benyújtó a kabinet tagja 901 - Nem releváns  a benyújtó a bizottság tagja volt 902 - Nem releváns  a benyújtó sem a parlamentnek, sem a kabinetnek, sem a bizottságnak nem tagja Mivel nincs túl sok adatpontunk, és ezek többsége a 900-as adatpont alá esik (lásd tv_tvjav_minta %&gt;% count(korm_ell)), érdemes összevonni a 0-ás és a 3-as változót, valamint az 1-es és a 4-es változót egy-egy értékbe, hogy jobban elemezhetek legyenek az eredményeink. Ehhez a korm_ell változó értékei alapján definiálunk egy új korm_ell2 változót. Az új változó definiálását és az értékadásokat a dplyr case_when() függvényével fogjuk megtenni. A függvényen belül a bal oldalra kerül, hogy milyen értékek alapján szeretnénk az új értéket meghatározni, a tilde (~) után pedig az, hogy mi legyen az újonnan létrehozott oszlop értéke. Tehát a case_when()-en belül lév els sor azt fejezi ki, hogy amennyiben a korm_ell egyenl 0-val, vagy (|) a korm_ell egyenl 3-mal, legyen a korm_ell2 értéke 0. tv_tvjav_minta &lt;- tv_tvjav_minta %&gt;% mutate( korm_ell2 = case_when(korm_ell == 0 | korm_ell == 3 ~ &quot;Ellenzéki képvisel&quot;, korm_ell == 1 | korm_ell == 4 ~ &quot;Kormánypárti képvisel&quot;, korm_ell == 2 ~ &quot;Kormányzati és ellenzéki képviselk közösen&quot;, korm_ell == 900 ~ &quot;Kabinet tagja&quot;, korm_ell == 901 ~ &quot;Parlamenti bizottság&quot;, korm_ell == 902 ~ &quot;Egyik sem&quot;), korm_ell2 = as.factor(korm_ell2) ) Miután létrehoztuk az új oszlopot, létrehozhatjuk a vizualizációt is annak alapján. Itt egy speciális pontdiagramot fogunk használni: geom_jitter(). Ez annyiban különbözik a pontdiagramtól, hogy kicsit szórtabban ábrázolja a diszkrét értékekre (évekre) es pontokat, hogy az egy helyen srsöd értékek ne takarják ki egymást. A facet_wrap segítségével tudjuk kategóriánként ábrázolni az évenkénti hasonlóságot. ggplot(tv_tvjav_minta, aes(ev, jaccard_index)) + geom_jitter(width = 0.1, alpha = 0.5) + scale_x_continuous(breaks = seq(1994, 2018, by = 2)) + facet_wrap(~ korm_ell2, ncol = 1) + labs( y = &quot;Jaccard hasonlóság&quot;, x = NULL ) Figure 10.7: Beterjeszto szerinti Jaccard hasonlóság Mivel a törvényjavaslatok túlnyomó többségét a kabinet tagjai nyújtják be, nem igazán tudunk érdemi következtetéseket levonni arra vonatkozóan, hogy az ellenzéki vagy a kormánypárti képviselk által benyújtott javaslatok módosulnak-e többet a vita folyamán. Amennyiben ezzel a kérdéssel alaposabban is szeretnénk foglalkozni, érdemes csak azokat a sorokat kiválasztani a hasonlóság-számításhoz, amelyekben a számunkra releváns megfigyelések szerepelnek. Ha azonban ezt az eljárást választjuk, mindenképpen fontos odafigyelni arra is, hogy az elemzésben használandó megfigyelések kiválogatása nehogy szelektív legyen valamely nem megfigyelt változó szempontjából, ezzel befolyásolva a kutatás eredményeit. https://quanteda.io/reference/textstat_simil.html "],["nlp-ch.html", "11 NLP és névelemfelismerés 11.1 A magyarlanc 11.2 A szeged ner 11.3 Angol nyelv szövegek névelemfelismerése", " 11 NLP és névelemfelismerés A természetes-nyelv feldolgozása (Natural Language Processing  NLP) a nyelvészet és a mesterséges intelligencia közös területe, amely a számítógépes módszerek segítségével elemzi az emberek által használt (természetes) nyelveket. Azaz képes feldolgozni különböz szöveges dokumentumok tartalmát, kinyerni a bennük található információkat, kategorizálni és rendszerezni azokat. Angol nyelv szövegek NLP elemzésére több R csomag is rendelkezésünre áll, ezek közül kettt mutatunk be röviden. Mivel magyar nyelv szövegek NLP elemzésére ezek a csomagok jelenleg nem alkalmasak, azt mutatjuk be, hogyan végezhetjük el a magyar nyelv szövegek mondatra és szavakra bontását, szófaji egyértelmsítését, morfológiai és szintaktikai elemzését az R program használata nélkül és azután a kapott fájlokkal hogyan végezhetünk az R program segítségével további elemzéseket.45 A fejezetben részletesen foglalkozunk a névelem-felismeréssel (Named Entity Recognition  NER). Névelemnek azokat a tokensorozatokat nevezzük, amelyek valamely entitást egyedi módon jelölnek. A névelem-felismerés az infomációkinyerés részterülete, melynek lényege, hogy automatikusan felismerjük a strukturálátlan szövegben szerepl tulajdonneveket, majd azokat kigyjtsük, és típusonként (például személynév, földrajzi név, márkanév, stb.) csoportosítsuk. Bár a tulajdonnevek mellett névelemnek tekinthetk még például a telefonszámok vagy az e-mail címek is, a névelem-felismerés leginkább mégis a tulajdonnevek felismerésére irányul. A névelem-felismerés a számítógépes nyelvészetben a korai 1990-es évektl kezdve fontos feladatnak és megoldandó problémának számít. A névelem-felismerés többféle módon is megoldható, így például felügyelt tanulással, szótár alapú módszerekkel vagy kollokációk elemzésével. A névelem-felismerés körében két alapvet módszer alkalmazására van lehetség. A szabályalapú módszer alkalmazása során elre megadott adatok alapján kerül kinyerésre az információ (ilyen szabály például a mondatközi nagybet mint a tulajdonnév kezdete). A másik módszer a statisztikai tanulás, amikor a gép alkot szabályokat a kutató elzetes mintakódolása alapján. A névelemfelismerés során nehézséget okozhat a különböz névelemosztályok közötti gyakori átfedés, így például ha egy adott szó településnév és vezetéknév is lehet. A magyar nyelv szövegekben a tulajdonnevek automatikus annotációjára jelenleg három módon van lehetség: tulajdonnév-felismer algoritmussal, szófaji címke szintjén történ megkülönböztetéssel, valamint szintaktikai szint címkézéssel. Utóbbi kettre példa a fejezetben is bemutatásra kerül magyarlanc elemz, ami szófaji szinten megkülönbözteti a tulajdonneveket, a szintaxis szintjén pedig jelöli a többtagúakat.(Zsibrita, Vincze, and Farkas 2013) A tulajdonnév-felismer algoritmusok megkeresik az adott szövegben a tulajdonneveket, majd azokat valamilyen kategóriába sorolják, ilyen magyar nyelv algoritmus a szeged ner, melynek alkalmazását szintén bemutatjuk.(Szarvas, Farkas, and Kocsor 2006) Fontos különbséget tenni a névelem-felismerés és a tulajdonnév-felismerés között. A névelem-felismerésbe beletartozik minden olyan kifejezés, amely a világ valamely entitására egyedi módon (unikálisan) referál. Ezzel szemben a tulajdonnév-felismerés, kizárólag a tulajdonnevekre koncentrál.(Üveges 2019; Vincze 2019) A magyarlancnyelvi elfeldolgozó eszköz a Szegedi Tudományegyetem fejlesztése,(Zsibrita, Vincze, and Farkas 2013) ami magyar nyelv txt formátumú fájlokat feldolgozva képes egy szöveg mondatokra és szavakra bontására, a szavak morfológiai elemzésére, szófaji egyértelmsítésére, emellett kétféle szintaktikai elemzést is képes hozzárendelni a mondatokhoz.46 A magyarlanchoz hasonlóan az UDPipe nev elemz szintén képes magyar nyelv nyers szövegek mondatra és szavakra bontására és szófaji elemzésére, azaz POS-taggelésére (Part of Speech-tagging) továbbá a mondatok függségi elemzésére. Ez az elemz a nemzetközileg elismert Universal Dependencies annotációs sémán alapul. (Straka and Straková 2017) A két nyelvi elemz hasonló funkcionalitásokkal rendelkezik, ugyanakkor az UDPipe technikailag könnyebben kezelhet, azonban kevésbé pontos elemzési eredményt ad, mivel jóval kisebb tanító anyagon lett betanítva, mint a magyarlanc.47 Az alábbiakban a magyarlanc és a szeged ner mködését és az általuk létrehozott fájlokkal R-ben végezhet elemzésekre mutatunk példákat. 11.1 A magyarlanc Az elemz használatának részletes leírás megtalálható a már jelzett honlapon, itt most csak vázlatosan ismeretetjük. Fontos kiemelni, hogy a magyarlanc JAVA modulokból áll, így használatához szükséges, hogy a számítógépen megfelel JAVA környezet legyen telepítve. Elször fenti oldalról le kell töltenünk a magyarlanc-3.0.jar fájlt, majd bemásolni azt abba a mappába, ahol az elemezni kívánt txt található. A parancssort Windows operációs rendszer alatt a számítógép keres mezjébe a cmd parancs beírásával tudjuk megnyitni. Ezután a parancsorban belépve abba a könyvtárba, ahol az elemezni kíván txt és a magyarlanc-3.0.jar elemz van, az alábbi parancs segítségével végezhetjük el az elemzést: java -Xmx1G -jar magyarlanc-3.0.jar -mode morphparse -input in.txt -output out.txt, ahol az in.txt helyébe az elemezni kívánt txt nevét, az out.txt helyébe, pedig az elemzés eredményeként létrejöv fájl nevét kell megadni. Példánkban az Országgylésben 2014 és 2018 között elhangzott véletlenszeren kiválasztott 25 napirend eltti felszólalás korpuszán szemléltetjük az elemz mködését.48 A 25 fájlt elemezhetjük egyesével, de ha ez a késbbi elemzéshez nem szükséges, a parancsorban a copy *.txt eredmeny.txt paranccsal egyesíthetjük azokat egy fájlba. Majd ezen az eredmeny.txt-n végezzük el az elemzést az alábbi paranccsal: java -Xmx1G -jar magyarlanc-3.0.jar -mode morphparse -input eredmeny.txt -output eredmeny_out.txt Az elemzés eredményéül kapott txt fájlban láthatjuk, hogy az elemz elvégezte a szövegek mondatokra bontását, tokenizálását, szótári alakra hozását és POS-taggelését, azaz meghatározta a szavak szófaját. Figure 11.1: A magyarlánc elemzo nyers eredménye Ezt követen célszer a txt fájlt excelbe beolvasva oszlopokra tagolni, az oszlopokat fejléccel ellátni, majd csv fájlként elmenteni. Figure 11.2: Az Excelben megnyitott magyarlánc eredmény Az így létrehozott .csv fájllal megyegyez adattáblát be tudjuk tölteni a HunMineR segítségével. library (dplyr) library(HunMineR) napirend_elotti &lt;- HunMineR::data_parlspeech_magyarlanc Az így létrehozott objektummal, mely esetünkben 17870 megfigyelést és 4 változót tartalmaz, ezután különböz mveleteket végezhetünk, a korábban már bemutatottak szerint, például dplyr csomag filter függvénye segítségével kiválogathatjuk az igéket, és elmenthetjük azokat egy újabb 1769 megfigyelést és 4 változót tartalmazó objektumba. verb_napirend_elotti &lt;- napirend_elotti %&gt;% filter(POS_tag == &quot;VERB&quot;) 11.2 A szeged ner A magyarlanc nyelvi elemzhöz hasonlóan használhatjuk a szeged ner elemzt is, melynek részletes leírása és maga a ner.jar elemz is megtalálható az alábbi oldalon: https://rgai.inf.u-szeged.hu/node/109. Az elemz a fent bemutatott módon szintén parancssorból indítható az alábbi parancs használatával: java -Xmx3G -jar ner.jar -mode predicate -input input.txt -output output.txt. Az elemz PER (személynév), LOC (hely(szín)), ORG (szervezet) és MISC (vegyes) címkét ad az egyes névelemeknek. Figure 11.3: A szeged ner elemzo eredménye A fentiekhez hasonlóan ezt a txt-t is átalakíthatjuk táblázattá, majd ezt a csv fájlt beolvashatjuk (a HunMineR csomag szintén tartalmazza ezt a data frame-et). napirend_elotti_ner &lt;- HunMineR::data_parlspeech_szner Ezután tetszlegesen kiválogathatjuk például a helyek neveit. A filterezés eredményeként láthatjuk, hogy az elemz korpuszunkban 175 szót azonosított és címkézett fel helynévként. loc_napirend_elotti &lt;- napirend_elotti_ner %&gt;% filter(ner == &quot;I-LOC&quot;) De ugyanígy kiváogathatjuk a személyneveket is, azonban itt figyelembe kell vennünk, hogy az elemz külön névelemként jelöli a vezeték és keresztneveket, a további elemzés szükségletei szerint ezeket utólag kell összevonnunk. pers_napirend_elotti &lt;- napirend_elotti_ner %&gt;% filter(ner == &quot;I-PER&quot;) Az így kiválogatott különböz névelemekkel azután további elemzéseket végezhetünk. 11.3 Angol nyelv szövegek névelemfelismerése Amennyiben angol nyelv korpusszal dolgozunk több lehetség is a rendelkezésünkre áll a névelemfelismerés elvégzésére.49 Ezek közül most röviden a spacyr használatát mutatjuk be.50 A spaCy nem egy R, hanem egy Phyton csomag51, amely azonban az R reticulate csomag segítségével nagyon jól együttmködik a kötetben rendszeresen használt quanteda csomaggal. Használatához a már megszokott módon installálnunk kell a spacyr csomagot, majd beolvasnunk és telepítenünk az angol nyelvi modellt. A Pythonban készült spacy-t a spacyr::spacy_install() paranccsal kell telepíteni (ezt elég egyszer megtenni, amikor elször használjuk a csomagot). library(spacyr) library(quanteda) library(ggplot2) spacy_initialize(model = &quot;en_core_web_sm&quot;) A spacy_parse() függvény segítségével lehetségünk van a szövegek tokenizálására, szótári alakra hozására és szófaji egyértelmsítésére. txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) # process documents and obtain a data.table parsedtxt &lt;- spacy_parse(txt) parsedtxt #&gt; doc_id sentence_id token_id token lemma pos entity #&gt; 1 d1 1 1 spaCy spacy NOUN #&gt; 2 d1 1 2 is be AUX #&gt; 3 d1 1 3 great great ADJ #&gt; 4 d1 1 4 at at ADP #&gt; 5 d1 1 5 fast fast ADJ #&gt; 6 d1 1 6 natural natural ADJ #&gt; 7 d1 1 7 language language NOUN #&gt; 8 d1 1 8 processing processing NOUN #&gt; 9 d1 1 9 . . PUNCT #&gt; 10 d2 1 1 Mr. Mr. PROPN #&gt; 11 d2 1 2 Smith Smith PROPN PERSON_B #&gt; 12 d2 1 3 spent spend VERB #&gt; 13 d2 1 4 two two NUM DATE_B #&gt; 14 d2 1 5 years year NOUN DATE_I #&gt; 15 d2 1 6 in in ADP #&gt; 16 d2 1 7 North North PROPN GPE_B #&gt; 17 d2 1 8 Carolina Carolina PROPN GPE_I #&gt; 18 d2 1 9 . . PUNCT Az elvégzett tokenizálás eredményébl adattáblát készíthetünk. spacy_tokenize(txt, remove_punct = TRUE, output = &quot;data.frame&quot;) %&gt;% tail() #&gt; doc_id token #&gt; 11 d2 spent #&gt; 12 d2 two #&gt; 13 d2 years #&gt; 14 d2 in #&gt; 15 d2 North #&gt; 16 d2 Carolina Ugyancsak lehetségünk van a különböz entitások, így például a tulajdonnevek kinyerésére. parsedtxt &lt;- spacy_parse(txt, lemma = FALSE, entity = TRUE, nounphrase = TRUE) entity_extract(parsedtxt) #&gt; doc_id sentence_id entity entity_type #&gt; 1 d2 1 Smith PERSON #&gt; 2 d2 1 North_Carolina GPE A tulajdonneveken túl felcímkézhetjük a dátumokat, eseményeket is. entity_extract(parsedtxt, type = &quot;all&quot;) #&gt; doc_id sentence_id entity entity_type #&gt; 1 d2 1 Smith PERSON #&gt; 2 d2 1 two_years DATE #&gt; 3 d2 1 North_Carolina GPE Az entity_consolidate() függvény segítségével arra is lehetségünk van, hogy a több szóból álló entitásokat egy tokenként kezeljük. entity_consolidate(parsedtxt) %&gt;% tail() #&gt; doc_id sentence_id token_id token pos entity_type #&gt; 11 d2 1 2 Smith ENTITY PERSON #&gt; 12 d2 1 3 spent VERB #&gt; 13 d2 1 4 two_years ENTITY DATE #&gt; 14 d2 1 5 in ADP #&gt; 15 d2 1 6 North_Carolina ENTITY GPE #&gt; 16 d2 1 7 . PUNCT A nounphrase_extract() függvény lehetséget ad az összetartozó kifejezések összefzésére. nounphrase_extract(parsedtxt) #&gt; doc_id sentence_id nounphrase #&gt; 1 d1 1 spaCy #&gt; 2 d1 1 fast_natural_language_processing #&gt; 3 d2 1 Mr._Smith #&gt; 4 d2 1 two_years #&gt; 5 d2 1 North_Carolina Majd képes arra, hogy ezeket az összetartozó kifejezéseket egyben kezelje. nounphrase_consolidate(parsedtxt) #&gt; doc_id sentence_id token_id token pos #&gt; 1 d1 1 1 spaCy nounphrase #&gt; 2 d1 1 2 is AUX #&gt; 3 d1 1 3 great ADJ #&gt; 4 d1 1 4 at ADP #&gt; 5 d1 1 5 fast_natural_language_processing nounphrase #&gt; 6 d1 1 6 . PUNCT #&gt; 7 d2 1 1 Mr._Smith nounphrase #&gt; 8 d2 1 2 spent VERB #&gt; 9 d2 1 3 two_years nounphrase #&gt; 10 d2 1 4 in ADP #&gt; 11 d2 1 5 North_Carolina nounphrase #&gt; 12 d2 1 6 . PUNCT Arra is lehetség van, hogy az egyes kifejezések közötti függségeket vizsgáljuk. spacy_parse(txt, dependency = TRUE, lemma = FALSE, pos = FALSE) #&gt; doc_id sentence_id token_id token head_token_id dep_rel entity #&gt; 1 d1 1 1 spaCy 2 nsubj #&gt; 2 d1 1 2 is 2 ROOT #&gt; 3 d1 1 3 great 2 acomp #&gt; 4 d1 1 4 at 2 prep #&gt; 5 d1 1 5 fast 8 amod #&gt; 6 d1 1 6 natural 7 amod #&gt; 7 d1 1 7 language 8 compound #&gt; 8 d1 1 8 processing 4 pobj #&gt; 9 d1 1 9 . 2 punct #&gt; 10 d2 1 1 Mr. 2 compound #&gt; 11 d2 1 2 Smith 3 nsubj PERSON_B #&gt; 12 d2 1 3 spent 3 ROOT #&gt; 13 d2 1 4 two 5 nummod DATE_B #&gt; 14 d2 1 5 years 3 dobj DATE_I #&gt; 15 d2 1 6 in 3 prep #&gt; 16 d2 1 7 North 8 compound GPE_B #&gt; 17 d2 1 8 Carolina 6 pobj GPE_I #&gt; 18 d2 1 9 . 3 punct A következkben a Szótáralapú elemzések, érzelem-elemzés fejezetben is használt Magyar Nemzeti Bank kamatdöntéseit kísér nemzetközi sajtóközleményei korpuszán mutatunk be egy példát a névelemfelismerésre és az eredmények vizualizálására. Els lépésként beolvassuk a szövegeket, majd a már megismert quanteda csomag segítségévek korpuszt készítünk bellük. mnb_df &lt;- HunMineR::data_mnb_pr corpus_mnb &lt;-corpus(mnb_df) Ezután a spacy_extract_entity() függvénye segítségévek elvégezzük a névelemfelismerést, a függvény argumentumában megadva, hogy milyen tipusú névelemeket szeretnénk kigyjteni a korpuszból. A lehetséges típusok a named, extended, vagy all.52 Az elemzés eredménye pedig készülhet listában, vagy készülhet belle adattábla. Példánkban mi kimenetként listát állítottunk be. A névelemek tokenjeit ezután megritkítottuk, és csak azokat hagytuk meg, amelyek legalább nyolc alkalommal szerepeltek a korpuszban. mnb_ner &lt;- spacy_extract_entity( corpus_mnb, output = c(&quot;list&quot;), type = (&quot;named&quot;), multithread = TRUE) mnb_tokens &lt;- tokens(mnb_ner) features &lt;- dfm(mnb_tokens) %&gt;% dfm_trim(min_termfreq = 8) %&gt;% featnames() mnb_tokens &lt;- tokens_select(mnb_tokens, features, padding = TRUE) Ezután a különböz alakban elforduló, de ugyanarra az entitásra vonatkozó névelemeket összevontuk. mnb &lt;- c(&quot;Magyar Nemzeti Bank&quot;, &quot;MAGYAR NEMZETI BANK&quot;, &quot;The Magyar Nemzeti Bank &quot;, &quot;the Magyar Nemzeti Bank&quot;, &quot;MNB&quot;, &quot;the Magyar Nemzeti Banks&quot;, &quot;Nemzeti Bank&quot;, &quot;The Magyar Nemzeti Banks MNB&quot; ) lemma &lt;- rep(&quot;Magyar Nemzeti Bank&quot;, length(mnb)) mnb_tokens &lt;- tokens_replace(mnb_tokens, mnb, lemma, valuetype = &quot;fixed&quot;) mc &lt;- c(&quot;Monetary Council&quot;, &quot;MONETARY COUNCIL&quot;, &quot;Magyar Nemzeti Bank Monetary Council&quot;, &quot;MAGYAR NEMZETI BANK Monetary Council&quot;, &quot;NEMZETI BANK Monetary Council&quot; ,&quot;The Monetary Council&quot;, &quot;Council&quot;, &quot;The Council&quot;, &quot;Councils&quot;, &quot;the Monetary Council&quot;, &quot;Monetary Councils&quot;, &quot;the Monetary Councils&quot;, &quot;The Monetary Councils&quot;, &quot;Monetray Council&quot;, &quot;May the Monetary Council&quot;) lemma &lt;- rep(&quot;Monetary Council&quot;, length(mc)) mnb_tokens &lt;- tokens_replace(mnb_tokens, mc, lemma, valuetype = &quot;fixed&quot;) Majd elkészítettük a szóbeágyazás fejezetben már megismert fcm-et, végezetül pedig egy együttes elfordulási mátrixot készítettünk a kinyert entitásokból és a ggplot segítségével ábrázoltuk.53 mnb_fcm &lt;- fcm(mnb_tokens, context = &quot;window&quot;, count = &quot;weighted&quot;, weights = 1 / (1:5), tri = TRUE) feat &lt;- names(topfeatures(mnb_fcm, 80)) mnb_fcm_select &lt;- fcm_select(mnb_fcm, pattern = feat, selection = &quot;keep&quot;) dim(mnb_fcm_select) #&gt; [1] 35 35 size &lt;- log(colSums(dfm_select(mnb_fcm, feat, selection = &quot;keep&quot;))) set.seed(144) textplot_network(mnb_fcm_select, min_freq = 0.7, vertex_size = size / max(size) * 3) Figure 11.4: A Magyar Nemzeti Bank korpusz névelemeinek együttelofordulási mátrixa A spacyr alapveten az angol nyelvi modellel mködik, de arra is van lehetség, hogy a spaCy egyéb beépített nyelvi modelljeit (német, spanyol, portugál) használjuk. Létezik magyar nyelvi modell is, ez azonban jelenleg még nincs integrálva a spaCy-be, hanem egy GitHub repozitoriumból tölthet le. Ennek R-ben történ megvalósításához azonban haladó R ismeretek szükségesek, azért ennek leírásától jelen kötetben eltekintünk. A magyar nyelvi modell és leírása elérhet az alábbi linken: https://github.com/oroszgy/spacy-hungarian-models Magyar nyelv szövegek NLP elemzésére használható eszközök részletes listája: https://github.com/oroszgy/awesome-hungarian-nlp A magyarlanc elérhet a &lt;https://rgai.inf.u-szeged.hu/magyarlanc&gt; oldalon, az innen letölthet jar fájl segítségével a txt formátumú szövegfájlok elemzése parancssorból lehetséges. Az UDPipe elérhetsége: http://lindat.mff.cuni.cz/services/udpipe A napirend eltti felszólalásokat tartalmazó korpusz a Hungarian Comparative Agendas Project keretében készült: https://cap.tk.hu/hu További lehetségekhez lásd például: https://analyticsindiamag.com/top-10-r-packages-for-natural-language-processing-nlp/ Részletes leírása: https://spacyr.quanteda.io/articles/using_spacyr.html Részletes leírása: https://spacy.io/usage/linguistic-features#named-entities Részletes leírását lásd: https://towardsdatascience.com/extend-named-entity-recogniser-ner-to-label-new-entities-with-spacy-339ee5979044 Részletes leírását lásd: https://tutorials.quanteda.io/basic-operations/fcm/fcm/ "],["osztályozás-és-felügyelt-tanulás.html", "12 Osztályozás és felügyelt tanulás 12.1 Fogalmi alapok 12.2 Osztályozás felügyelt tanulással", " 12 Osztályozás és felügyelt tanulás 12.1 Fogalmi alapok A mesterséges intelligencia két fontos társadalomtudományi alkalmazási területe a felügyelet nélküli és a felügyelt tanulás. Míg az elz esetben  ahogy azt a Felügyelet nélküli tanulás: topik modellezés magyar törvényszövegeken fejezetben bemutattuk  az emberi beavatkozás néhány kulcsparaméter megadására (így pl. a kívánt topikok számának meghatározására) szorítkozik, addig a felügyelt tanulás esetében a kutatónak nagyobb mozgástere van tanítani a gépet. Ennyiben a felügyelt tanulás alkalmasabb hipotézisek tesztelésére, mint az adatok rejtett mintázatait felfedez felügyelet nélküli tanulás. A felügyelt tanulási feladat megoldása egy úgynevezett tanító halmaz (training set) meghatározásával kezddik, melynek során a kutatók saját maguk végzik el kézzel azt a feladatot melyet a továbbiakban gépi közremködéssel szeretnének nagyobb nagyságrendben, de egyben érvényesen (validity) és megbízhatóan (reliability) kivitelezni. Eredményeinket az ugyanúgy eredetileg kézzel lekódolt, de a modell-építés során félretett teszthalmazunkon (test set) értékelhetjük. Ennek során négy kategóriába rendezzük modellünk elrejelzéseit. Egy, a politikusi beszédeket a pozitív hangulatuk alapján osztályozó példát véve ezek a következk: azok a beszédek amelyeket a modell helyesen sorolt be pozitívba (valódi pozitív), vagy negatívba (valódi negatív), illetve azok, amelyek hibásan szerepelnek a pozitív (ál-pozitív), vagy a negatív kategóriában (ál-negatív). Mindezek együttesen egy ún. tévesztési táblát (confusion matrix) adnak, melynek további elemzésével ítéletet alkothatunk modellépítésünk eredményességérl. A felügyelt tanulás számos kutatási feladat megoldására alkalmazhatjuk, melyek közül a leggyakoribbak a különböz osztályozási (classification) feladatok. Miközben ezek  így pl. a véleményelemzés  szótáralapú módszertannal is megoldhatóak (lásd a Szótáralapú elemzések, érzelem-elemzés fejezetet), a felügyelt tanulás a nagyobb élmunkaigényt rendszerint jobb eredményekkel és rugalmasabb felhasználhatósággal hálálja meg (gondoljunk csak a szótárak domain-függségére). A felügyelt tanulás egyben a mesterséges intelligencia kutatásának gyorsan fejld területe, mely az e fejezetben tárgyalt algoritmus-központú gépi tanuláson túl az ún. mélytanulás (deep learning) és a neurális hálók területén is zajlik egyre látványosabb sikerekkel. 12.2 Osztályozás felügyelt tanulással Az alábbi fejezetben a CAP magyar média gyjteményébl a napilap címlapokat tartalmazó modult használjuk.54 Az induló adatbázis 61 835 cikk szövegét és metaadatait (összesen öt változót: sorszám, fájlnév, a közpolitikai osztály kódja, szöveg, illetve a korpusz forrása  Magyar Nemzet vagy Népszabadság) tartalmazza. Az a célunk, hogy az egyes cikkekhez kézzel, jó minségben (két, egymástól függetlenül dolgozó kódoló által) kiosztott és egyeztetett közpolitikai kódokat  ez a tanítóhalmaz  arra használjuk, hogy meghatározzuk egy kiválasztott cikkcsoport hasonló kódjait. Az osztályozási feladathoz a CAP közpolitikai kódrendszerét használjuk, mely 21 közpolitikai kategóriát határoz meg az oktatástól az egészségügyön át a honvédelemig.55 Annak érdekében, hogy egyértelmen értékelhessük a gépi tanulás hatékonyságát, a kiválasztott cikkcsoport (azaz a teszthalmaz) esetében is ismerjük a kézi kódolás eredményét (éles kutatási helyzetben, ismeretlen kódok esetében ugyanakkor ezt gyakran szintén csak egy kisebb mintán tudjuk kézzel validálni). További fontos lépés, hogy az észszer futási id érdekében a gyakorlat során a teljes adatbázisból  és ezen belül is csak a Népszabadság részhalmazból  fogunk venni egy 4500 darabos mintát. E mintán fogjuk vizsgálni, hogy milyen hatékonysággal képes a modellünk egy megadott közpolitikai kódba besorolni egy adott cikket, illetve, hogy ezt képes-e a hús-vér kutatókkal azonos színvonalon megtenni. Nézzük mindezek után a kutatás lépéseit! A elször behívjuk a szükséges csomagokat, melyek közül a quanteda a szokásos szövegbányászati alapcsomagunk, az e1071 az SVM algoritmus használatát teszi a lehetvé, a ggplot2 a vizualizációt segíti, a dplyr a korpuszon végzett mveletekhez kell, a SparseM pedig a mátrixtranszformációkhoz. library(quanteda) library(e1071) library(ggplot2) library(dplyr) library(SparseM) library(HunMineR) set.seed(1234) Ezt követen megadjuk azt a kódkategóriát, melynek kapcsán szeretnénk elvégezni a bináris osztályozást (beletartozik-e az adott cikk az adott kategóriába vagy nem). A kódban ez az egyes, azaz a makrogazdaság (adózás, költségvetés, monetáris politika stb.) lesz. CAPcode &lt;- 1 Ezt követi a szövegelkészítésen és tisztításon már átesett adatok betöltése. Az egyszerség kedvéért a HunMineR csomagból töltjük be az elkészített adatokat. Data_NOL_MNO &lt;- HunMineR::data_nol_mno_clean A következ lépésben eltávolítjuk a szöveges adatot nem tartalmazó (a text változóra semmilyen értéket nem adó) sorokat az adatbázisból. Data_NOL_MNO_ures &lt;- Data_NOL_MNO[Data_NOL_MNO$text == &quot;&quot;,] Data_NOL_MNO &lt;- Data_NOL_MNO[!(Data_NOL_MNO$filename %in% Data_NOL_MNO_ures$filename),] nrow(Data_NOL_MNO_ures) #&gt; [1] 13 Látható, hogy összesen 13 ilyen elemet találtunk. Majd szétválasztjuk a két újsághoz tartozó cikkeket is (esetünkben a Népszabadságot alapul véve). Data_NOL &lt;- Data_NOL_MNO[Data_NOL_MNO$corpus == &quot;NOL&quot;,] Mint az Environment ablakban is látható, ez a lépés 37 135 cikkes halmazt adott. Ezen a ponton megkezdhetjük a felügyelt gépi tanulásra épül lépéseket! Elször egy 4500-as mintát veszünk az adatbázisból, hogy rövidítsük a futásidt. Data_NOL &lt;- Data_NOL[sample(nrow(Data_NOL), 4500), ] Majd kutatási feladatunknak megfelelen felveszünk egy új címkét (label), ami már kifejezetten azt mutatja, hogy egy cikk a makrogazdasági (1) vagy bármilyen más (0) osztályba tartozik-e. Data_NOL$label &lt;- ifelse(Data_NOL$majortopic_code == CAPcode, 1, 0) Ezt követen felosztjuk a 4500 cikket 2:1 arányban tanító- és teszthalmazra. training &lt;- Data_NOL[sample(nrow(Data_NOL), 3000), ] Data_NOL$training &lt;- ifelse(Data_NOL$filename %in% training$filename, 1, 0) Ahhoz, hogy a quanteda dtm-et tudjon készíteni a cikk-gyjteményünkbl, elször egy korpusz objektumot kell létrehozni bellük. lemma_corpus_NOL &lt;- corpus(Data_NOL, docid_field = &quot;filename&quot;, text_field = &quot;text&quot;) Így a dfm függvénnyel már létre tudjuk hozni a dokumentum-kifejezés mátrixunkat! dtm_NOL &lt;- dfm(lemma_corpus_NOL) A dimenzió-csökkentés érdekében eltávolítjuk a ritka (5-nél kevesebbszer szerepl) kifejezéseket a mátrixból. Láthatjuk, hogy ezzel a feature-ök majd 81 százalékát eltávolítottuk. dtm_NOL &lt;- dfm_trim(dtm_NOL, min_docfreq = 5, verbose=TRUE) Ezen a ponton megkezdhetjük a modellünk tanítását! Erre a fent említett SVM algoritmust használjuk. E lépés futási ideje akár 12 percig is eltarthat. SVMmodel &lt;- svm( x=dtm_NOL[dtm_NOL@docvars$training == 1,], y =dtm_NOL[dtm_NOL@docvars$training == 1,]@docvars$label, kernel=&quot;linear&quot;, cost =0.1, probability=TRUE ) A létrejöv predictions objektum tartalmazza majd az elrejelzési eredményeket. predictions &lt;- predict(SVMmodel, dtm_NOL[dtm_NOL@docvars$training == 0,]) Data_NOL_predictions &lt;- cbind(Data_NOL[Data_NOL$training == 0,], predictions) Ezt követen meghatározzuk azt a küszöbértéket, ahonnan egy elrejelzést 1-es címkéhez tartozónak (azaz egy cikk szövegét makrogazdaságinak) tekintünk. cutoff_point = 0.5 Majd ez alapján átalakítjuk az SVM eredményeinket immár a végleges, a bináris osztályozási feladatnak megfelel elrejelzésekké. A cbind függvénnyel pedig összevonjuk egy táblázatba az elrejelzés eredményét és annak cimkéit. predictions_label &lt;- ifelse(predictions &gt; 0.5, 1, 0) Data_NOL_predictions &lt;- cbind(Data_NOL_predictions, predictions_label) Miután eredményeink elkészültek, kiszámoljuk a felügyelt gépi tanulás szokásos eredményességi mutatóit (ettl a résztl kódunkban Pablo Barbera munkájára támaszkodunk)56. Mindezek alapjául a tévesztési mátrix szolgál, elször ennek adatait vizsgáljuk meg. table(Data_NOL_predictions$predictions_label, Data_NOL_predictions$label) #&gt; #&gt; 0 1 #&gt; 0 1086 114 #&gt; 1 206 94 Látható, hogy modellünk alapveten helyesen értékelte, hogy a teszthalmaz cikkeinek dönt többsége nem makrogazdasági tárgyú (1139). Szintén sikerrel sorolt be 83 cikket a makrogazdasági osztályba. Ugyanakkor 117-et hibásan tartott 1-es kategóriásnak, 161-et pedig nem talált meg közülük. A mutatószámok esetében kezdjük a legáltalánosabbal, a hitelességgel (accuracy), ami a mátrix átlójára épít. accuracy &lt;- function(ypred, y){ tab &lt;- table(ypred, y) return(sum(diag(tab))/sum(tab)) } accuracy(Data_NOL_predictions$predictions_label, Data_NOL_predictions$label) #&gt; [1] 0.7866667 Ennek adatai azt mutatják, hogy nagy általánosságban jól mködött a modell. Ugyanakkor kutatási kérdésünk szempontjából fontosabb, hogy milyen arányban találtuk meg a makrogazdasági cikkeket (felidézés  recall), illetve milyen pontossággal osztottuk ki ezeket a kódokat (pontosság  precision). A következ lépésben így elször definiáljuk, majd kiszámoljuk a pontosságot és a felidézést, valamint ezek harmonikus átlagát, az F1 mutatót. precision &lt;- function(ypred, y){ tab &lt;- table(ypred, y) return((tab[2,2])/(tab[2,1]+tab[2,2])) } recall &lt;- function(ypred, y){ tab &lt;- table(ypred, y) return(tab[2,2]/(tab[1,2]+tab[2,2])) } F1 &lt;- function(ypred, y){ return(2*precision(ypred, y)*recall(ypred, y)/(precision(ypred, y)+recall(ypred, y))) } precision(Data_NOL_predictions$predictions_label, Data_NOL_predictions$label) #&gt; [1] 0.3133333 recall(Data_NOL_predictions$predictions_label, Data_NOL_predictions$label) #&gt; [1] 0.4519231 F1(Data_NOL_predictions$predictions_label, Data_NOL_predictions$label) #&gt; [1] 0.3700787 Összességében azt állapíthatjuk meg, hogy játékmodellünk közel hasonló arányú, azaz 60 százalékos pontosságot, illetve fedést eredményezett. Ez elmaradt a ketts vak kézi kódolás akár 80-90 százalékos találati arányától, ugyanakkor mintánkat didaktikai okokból szándékosan kisebbre vettük a lehetségesnél. A nagymintás adatok alapján (lásd Sebk and Kacsuk (2021)) különösen a pontosság esetében akár 90 százalék feletti érték is elérhet felügyelt gépi tanulással, mely ponton már világossá válik a mesterséges intelligenciára épül megközelítés versenyképessége a kézi kódoláséval, különösen nagymintás projektek esetében. De térjünk még vissza eredményeinkhez és vizsgáljuk meg ket közpolitikai kódonként is! Itt is látható, hogy 83 cikkre jeleztük elre helyesen az 1-es kódot. table(Data_NOL_predictions$predictions_label, Data_NOL_predictions$majortopic_code) #&gt; #&gt; 1 2 3 4 5 6 7 8 9 10 12 13 14 15 16 17 18 19 20 21 23 #&gt; 0 114 18 49 16 21 35 7 15 5 38 48 17 7 26 27 36 4 290 80 4 14 #&gt; 1 94 4 10 12 11 7 3 9 2 12 12 5 8 10 5 1 2 25 23 0 0 #&gt; #&gt; 26 27 28 29 31 33 34 35 36 99 #&gt; 0 12 8 80 116 11 58 8 27 8 1 #&gt; 1 3 1 11 10 0 13 2 5 0 0 Végezetül elrejelzéseinket ezek kódonkénti gyakorisága szempontjából ábrázoljuk. ggplot_data &lt;- Data_NOL_predictions[,c(&quot;majortopic_code&quot;,&quot;predictions_label&quot;)] ggplot_data$predictions_label &lt;- factor(ggplot_data$predictions_label) df &lt;- ggplot_data %&gt;% group_by(majortopic_code, predictions_label) %&gt;% summarise(n = n()) %&gt;% filter(majortopic_code &lt; 30) ggplot(df, aes(majortopic_code, n, fill = predictions_label)) + geom_bar(, stat = &quot;identity&quot;) + labs(fill = NULL) + theme(legend.position = &quot;bottom&quot;) Figure 12.1: Az SVM modell klasszifikációs eredményei A korpusz regisztációt követen elérhet az alábbi linken: https://cap.tk.hu/a-media-es-a-kozvelemeny-napirendje A kódkönyv regisztrációt követen elérhet az alábbi linken: https://cap.tk.hu/kozpolitikai-cap Elérhet az alábbi linken: http://pablobarbera.com/ECPR-SC105/code/12-advanced-sml.html. "],["fuggelek.html", "13 Függelék 13.1 Az R és az RStudio használata 13.2 Az RStudio kezdfelülete 13.3 Projekt alapú munka 13.4 Scriptek szerkesztése, függvények használata 13.5 R csomagok 13.6 Objektumok tárolása, értékadás 13.7 Vektorok 13.8 Faktorok 13.9 Az adattáblák 13.10 Vizualizáció", " 13 Függelék 13.1 Az R és az RStudio használata Az R egy programozási nyelv, amely alkalmas statisztikai számítások elvégzésére és ezek eredményeinek grafikus megjelenítésére. Az R ingyenes, nyílt forráskódú szoftver, mely telepíthet mind Windows, mind Linux, mind MacOS operációs rendszerek alatt, az alábbi oldalról: https://cran.r-project.org/ Az RStudio az R integrált fejleszti környezete (integrated development environment  IDE), mely egy olyan felhasználóbarát felületet biztosít, ami egyszerbb és átláthatóbb munkát tesz lehetvé. Az RStudio az alábbi oldalról tölthet le: https://rstudio.com/products/rstudio/download/ A point and click\" szoftverekkel szemben az R használata során scripteket kell írni, ami bizonyos programozási jártasságot feltételez, de a késbbiekben lehetvé teszi azt adott kutatási kérdéshez maximálisan illeszked kódok összeállítását, melyek segítségével az elemzések mások számára is megbízhatóan reprodukálhatók lesznek. Ugyancsak az R használata mellett szól, hogy komoly fejleszti és felhasználói közösséggel rendelkezik, így a használat során felmerül problémákra általában gyorsan megoldást találhatunk. 13.2 Az RStudio kezdfelülete Az RStudio kezdfelülete négy panelbl, eszközsorból és menüsorból áll: Figure 13.1: RStudio felhasználói felület Az (1) editor ablak szolgál a kód beírására, futtatására és mentésére. A (2) console ablakban jelenik meg a lefuttatott kód és az eredmények. A jobb fels ablak (3) environment fülén láthatóak a memóriában tárolt adatállományok, változók és felhasználói függvények. A history fül mutatja a korábban lefuttatott utasításokat. A jobb alsó ablak (4) files fülén az aktuális munkakönyvtárban tárolt mappákat és fájlokat találjuk, míg a plot fülön az elemzéseink során elkészített ábrák jelennek meg. A packages fülön frissíthetjük a meglév r csomagokat és telepíthetünk újakat. A help fülön a különböz függvények, parancsok leírását, és használatát találjuk meg. A Tools -&gt; Global Options menüpont alatt végezhetjük el az RStudio testreszabását. Így például beállíthatjuk az ablaktér elrendezését (Pane layout), vagy a színvilágot (Appearance), illetve azt, hogy a kódok ne fussanak ki az ablakból (Code -&gt; Editing -&gt; Soft wrap R source files) 13.3 Projekt alapú munka Bár nem kötelez, de javasolt, hogy az RStudio-ban projekt alapon dolgozzunk, mivel így az összes  az adott projekttel kapcsolatos fájlt  egy mappában tárolhatjuk. Új projekt beállítását a File-&gt;New Project menüben tehetjük meg, ahol a saját gépünk egy könyvtárát kell kiválasztani, ahová az R a scripteket, az adat- és elzményfájlokat menti. Ezenkívül a Tools-&gt;Global Options-&gt;General menüpont alatt le kell tiltani a Restore most recently opened project at startup\" és a Restore .RData ino workspace at startup\" beállítást, valamint Save workspace to .RData on exit\" legördül menüjében be kell állítani a Never\" értéket. Figure 13.2: RStudio projekt beállítások A szükséges beállítások után a File -&gt; New Project menüben hozhatjuk létre a projektet. Itt lehetségünk van azt is kiválasztani, hogy a projektünket egy teljesen új könyvtárba, vagy egy meglévbe kívánjuk menteni, esetleg egy meglév projekt új verzióját szeretnénk létrehozni. Ha sikeresen létrehoztuk a projektet, az RStudio jobb fels sarkában látnunk kell annak nevét. 13.4 Scriptek szerkesztése, függvények használata Új script a File -&gt; New -&gt; File -&gt; R Script menüpontban hozható létre, mentésére a File-&gt;Save menüpontban egy korábbi script megnyitására File -&gt; Open menüpontban van lehetségünk. Script bármilyen szövegszerkesztvel írható, majd beilleszthet az editor ablakba. A scripteket érdemes magyarázatokkal (kommentekkel) ellátni, hogy a késbbiekben pontosan követhet legyen, hogy melyik parancs segítségével pontosan milyen lépéseket hajtottunk végre. A magyarázatokat vagy más néven kommenteket kettskereszt (#) karakterrel vezetjük be. A scriptbeli utasítások az azokat tartalmazó sorokra állva vagy több sort kijelölve a Run feliratra kattintva vagy a Ctrl+Enter billentyparanccsal futtathatók le. A lefuttatott parancsok és azok eredményei ezután a bal alsó sarokban lév console ablakban jelennek meg és ugyanitt kapunk hibaüzenetet is, ha valamilyen hibát vétettünk a script írása közben. A munkafolyamat során létrehozott állományok (ábrák, fájlok) az ún. munkakönyvtárba (working directory) mentdnek. Az aktuális munkakönyvtár neve, elérési útja a getwd() utasítással jeleníthet meg. A könyvtárban található állományok listázására a list.files() utasítással van lehetségünk. Ha a korábbiaktól eltér munkakönyvtárat akarunk megadni, azt a setwd() függvénnyel tehetjük meg, ahol a ()-ben az adott mappa elérési útját kell megadnunk. Az elérési útban a meghajtó azonosítóját, majd a mappák, almappák nevét vagy egy normál irányú perjel (/), vagy két fordított perjel (\\\\) választja el, mivel az elérési út karakterlánc, ezért azt idézjelek vagy aposztrófok közé kell tennünk. Az aktuális munkakönyvtárba beléphetünk a jobb alsó ablak file lapján a More -&gt; Go To Working Directory segítségével. Ugyanitt a Set Working Directory-val munkakönyvtárnak állíthatjuk be az a mappát, amelyben épp benne vagyunk. Figure 13.3: Working directory beállítások A munkafolyamat befejezésére a q() vagy quit() függvénnyel van lehetségünk. Az R-ben objektumokkal dolgozunk, amik a teljesség igénye nélkül lehetnek például egyszer szám vektortok, vagy akár komplex listák, illetve függvények, ábrák. A munkafolyamat során létrehozott objektumok az RStudio jobb fels ablakának environment fülén jelennek meg. A mentett objektumokat a fent látható sepr ikonra kattintva törölhetjük a memóriából. Az environment ablakra érdemes úgy gondolni hogy ott jelennek meg a memóriában tárolt értékek. Az RStudio jobb alsó ablakának plots fülén láthatjuk azon parancsok eredményét, melyek kimenete valamilyen ábra. A packages fülnél a már telepített és a letölthet kiegészít csomagokat jeleníthetjük meg. A help fülön a korábban említettek szerint a súgó érhet el. Az RStudio-ban használható billentyparancsok teljes listáját Alt+Shift+K billentykombinációval tekinthetjük meg. Néhány gyakrabban használt, hasznos billentyparancs: Ctrl+Enter: futtassa a kódot az aktuális sorban Ctrl+Alt+B: futtassa a kódot az elejétl az aktuális sorig Ctrl+Alt+E: futtassa a kódot az aktuális sortól a forrásfájl végéig Ctrl+D: törölje az aktuális sort Az R-ben beépített függvények (function) állnak rendelkezésünkre a számítások végrehajtására, emellett több csomag (package) is letölthet, amelyek különböz függvényeket tartalmaznak. A függvények a következképpen épülnek fel: Függvénynév(paraméter). Például tartalom képernyre való kiíratását a print() függvénnyel tehetjük, amelynek gömböly zárójelekkel határolt részébe írhatjuk a megjelenítend szöveget. A citation() függvénnyel lekérdezhetjük az egyes beépített csomagokra való hivatkozást is: a citation(quanteda) függvény a quanteda csomag hivatkozását adja meg. Az R súgórendszere a help.start() utasítással indítható el. Egy adott függvényre vonatkozó súgórészlet a függvények neve elé kérdjel írásával, vagy a help() argumentumába a kérdéses függvény nevének beírásával jeleníthet meg (például: help(sum)). 13.5 R csomagok Az R-ben telepíthetk kiegészít csomagok (packages), amelyek alapértelmezetten el nem érhet algoritmusokat, függvényeket tartalmaznak. A csomagok saját dokumentációval rendelkeznek, amelyeket fel kell tüntetni a használatukkal készült publikációink hivatkozáslistájában. A csomagok telepítésére több lehetségünk is van: használhatjuk a menüsor Tools -&gt; Install Packages menüpontját, vagy a jobb alsó ablak packages fül Install menüpontját, illetve az editor ablakban az install.packages() parancsot futtatva, ahol a ()-be a telepíteni kívánt csomag nevét kell beírnunk (pl. install.packages(dplyr)). Figure 13.4: Packages fül 13.6 Objektumok tárolása, értékadás Az objektumok lehetnek például vektorok, mátrixok, tömbök (array), adat táblák (data frame). Értékadás nélkül az R csak megjeleníti a mveletek eredményét, de nem tárolja el azokat. Az eredmények eltárolásához azokat egy objektumba kell elmentenünk. Ehhez meg kell adnunk az objektum nevét majd az &lt;- után adjuk meg annak értékét: a &lt;- 12 + 3. Futtatás után az environments fülön megjelenik az a objektum, melynek értéke 15. Az objektumok elnevezésénél figyelnünk kell arra, hogy az R különbséget tesz a kis és a nagybetk között, valamint azt, hogy az ugyanolyan nev objektumokat kérdés nélkül felülírja és ezt a felülírást nem lehet visszavonni. 13.7 Vektorok Az R-ben kétféle típusú vektort különböztetünk meg: egyedüli vektor (atomic vector), lista (list). Az egyedüli vektornak hat típusa van: logikai (logical), egész szám (integer), természetes szám (double), karakter (character), komplex szám (complex) és nyers adat (raw). A leggyakrabban valamilyen numerikus, logikai vagy karakter vektorral használjuk. Az egyedüli vektorok onnan kapták a nevüket hogy csak egy féle adattípust tudnak tárolni. A listák ezzel szemben gyakorlatilag bármit tudnak tárolni, akár több listát is egybeágyazhatunk. A vektorok és a listák azok az építelemek, amikbl felépülnek az R objektumaink. Több érték vagy azonos típusú objektum összefzését a c() függvénnyel végezhetjük el. A lenti példában három különböz objektumot kreálunk: egy numerikusat, egy karaktert és egy logikait. A karakter vektorban az elemeket idézjellel és vesszvel szeparáljuk. A logikai vektor csak TRUE, illetve FALSE értékeket tartalmazhat. numerikus &lt;- c(1,2,3,4,5) karakter &lt;- c(&quot;kutya&quot;,&quot;macska&quot;,&quot;ló&quot;) logikai &lt;- c(TRUE, TRUE, FALSE) A létrehozott vektorokkal különböz mveleteket végezhetünk el, például összeadhatjuk numerikus vektorainkat. Ebben az esetben az els vektor els eleme a második vektor els eleméhez adódik. c(1:4) + c(10,20,30,40) #&gt; [1] 11 22 33 44 A karaktervektorokat össze is fzhetjük egymással. Példánkban egy új objektumot is létrehoztunk, ezért a jobb fels ablakban, az environment fülön láthatjuk, hogy a létrejött karakter_kombinalt objektum egy négy elem (hosszúságú) karaktervektor (chr [1:4]), melynek elemei a kutya,macska,ló,nyúl. Az objektumként tárolt vektorok tartalmát az adott sort lefuttatva írathatjuk ki a console ablakba. Ugyanezt megtehetjük print() függvény segítségével is, ahol a függvény arrgumentumában () az adott objektum nevét kell szerepeltetnünk. karakter1 &lt;- c(&quot;kutya&quot;,&quot;macska&quot;,&quot;ló&quot;) karakter2 &lt;-c(&quot;nyúl&quot;) karakter_kombinalt &lt;-c(karakter1, karakter2) karakter_kombinalt #&gt; [1] &quot;kutya&quot; &quot;macska&quot; &quot;ló&quot; &quot;nyúl&quot; Ha egy vektorról szeretnénk megtudni, hogy milyen típusú, azt a typeof() vagy a class() paranccsal tehetjük meg, ahol ()-ben az adott objektumként tárolt vektor nevét kell megadnunk: typeof(karakter1). A vektor hosszúságát (a benne tárolt elemek száma vektorok esetén) a lenght() függvénnyel tudhatjuk meg. typeof(karakter1) #&gt; [1] &quot;character&quot; length(karakter1) #&gt; [1] 3 13.8 Faktorok A faktorok a kategórikus adatok tárolására szolgálnak. Faktor típusú változó a factor() függvénnyel hozható létre. A faktor szintjeit (igen, semleges, nem), a levels() függvénnyel kaphatjuk meg, míg az adatok címkéit (tehát a kapott válaszok számát), a labels() paranccsal érhetjük el. survey_response &lt;- factor(c(&quot;igen&quot;, &quot;semleges&quot;, &quot;nem&quot;, &quot;semleges&quot;, &quot;nem&quot;, &quot;nem&quot;, &quot;igen&quot;), ordered = TRUE) levels(survey_response) #&gt; [1] &quot;igen&quot; &quot;nem&quot; &quot;semleges&quot; labels(survey_response) #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; 13.9 Az adattáblák Az adattábla (data frame) a statisztikai és adatelemzési folyamatok egyik leggyakrabban használt adattárolási formája. Az adattáblák különféle oszlopokból állhatnak, amelyek különféle típusú adatokat tartalmazhatnak, de egy oszlop csak egy típusú adatból állhat. Az itt bemutatott adattábla 7 megfigyelést és 4 féle változót tartalmaz (id, country, pop, continent). #&gt; id orszag nepesseg kontinens #&gt; 1 1 Thailand 68.7 Asia #&gt; 2 2 Norway 5.2 Europe #&gt; 3 3 North Korea 24.0 Asia #&gt; 4 4 Canada 47.8 North America #&gt; 5 5 Slovenia 2.0 Europe #&gt; 6 6 France 63.6 Europe #&gt; 7 7 Venezuela 31.6 South America A data frame-be rendezett adatokhoz különböz módon férhetünk hozzá, például a data frame nevének, majd utána []-ben a kívánt sor(ok) számának megadásával, kiírathatjuk a console ablakba annak tetszleges sorát ás oszlopát: orszag_adatok[1, 1]. Az R több különböz módot kínál a data frame sorainak és oszlopainak eléréséhez. A [ általános használata: data_frame[sor, oszlop]. Egy másik megoldás a $ haszálata: data_frame$oszlop. orszag_adatok[1, 4] #&gt; [1] Asia #&gt; Levels: Asia Europe North America South America orszag_adatok$orszag #&gt; [1] &quot;Thailand&quot; &quot;Norway&quot; &quot;North Korea&quot; &quot;Canada&quot; &quot;Slovenia&quot; &quot;France&quot; #&gt; [7] &quot;Venezuela&quot; 13.10 Vizualizáció library(ggplot2) library(gapminder) Az elemzéseinkhez használt data frame adatainak alapján a ggplot2 csomag segítségével lehetségünk van különböz vizualizációk készítésére is. A különböz vizualizációs lehetségeket és a kötetben található vizualizációk színes változatait techinkai okokból a kötet online verziójában mutatjuk be, ami elérhet a poltextLab projekt github oldalán: https://poltextlab.github.io/text_mining_with_r/, illetve a kötet saját oldalán: www.hunminer.hu. Arun, Rajkumar, Venkatasubramaniyan Suresh, CE Veni Madhavan, and MN Narasimha Murthy. 2010. On Finding the Natural Number of Topics with Latent Dirichlet Allocation: Some Observations. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 391402. Baccianella, Stefano, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In Lrec, 10:22002204. Bar, Daniel, Torsten Zesch, and Iryna Gurevych. 2011. A Reflective View on Text Similarity. Proceedings of Recent Advances in Natural Language Processing, 51520. Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research 3 (Jan): 9931022. Brady, Henry E. 2019. The Challenge of Big Data and Data Science. Annual Review of Political Science 22 (1): 297323. https://doi.org/10.1146/annurev-polisci-090216-023229. Burtejin, Zorgit. 2016. Csoportosítás (Klaszterezés). In Kvantitatív Szövegelemzés és Szövegbányászat a Politikatudományban, edited by Miklós Sebk, 85101. Budapest: LHarmattan. Cao, Juan, Tian Xia, Jintao Li, Yongdong Zhang, and Sheng Tang. 2009. A Density-Based Method for Adaptive LDA Model Selection. Neurocomputing 72 (7-9): 177581. Deveaud, Romain, Eric SanJuan, and Patrice Bellot. 2014. Accurate and Effective Latent Concept Modeling for Ad Hoc Information Retrieval. Document Numérique 17 (1): 6184. Griffiths, T. L., and M. Steyvers. 2004. Finding Scientific Topics. Proceedings of the National Academy of Sciences 101 (Supplement 1): 522835. https://doi.org/10.1073/pnas.0307752101. Grimmer, Justin, and Brandon M Stewart. 2013. Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis 21 (3): 26797. Hjorth, Frederik, Robert Klemmensen, Sara Hobolt, Martin Ejnar Hansen, and Peter Kurrild-Klitgaard. 2015. Computers, Coders, and Voters: Comparing Automated Methods for Estimating Party Positions. Research &amp; Politics 2 (2): 2053168015580476. Jacobi, Carina, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative Analysis of Large Amounts of Journalistic Texts Using Topic Modelling. Digital Journalism 4 (1): 89106. Kusner, Matt J, Yu Sun, Nicholas I Kolkin, and Kilian Q Weinberger. 2015. From Word Embeddings To Document Distances. Proceedings of the 32nd International Conference on Machine Learning. Kwartler, Ted. 2017. Text Mining in Practice with R. John Wiley &amp; Sons. Ladd, John R. 2020. Understanding and Using Common Similarity Measures for Text Analysis. The Programming Historian 9. https://doi.org/10.46430/phen0089. Laver, Michael, Kenneth Benoit, and John Garry. 2003. Extracting Policy Positions from Political Texts Using Words as Data. American Political Science Review, 31131. Laver, Michael, and John Garry. 2000. Estimating Policy Positions from Political Texts. American Journal of Political Science, 61934. Liu, Bing. 2010. Sentiment Analysis and Subjectivity. Handbook of Natural Language Processing 2 (2010): 62766. Loughran, Tim, and Bill McDonald. 2011. When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks. The Journal of Finance 66 (1): 3565. Máté, Ákos, Miklós Sebk, and Tamás Barczikay. 2021. The Effect of Central Bank Communication on Sovereign Bond Yields: The Case of Hungary. Edited by Hiranya K. Nath. PLOS ONE 16 (2): e0245515. https://doi.org/10.1371/journal.pone.0245515. Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. arXiv Preprint arXiv:1301.3781. Mikolov, Tomas, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Advances in Pre-Training Distributed Word Representations. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018). Pennington, Jeffrey, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 153243. Phan, Xuan-Hieu, Le-Minh Nguyen, and Susumu Horiguchi. 2008. Learning to Classify Short and Sparse Text &amp; Web with Hidden Topics from Large-Scale Data Collections. In, 91100. Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and David G Rand. 2014. Structural Topic Models for Open-Ended Survey Responses. American Journal of Political Science 58 (4): 106482. Russel, Stuart, and Peter Norvig. 2005. Mesterséges Intelligencia. Panem Kft. Schütze, Hinrich, Christopher D Manning, and Prabhakar Raghavan. 2008. Introduction to Information Retrieval. Vol. 39. Cambridge University Press Cambridge. Sebk, Miklós. 2016. Kvantitatív Szövegelemzés és Szövegbányászat a Politikatudományban. LHarmattan Kiadó. Sebk, Miklós, and Zoltán Kacsuk. 2021. The Multiclass Classification of Newspaper Articles with Machine Learning: The Hybrid Binary Snowball Approach. Political Analysis 29 (2): 23649. https://doi.org/10.1017/pan.2020.27. Sieg, Adrien. 2018. Text Similarities : Estimate the Degree of Similarity Between Two Texts. Medium. https://medium.com/@adriensieg/text-similarities-da019229c894. Silge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. \" OReilly Media, Inc.\". Slapin, Jonathan B, and SvenOliver Proksch. 2008. A Scaling Model for Estimating Timeseries Party Positions from Texts. American Journal of Political Science 52 (3): 70522. Spirling, Arthur, and Pedro L Rodriguez. 2021. Word Embeddings. Journal of Politics. https://polmeth.mit.edu/sites/default/files/documents/Pedro_Rodriguez.pdf. Straka, Milan, and Jana Straková. 2017. Tokenizing, Pos Tagging, Lemmatizing and Parsing Ud 2.0 with Udpipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, 8899. Szarvas, György, Richárd Farkas, and András Kocsor. 2006. A Multilingual Named Entity Recognition System Using Boosting and C4. 5 Decision Tree Learning Algorithms. In International Conference on Discovery Science, 26778. Springer. Tan, Pang-Ning, Michael Steinbach, and Vipin Kumar. 2011. Bevezetés Az Adatbányászatba. Panem Kft. Tikk, Domonkos. 2007. Szövegbányászat. Budapest: Typotext. Üveges, István. 2019. Named Entity Recognition in the Miskolc Legal Corpus. Vincze, Veronika. 2019. Beszéd és Nyelvelemz Szoftverek. In Beszéd és Nyelvelemz Szoftverek a Versenyképességért és Az Esélyegyenlségért HunCLARIN Korpuszok és Nyelvtechnológiai Eszközök a bölcsészet és társadalomtudományokban, 722. Szeged. Wang, Jiapeng, and Yihong Dong. 2020. Measurement of Text Similarity: A Survey. Information 11 (9): 421. https://doi.org/10.3390/info11090421. Welbers, Kasper, Wouter Van Atteveldt, and Kenneth Benoit. 2017. Text Analysis in R. Communication Methods and Measures 11 (4): 24565. Wickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" OReilly Media, Inc.\". Young, Lori, and Stuart Soroka. 2012. Affective News: The Automated Coding of Sentiment in Political Texts. Political Communication 29 (2): 20531. Zsibrita, János, Veronika Vincze, and Richárd Farkas. 2013. Magyarlanc: A Tool for Morphological and Dependency Parsing of Hungarian. In Proceedings of the International Conference Recent Advances in Natural Language Processing RANLP 2013, 76371. "]]
